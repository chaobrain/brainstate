{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 19: Sequence Modeling with RNNs and Transformers\n",
    "\n",
    "In this tutorial, we'll build sequence models for tasks like text generation and time series prediction using RNNs, LSTMs, and attention mechanisms.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "- Build RNN and LSTM models for sequence tasks\n",
    "- Implement attention mechanisms\n",
    "- Create a simple Transformer architecture\n",
    "- Train models for text generation\n",
    "- Handle variable-length sequences\n",
    "- Implement teacher forcing and sampling strategies\n",
    "- Visualize attention weights\n",
    "\n",
    "## What We'll Build\n",
    "\n",
    "We'll create:\n",
    "- Character-level language model with RNN/LSTM\n",
    "- Attention-based sequence-to-sequence model\n",
    "- Simple Transformer for sequence modeling\n",
    "- Text generation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brainstate as bst\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "import string\n",
    "\n",
    "# Set random seed\n",
    "bst.random.seed(42)\n",
    "\n",
    "print(f\"JAX devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Preparation: Character-Level Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text data\n",
    "sample_text = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog.\n",
    "Neural networks are powerful tools for machine learning.\n",
    "Deep learning has revolutionized artificial intelligence.\n",
    "Transformers have become the dominant architecture for NLP.\n",
    "Attention is all you need for sequence modeling.\n",
    "Recurrent neural networks process sequences step by step.\n",
    "LSTMs solve the vanishing gradient problem in RNNs.\n",
    "BrainState makes it easy to build neural networks with JAX.\n",
    "\"\"\"\n",
    "\n",
    "class CharacterTokenizer:\n",
    "    \"\"\"Simple character-level tokenizer.\"\"\"\n",
    "    \n",
    "    def __init__(self, text: str):\n",
    "        # Get unique characters\n",
    "        self.chars = sorted(list(set(text)))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        \n",
    "        # Create mappings\n",
    "        self.char_to_idx = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
    "    \n",
    "    def encode(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Convert text to indices.\"\"\"\n",
    "        return np.array([self.char_to_idx[ch] for ch in text])\n",
    "    \n",
    "    def decode(self, indices: np.ndarray) -> str:\n",
    "        \"\"\"Convert indices to text.\"\"\"\n",
    "        return ''.join([self.idx_to_char[int(idx)] for idx in indices])\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = CharacterTokenizer(sample_text)\n",
    "encoded_text = tokenizer.encode(sample_text)\n",
    "\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Characters: {''.join(tokenizer.chars[:20])}...\")\n",
    "print(f\"\\nOriginal text length: {len(sample_text)}\")\n",
    "print(f\"Encoded length: {len(encoded_text)}\")\n",
    "print(f\"\\nSample encoding: {encoded_text[:20]}\")\n",
    "print(f\"Decoded: {tokenizer.decode(encoded_text[:20])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_length):\n",
    "    \"\"\"Create input-target pairs for sequence prediction.\n",
    "    \n",
    "    Args:\n",
    "        data: Encoded text data\n",
    "        seq_length: Length of input sequences\n",
    "        \n",
    "    Returns:\n",
    "        inputs: Array of shape (n_sequences, seq_length)\n",
    "        targets: Array of shape (n_sequences, seq_length)\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    \n",
    "    for i in range(len(data) - seq_length):\n",
    "        inputs.append(data[i:i+seq_length])\n",
    "        targets.append(data[i+1:i+seq_length+1])\n",
    "    \n",
    "    return np.array(inputs), np.array(targets)\n",
    "\n",
    "# Create sequences\n",
    "seq_length = 20\n",
    "X, y = create_sequences(encoded_text, seq_length)\n",
    "\n",
    "# Split into train/val\n",
    "split_idx = int(0.8 * len(X))\n",
    "X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "\n",
    "print(f\"Training sequences: {X_train.shape}\")\n",
    "print(f\"Validation sequences: {X_val.shape}\")\n",
    "print(f\"\\nExample sequence:\")\n",
    "print(f\"Input:  {tokenizer.decode(X_train[0])}\")\n",
    "print(f\"Target: {tokenizer.decode(y_train[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RNN Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLanguageModel(bst.graph.Node):\n",
    "    \"\"\"Simple RNN for character-level language modeling.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim=64, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = bst.ParamState(\n",
    "            bst.random.randn(vocab_size, embedding_dim) * 0.1\n",
    "        )\n",
    "        \n",
    "        # RNN cell\n",
    "        self.rnn_cell = bst.nn.RNNCell(embedding_dim, hidden_dim)\n",
    "        \n",
    "        # Output projection\n",
    "        self.fc_out = bst.nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def __call__(self, x, hidden=None):\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Input indices of shape (batch, seq_len)\n",
    "            hidden: Initial hidden state\n",
    "            \n",
    "        Returns:\n",
    "            logits: Output logits of shape (batch, seq_len, vocab_size)\n",
    "            hidden: Final hidden state\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = x.shape\n",
    "        \n",
    "        # Initialize hidden state if not provided\n",
    "        if hidden is None:\n",
    "            hidden = jnp.zeros((batch_size, self.hidden_dim))\n",
    "        \n",
    "        # Embed input\n",
    "        embedded = self.embedding.value[x]  # (batch, seq_len, embedding_dim)\n",
    "        \n",
    "        # Process sequence\n",
    "        outputs = []\n",
    "        for t in range(seq_len):\n",
    "            hidden = self.rnn_cell(embedded[:, t, :], hidden)\n",
    "            outputs.append(hidden)\n",
    "        \n",
    "        # Stack outputs\n",
    "        outputs = jnp.stack(outputs, axis=1)  # (batch, seq_len, hidden_dim)\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        logits = jax.vmap(self.fc_out)(outputs)  # (batch, seq_len, vocab_size)\n",
    "        \n",
    "        return logits, hidden\n",
    "\n",
    "# Create model\n",
    "rnn_model = RNNLanguageModel(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    embedding_dim=64,\n",
    "    hidden_dim=128\n",
    ")\n",
    "\n",
    "# Test forward pass\n",
    "test_input = jnp.array(X_train[:4])\n",
    "test_logits, test_hidden = rnn_model(test_input)\n",
    "\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Logits shape: {test_logits.shape}\")\n",
    "print(f\"Hidden shape: {test_hidden.shape}\")\n",
    "\n",
    "# Count parameters\n",
    "n_params = sum(p.value.size for p in rnn_model.states(bst.ParamState).values())\n",
    "print(f\"\\nTotal parameters: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LSTM Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(bst.graph.Node):\n",
    "    \"\"\"LSTM for character-level language modeling.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim=64, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = bst.ParamState(\n",
    "            bst.random.randn(vocab_size, embedding_dim) * 0.1\n",
    "        )\n",
    "        \n",
    "        # LSTM cell\n",
    "        self.lstm_cell = bst.nn.LSTMCell(embedding_dim, hidden_dim)\n",
    "        \n",
    "        # Output projection\n",
    "        self.fc_out = bst.nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def __call__(self, x, state=None):\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Input indices of shape (batch, seq_len)\n",
    "            state: Initial (hidden, cell) state tuple\n",
    "            \n",
    "        Returns:\n",
    "            logits: Output logits of shape (batch, seq_len, vocab_size)\n",
    "            state: Final (hidden, cell) state\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = x.shape\n",
    "        \n",
    "        # Initialize state if not provided\n",
    "        if state is None:\n",
    "            hidden = jnp.zeros((batch_size, self.hidden_dim))\n",
    "            cell = jnp.zeros((batch_size, self.hidden_dim))\n",
    "            state = (hidden, cell)\n",
    "        \n",
    "        hidden, cell = state\n",
    "        \n",
    "        # Embed input\n",
    "        embedded = self.embedding.value[x]  # (batch, seq_len, embedding_dim)\n",
    "        \n",
    "        # Process sequence\n",
    "        outputs = []\n",
    "        for t in range(seq_len):\n",
    "            hidden, cell = self.lstm_cell(embedded[:, t, :], (hidden, cell))\n",
    "            outputs.append(hidden)\n",
    "        \n",
    "        # Stack outputs\n",
    "        outputs = jnp.stack(outputs, axis=1)  # (batch, seq_len, hidden_dim)\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        logits = jax.vmap(self.fc_out)(outputs)  # (batch, seq_len, vocab_size)\n",
    "        \n",
    "        return logits, (hidden, cell)\n",
    "\n",
    "# Create LSTM model\n",
    "lstm_model = LSTMLanguageModel(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    embedding_dim=64,\n",
    "    hidden_dim=128\n",
    ")\n",
    "\n",
    "# Test forward pass\n",
    "test_logits, test_state = lstm_model(test_input)\n",
    "\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Logits shape: {test_logits.shape}\")\n",
    "print(f\"Hidden state shape: {test_state[0].shape}\")\n",
    "print(f\"Cell state shape: {test_state[1].shape}\")\n",
    "\n",
    "n_params = sum(p.value.size for p in lstm_model.states(bst.ParamState).values())\n",
    "print(f\"\\nTotal parameters: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(logits, targets):\n",
    "    \"\"\"Compute cross-entropy loss for sequences.\n",
    "    \n",
    "    Args:\n",
    "        logits: Predicted logits of shape (batch, seq_len, vocab_size)\n",
    "        targets: True indices of shape (batch, seq_len)\n",
    "        \n",
    "    Returns:\n",
    "        Scalar loss value\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, vocab_size = logits.shape\n",
    "    \n",
    "    # Reshape for easier computation\n",
    "    logits_flat = logits.reshape(-1, vocab_size)\n",
    "    targets_flat = targets.reshape(-1)\n",
    "    \n",
    "    # One-hot encode targets\n",
    "    one_hot = jax.nn.one_hot(targets_flat, vocab_size)\n",
    "    \n",
    "    # Compute log probabilities\n",
    "    log_probs = jax.nn.log_softmax(logits_flat, axis=-1)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = -jnp.mean(jnp.sum(one_hot * log_probs, axis=-1))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def perplexity(loss):\n",
    "    \"\"\"Compute perplexity from loss.\"\"\"\n",
    "    return jnp.exp(loss)\n",
    "\n",
    "def train_step(model, x_batch, y_batch, learning_rate=0.001):\n",
    "    \"\"\"Perform one training step.\"\"\"\n",
    "    with bst.environ.context(fit=True):\n",
    "        def loss_fn():\n",
    "            logits, _ = model(jnp.array(x_batch))\n",
    "            return cross_entropy_loss(logits, jnp.array(y_batch))\n",
    "        \n",
    "        # Compute gradients\n",
    "        loss, grads = bst.augment.grad(\n",
    "            loss_fn,\n",
    "            model.states(bst.ParamState),\n",
    "            return_value=True\n",
    "        )()\n",
    "        \n",
    "        # Update parameters\n",
    "        for name, grad in grads.items():\n",
    "            model.states()[name].value -= learning_rate * grad\n",
    "        \n",
    "        return float(loss)\n",
    "\n",
    "def eval_step(model, x_batch, y_batch):\n",
    "    \"\"\"Perform one evaluation step.\"\"\"\n",
    "    with bst.environ.context(fit=False):\n",
    "        logits, _ = model(jnp.array(x_batch))\n",
    "        loss = cross_entropy_loss(logits, jnp.array(y_batch))\n",
    "        return float(loss)\n",
    "\n",
    "# Test training step\n",
    "batch_size = 32\n",
    "x_batch = X_train[:batch_size]\n",
    "y_batch = y_train[:batch_size]\n",
    "\n",
    "initial_loss = eval_step(lstm_model, x_batch, y_batch)\n",
    "print(f\"Initial loss: {initial_loss:.4f}\")\n",
    "print(f\"Initial perplexity: {perplexity(initial_loss):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, X, y, batch_size, learning_rate):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    losses = []\n",
    "    n_samples = len(X)\n",
    "    \n",
    "    # Shuffle data\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    \n",
    "    for start_idx in range(0, n_samples, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, n_samples)\n",
    "        batch_indices = indices[start_idx:end_idx]\n",
    "        \n",
    "        x_batch = X[batch_indices]\n",
    "        y_batch = y[batch_indices]\n",
    "        \n",
    "        loss = train_step(model, x_batch, y_batch, learning_rate)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    return np.mean(losses)\n",
    "\n",
    "def evaluate(model, X, y, batch_size):\n",
    "    \"\"\"Evaluate model.\"\"\"\n",
    "    losses = []\n",
    "    n_samples = len(X)\n",
    "    \n",
    "    for start_idx in range(0, n_samples, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, n_samples)\n",
    "        x_batch = X[start_idx:end_idx]\n",
    "        y_batch = y[start_idx:end_idx]\n",
    "        \n",
    "        loss = eval_step(model, x_batch, y_batch)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    return np.mean(losses)\n",
    "\n",
    "# Training configuration\n",
    "config = {\n",
    "    'num_epochs': 50,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 0.003,\n",
    "}\n",
    "\n",
    "# Create fresh model\n",
    "model = LSTMLanguageModel(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    embedding_dim=64,\n",
    "    hidden_dim=128\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "history = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "print(\"Training LSTM Language Model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(config['num_epochs']):\n",
    "    # Train\n",
    "    train_loss = train_epoch(\n",
    "        model, X_train, y_train,\n",
    "        config['batch_size'],\n",
    "        config['learning_rate']\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_loss = evaluate(model, X_val, y_val, config['batch_size'])\n",
    "    \n",
    "    # Record history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    \n",
    "    # Print progress\n",
    "    if epoch % 10 == 0 or epoch == config['num_epochs'] - 1:\n",
    "        print(f\"Epoch {epoch:2d}: \"\n",
    "              f\"train_loss={train_loss:.4f} (ppl={perplexity(train_loss):.2f}), \"\n",
    "              f\"val_loss={val_loss:.4f} (ppl={perplexity(val_loss):.2f})\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "epochs = range(len(history['train_loss']))\n",
    "\n",
    "# Loss\n",
    "ax1.plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "ax1.plot(epochs, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Perplexity\n",
    "train_ppl = [perplexity(loss) for loss in history['train_loss']]\n",
    "val_ppl = [perplexity(loss) for loss in history['val_loss']]\n",
    "\n",
    "ax2.plot(epochs, train_ppl, 'b-', label='Train Perplexity', linewidth=2)\n",
    "ax2.plot(epochs, val_ppl, 'r-', label='Val Perplexity', linewidth=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Perplexity')\n",
    "ax2.set_title('Training and Validation Perplexity')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_text, length=100, temperature=1.0):\n",
    "    \"\"\"Generate text using the trained model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained language model\n",
    "        start_text: Starting string\n",
    "        length: Number of characters to generate\n",
    "        temperature: Sampling temperature (higher = more random)\n",
    "        \n",
    "    Returns:\n",
    "        Generated text string\n",
    "    \"\"\"\n",
    "    # Encode start text\n",
    "    current_seq = tokenizer.encode(start_text)\n",
    "    generated = list(current_seq)\n",
    "    \n",
    "    with bst.environ.context(fit=False):\n",
    "        state = None\n",
    "        \n",
    "        for _ in range(length):\n",
    "            # Prepare input (last seq_length characters)\n",
    "            input_seq = current_seq[-seq_length:]\n",
    "            input_seq = jnp.array(input_seq).reshape(1, -1)\n",
    "            \n",
    "            # Pad if necessary\n",
    "            if input_seq.shape[1] < seq_length:\n",
    "                pad_length = seq_length - input_seq.shape[1]\n",
    "                padding = jnp.zeros((1, pad_length), dtype=jnp.int32)\n",
    "                input_seq = jnp.concatenate([padding, input_seq], axis=1)\n",
    "            \n",
    "            # Get predictions\n",
    "            logits, state = model(input_seq, state)\n",
    "            \n",
    "            # Get last time step\n",
    "            last_logits = logits[0, -1, :] / temperature\n",
    "            \n",
    "            # Sample from distribution\n",
    "            probs = jax.nn.softmax(last_logits)\n",
    "            next_idx = np.random.choice(len(probs), p=np.array(probs))\n",
    "            \n",
    "            # Append to sequence\n",
    "            generated.append(next_idx)\n",
    "            current_seq = np.append(current_seq, next_idx)\n",
    "    \n",
    "    return tokenizer.decode(np.array(generated))\n",
    "\n",
    "# Generate samples with different temperatures\n",
    "start_text = \"The \"\n",
    "temperatures = [0.5, 1.0, 1.5]\n",
    "\n",
    "print(\"Generated Text Samples:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for temp in temperatures:\n",
    "    generated = generate_text(model, start_text, length=150, temperature=temp)\n",
    "    print(f\"\\nTemperature = {temp}:\")\n",
    "    print(generated)\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(bst.graph.Node):\n",
    "    \"\"\"Scaled dot-product attention mechanism.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.scale = jnp.sqrt(dim)\n",
    "    \n",
    "    def __call__(self, query, key, value, mask=None):\n",
    "        \"\"\"Compute attention.\n",
    "        \n",
    "        Args:\n",
    "            query: Query tensor of shape (batch, seq_len_q, dim)\n",
    "            key: Key tensor of shape (batch, seq_len_k, dim)\n",
    "            value: Value tensor of shape (batch, seq_len_v, dim)\n",
    "            mask: Optional mask\n",
    "            \n",
    "        Returns:\n",
    "            output: Attended values\n",
    "            attention_weights: Attention weights\n",
    "        \"\"\"\n",
    "        # Compute attention scores\n",
    "        scores = jnp.matmul(query, key.transpose(0, 2, 1)) / self.scale\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            scores = jnp.where(mask, scores, -1e9)\n",
    "        \n",
    "        # Compute attention weights\n",
    "        attention_weights = jax.nn.softmax(scores, axis=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        output = jnp.matmul(attention_weights, value)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "class AttentionLSTM(bst.graph.Node):\n",
    "    \"\"\"LSTM with self-attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim=64, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Embedding\n",
    "        self.embedding = bst.ParamState(\n",
    "            bst.random.randn(vocab_size, embedding_dim) * 0.1\n",
    "        )\n",
    "        \n",
    "        # LSTM\n",
    "        self.lstm_cell = bst.nn.LSTMCell(embedding_dim, hidden_dim)\n",
    "        \n",
    "        # Attention\n",
    "        self.attention = ScaledDotProductAttention(hidden_dim)\n",
    "        \n",
    "        # Output\n",
    "        self.fc_out = bst.nn.Linear(hidden_dim * 2, vocab_size)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Forward pass with attention.\"\"\"\n",
    "        batch_size, seq_len = x.shape\n",
    "        \n",
    "        # Embed\n",
    "        embedded = self.embedding.value[x]\n",
    "        \n",
    "        # LSTM processing\n",
    "        hidden = jnp.zeros((batch_size, self.hidden_dim))\n",
    "        cell = jnp.zeros((batch_size, self.hidden_dim))\n",
    "        \n",
    "        lstm_outputs = []\n",
    "        for t in range(seq_len):\n",
    "            hidden, cell = self.lstm_cell(embedded[:, t, :], (hidden, cell))\n",
    "            lstm_outputs.append(hidden)\n",
    "        \n",
    "        lstm_outputs = jnp.stack(lstm_outputs, axis=1)\n",
    "        \n",
    "        # Self-attention\n",
    "        attended, attn_weights = self.attention(\n",
    "            lstm_outputs, lstm_outputs, lstm_outputs\n",
    "        )\n",
    "        \n",
    "        # Concatenate LSTM output and attended output\n",
    "        combined = jnp.concatenate([lstm_outputs, attended], axis=-1)\n",
    "        \n",
    "        # Output projection\n",
    "        logits = jax.vmap(self.fc_out)(combined)\n",
    "        \n",
    "        return logits, attn_weights\n",
    "\n",
    "# Create attention model\n",
    "attn_model = AttentionLSTM(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    embedding_dim=64,\n",
    "    hidden_dim=128\n",
    ")\n",
    "\n",
    "# Test\n",
    "test_logits, test_attn = attn_model(test_input)\n",
    "print(f\"Logits shape: {test_logits.shape}\")\n",
    "print(f\"Attention weights shape: {test_attn.shape}\")\n",
    "\n",
    "n_params = sum(p.value.size for p in attn_model.states(bst.ParamState).values())\n",
    "print(f\"\\nTotal parameters: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get attention weights for a sample\n",
    "sample_input = jnp.array(X_train[0:1])  # Single sequence\n",
    "\n",
    "with bst.environ.context(fit=False):\n",
    "    _, attn_weights = attn_model(sample_input)\n",
    "\n",
    "# Plot attention heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(attn_weights[0], cmap='viridis', aspect='auto')\n",
    "plt.colorbar(label='Attention Weight')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.title('Self-Attention Weights')\n",
    "\n",
    "# Add text labels\n",
    "input_text = tokenizer.decode(X_train[0])\n",
    "plt.text(0.5, -0.1, f'Input: \"{input_text}\"', \n",
    "         transform=plt.gca().transAxes, ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Simple Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(bst.graph.Node):\n",
    "    \"\"\"Simple Transformer block with self-attention and feed-forward.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim, num_heads=4, ff_dim=256):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Multi-head attention (simplified)\n",
    "        self.attention = ScaledDotProductAttention(dim)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.ln1 = bst.nn.LayerNorm([dim])\n",
    "        self.ln2 = bst.nn.LayerNorm([dim])\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ff1 = bst.nn.Linear(dim, ff_dim)\n",
    "        self.ff2 = bst.nn.Linear(ff_dim, dim)\n",
    "    \n",
    "    def __call__(self, x, mask=None):\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Input of shape (batch, seq_len, dim)\n",
    "            mask: Optional attention mask\n",
    "            \n",
    "        Returns:\n",
    "            Output of same shape as input\n",
    "        \"\"\"\n",
    "        # Self-attention with residual\n",
    "        attn_out, _ = self.attention(x, x, x, mask)\n",
    "        x = self.ln1(x + attn_out)\n",
    "        \n",
    "        # Feed-forward with residual\n",
    "        ff_out = self.ff2(jax.nn.relu(self.ff1(x)))\n",
    "        x = self.ln2(x + ff_out)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class SimpleTransformer(bst.graph.Node):\n",
    "    \"\"\"Simple Transformer for sequence modeling.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, dim=128, num_layers=2, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim = dim\n",
    "        \n",
    "        # Embedding\n",
    "        self.embedding = bst.ParamState(\n",
    "            bst.random.randn(vocab_size, dim) * 0.1\n",
    "        )\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = []\n",
    "        for i in range(num_layers):\n",
    "            block = TransformerBlock(dim, num_heads, ff_dim=dim*4)\n",
    "            self.blocks.append(block)\n",
    "            setattr(self, f'block_{i}', block)\n",
    "        \n",
    "        # Output\n",
    "        self.fc_out = bst.nn.Linear(dim, vocab_size)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        # Embed\n",
    "        x = self.embedding.value[x]\n",
    "        \n",
    "        # Transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Output projection\n",
    "        logits = jax.vmap(self.fc_out)(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Create Transformer\n",
    "transformer = SimpleTransformer(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    dim=128,\n",
    "    num_layers=2,\n",
    "    num_heads=4\n",
    ")\n",
    "\n",
    "# Test\n",
    "test_logits = transformer(test_input)\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {test_logits.shape}\")\n",
    "\n",
    "n_params = sum(p.value.size for p in transformer.states(bst.ParamState).values())\n",
    "print(f\"\\nTotal parameters: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we built sequence models:\n",
    "\n",
    "1. **Character-Level Tokenization**: Simple text encoding\n",
    "2. **RNN Language Model**: Basic recurrent architecture\n",
    "3. **LSTM Language Model**: Improved with LSTM cells\n",
    "4. **Training Pipeline**: Complete training loop for sequences\n",
    "5. **Text Generation**: Sampling strategies with temperature\n",
    "6. **Attention Mechanism**: Scaled dot-product attention\n",
    "7. **Attention Visualization**: Heatmaps of attention weights\n",
    "8. **Transformer Block**: Self-attention with feed-forward\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **RNNs process sequences step-by-step**\n",
    "- **LSTMs handle long-term dependencies better**\n",
    "- **Attention allows focusing on relevant parts**\n",
    "- **Transformers use self-attention exclusively**\n",
    "- **Temperature controls generation randomness**\n",
    "- **Perplexity measures language model quality**\n",
    "- Sequence modeling requires careful state management\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next tutorial, we'll explore:\n",
    "- **Brain-Inspired Computing**: Spiking neural networks\n",
    "- Neurodynamics and biological models\n",
    "- Plasticity and learning rules\n",
    "- Event-driven computation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

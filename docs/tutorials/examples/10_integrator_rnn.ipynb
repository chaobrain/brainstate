{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Training Recurrent Neural Networks: Integrator Task\n",
    "\n",
    "This tutorial demonstrates how to train a recurrent neural network (RNN) to perform an **integration task** - a fundamental computation in neuroscience where networks must accumulate evidence over time.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "- Understand the integration task and its importance\n",
    "- Build custom RNN cells in BrainState\n",
    "- Train RNNs on temporal tasks\n",
    "- Use trainable initial states\n",
    "- Apply L2 regularization to prevent overfitting\n",
    "- Visualize RNN predictions on time-series data\n",
    "\n",
    "## The Integration Task\n",
    "\n",
    "**Goal**: Given a noisy input signal, the network must compute the cumulative sum (integral) over time.\n",
    "\n",
    "```\n",
    "Input:  [x₁, x₂, x₃, ...]\n",
    "Output: [x₁, x₁+x₂, x₁+x₂+x₃, ...]\n",
    "```\n",
    "\n",
    "This task requires:\n",
    "- **Memory**: Remember past inputs\n",
    "- **Accumulation**: Continuously integrate information\n",
    "- **Robustness**: Handle noise in inputs\n",
    "\n",
    "**Applications**:\n",
    "- Evidence accumulation in decision-making\n",
    "- Position estimation from velocity\n",
    "- Financial modeling (cumulative returns)\n",
    "- Signal processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import brainstate\n",
    "import braintools\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "brainstate.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hyperparams",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task parameters\n",
    "dt = 0.04              # Time step\n",
    "num_step = int(1.0 / dt)  # Steps per trial (25 steps for 1.0 time unit)\n",
    "num_batch = 512        # Batch size\n",
    "\n",
    "# Network parameters\n",
    "num_hidden = 100       # Hidden units in RNN\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.025\n",
    "lr_decay_rate = 0.99975\n",
    "l2_reg = 2e-4\n",
    "num_epochs = 5\n",
    "batches_per_epoch = 500\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Sequence length: {num_step} steps\")\n",
    "print(f\"  Batch size: {num_batch}\")\n",
    "print(f\"  Hidden units: {num_hidden}\")\n",
    "print(f\"  Training batches: {num_epochs * batches_per_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_section",
   "metadata": {},
   "source": [
    "## Generate Data\n",
    "\n",
    "### Data Generation Function\n",
    "\n",
    "We'll create random walk inputs and compute their cumulative sums as targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_gen",
   "metadata": {},
   "outputs": [],
   "source": [
    "@brainstate.transform.jit(static_argnums=2)\n",
    "def build_inputs_and_targets(mean=0.025, scale=0.01, batch_size=10):\n",
    "    \"\"\"Generate integration task data.\n",
    "    \n",
    "    Args:\n",
    "        mean: Mean of the random walk bias\n",
    "        scale: Standard deviation of noise\n",
    "        batch_size: Number of sequences\n",
    "        \n",
    "    Returns:\n",
    "        inputs: [num_step, batch_size, 1] - Input sequences\n",
    "        targets: [num_step, batch_size, 1] - Target cumulative sums\n",
    "    \"\"\"\n",
    "    # Create initial bias\n",
    "    sample = brainstate.random.normal(size=(1, batch_size, 1))\n",
    "    bias = mean * 2.0 * (sample - 0.5)\n",
    "    \n",
    "    # Generate white noise\n",
    "    samples = brainstate.random.normal(size=(num_step, batch_size, 1))\n",
    "    noise_t = scale / dt ** 0.5 * samples\n",
    "    \n",
    "    # Inputs = bias + noise\n",
    "    inputs = bias + noise_t\n",
    "    \n",
    "    # Targets = cumulative sum of inputs\n",
    "    targets = jnp.cumsum(inputs, axis=0)\n",
    "    \n",
    "    return inputs, targets\n",
    "\n",
    "\n",
    "def train_data():\n",
    "    \"\"\"Generator for training data.\"\"\"\n",
    "    for _ in range(batches_per_epoch * num_epochs):\n",
    "        yield build_inputs_and_targets(0.025, 0.01, num_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize_data",
   "metadata": {},
   "source": [
    "### Visualize Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate one batch for visualization\n",
    "sample_inputs, sample_targets = build_inputs_and_targets(0.025, 0.01, 3)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Plot inputs\n",
    "for i in range(3):\n",
    "    axes[0].plot(sample_inputs[:, i, 0], alpha=0.7, label=f'Sample {i+1}')\n",
    "axes[0].set_xlabel('Time Step')\n",
    "axes[0].set_ylabel('Input Value')\n",
    "axes[0].set_title('Input Sequences (Random Walks)', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot targets\n",
    "for i in range(3):\n",
    "    axes[1].plot(sample_targets[:, i, 0], alpha=0.7, label=f'Sample {i+1}')\n",
    "axes[1].set_xlabel('Time Step')\n",
    "axes[1].set_ylabel('Cumulative Sum')\n",
    "axes[1].set_title('Target Sequences (Integrals)', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_section",
   "metadata": {},
   "source": [
    "## Building the RNN\n",
    "\n",
    "### Custom RNN Cell\n",
    "\n",
    "We'll create a vanilla RNN cell with optional trainable initial state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rnn_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(brainstate.nn.Module):\n",
    "    \"\"\"Vanilla RNN cell with trainable weights.\n",
    "    \n",
    "    h_t = activation(W_combined @ [x_t; h_{t-1}] + b)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_in: int,\n",
    "        num_out: int,\n",
    "        state_initializer: Callable = braintools.init.ZeroInit(),\n",
    "        w_initializer: Callable = braintools.init.XavierNormal(),\n",
    "        b_initializer: Callable = braintools.init.ZeroInit(),\n",
    "        activation: Callable = brainstate.nn.relu,\n",
    "        train_state: bool = False,  # Whether to train initial state\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_out = num_out\n",
    "        self.train_state = train_state\n",
    "        \n",
    "        # Activation function\n",
    "        self.activation = activation\n",
    "        \n",
    "        # Combined weight matrix [input; hidden] -> hidden\n",
    "        W = braintools.init.param(\n",
    "            w_initializer, \n",
    "            (num_in + num_out, num_out)\n",
    "        )\n",
    "        b = braintools.init.param(b_initializer, (num_out,))\n",
    "        \n",
    "        self.W = brainstate.ParamState(W)\n",
    "        self.b = brainstate.ParamState(b) if b is not None else None\n",
    "        \n",
    "        # Trainable initial state (optional)\n",
    "        if train_state:\n",
    "            self.state2train = brainstate.ParamState(\n",
    "                braintools.init.ZeroInit()(num_out)\n",
    "            )\n",
    "        \n",
    "        self._state_initializer = state_initializer\n",
    "    \n",
    "    def init_state(self, batch_size=None, **kwargs):\n",
    "        \"\"\"Initialize hidden state.\"\"\"\n",
    "        self.state = brainstate.HiddenState(\n",
    "            braintools.init.param(\n",
    "                self._state_initializer, \n",
    "                (self.num_out,), \n",
    "                batch_size\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Use trainable initial state if specified\n",
    "        if self.train_state:\n",
    "            self.state.value = jnp.repeat(\n",
    "                jnp.expand_dims(self.state2train.value, axis=0), \n",
    "                batch_size, \n",
    "                axis=0\n",
    "            )\n",
    "    \n",
    "    def update(self, x):\n",
    "        \"\"\"Update RNN cell for one time step.\n",
    "        \n",
    "        Args:\n",
    "            x: Input [batch, input_dim]\n",
    "            \n",
    "        Returns:\n",
    "            h: New hidden state [batch, hidden_dim]\n",
    "        \"\"\"\n",
    "        # Concatenate input and previous hidden state\n",
    "        x_combined = jnp.concatenate([x, self.state.value], axis=-1)\n",
    "        \n",
    "        # Linear transformation\n",
    "        h = x_combined @ self.W.value\n",
    "        if self.b is not None:\n",
    "            h += self.b.value\n",
    "        \n",
    "        # Apply activation\n",
    "        h = self.activation(h)\n",
    "        \n",
    "        # Update state\n",
    "        self.state.value = h\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "network_section",
   "metadata": {},
   "source": [
    "### Complete RNN Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rnn_network",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(brainstate.nn.Module):\n",
    "    \"\"\"RNN with recurrent layer and linear output.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_in, num_hidden):\n",
    "        super().__init__()\n",
    "        \n",
    "        # RNN layer with trainable initial state\n",
    "        self.rnn = RNNCell(num_in, num_hidden, train_state=True)\n",
    "        \n",
    "        # Output projection\n",
    "        self.out = brainstate.nn.Linear(num_hidden, 1)\n",
    "    \n",
    "    def update(self, x):\n",
    "        \"\"\"Process one time step.\n",
    "        \n",
    "        Args:\n",
    "            x: Input at current time step\n",
    "            \n",
    "        Returns:\n",
    "            output: Prediction at current time step\n",
    "        \"\"\"\n",
    "        # RNN forward pass using >> operator (pipe)\n",
    "        return x >> self.rnn >> self.out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_model",
   "metadata": {},
   "source": [
    "### Create Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instantiate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RNN model\n",
    "model = RNN(num_in=1, num_hidden=num_hidden)\n",
    "\n",
    "# Get trainable parameters\n",
    "weights = model.states(brainstate.ParamState)\n",
    "\n",
    "# Create optimizer with learning rate decay\n",
    "lr_schedule = braintools.optim.ExponentialDecayLR(\n",
    "    lr=learning_rate, \n",
    "    decay_steps=1, \n",
    "    decay_rate=lr_decay_rate\n",
    ")\n",
    "optimizer = braintools.optim.Adam(lr=lr_schedule, eps=1e-1)\n",
    "optimizer.register_trainable_weights(weights)\n",
    "\n",
    "print(f\"Model created:\")\n",
    "print(f\"  Parameters: {sum(p.value.size for p in weights.to_dict_values()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_section",
   "metadata": {},
   "source": [
    "## Training the RNN\n",
    "\n",
    "### Define Prediction and Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "@brainstate.transform.jit\n",
    "def f_predict(inputs):\n",
    "    \"\"\"Make predictions for a sequence.\n",
    "    \n",
    "    Args:\n",
    "        inputs: [num_steps, batch_size, 1]\n",
    "        \n",
    "    Returns:\n",
    "        predictions: [num_steps, batch_size, 1]\n",
    "    \"\"\"\n",
    "    # Initialize RNN state\n",
    "    brainstate.nn.init_all_states(model, batch_size=inputs.shape[1])\n",
    "    \n",
    "    # Process sequence\n",
    "    return brainstate.transform.for_loop(model.update, inputs)\n",
    "\n",
    "\n",
    "def f_loss(inputs, targets, l2_reg=2e-4):\n",
    "    \"\"\"Compute loss with L2 regularization.\n",
    "    \n",
    "    Args:\n",
    "        inputs: Input sequences\n",
    "        targets: Target sequences\n",
    "        l2_reg: L2 regularization coefficient\n",
    "        \n",
    "    Returns:\n",
    "        loss: Total loss value\n",
    "    \"\"\"\n",
    "    # Get predictions\n",
    "    predictions = f_predict(inputs)\n",
    "    \n",
    "    # Mean squared error\n",
    "    mse = braintools.metric.squared_error(predictions, targets).mean()\n",
    "    \n",
    "    # L2 regularization on weights\n",
    "    l2 = 0.0\n",
    "    for weight in weights.values():\n",
    "        for leaf in jax.tree.leaves(weight.value):\n",
    "            l2 += jnp.sum(leaf ** 2)\n",
    "    \n",
    "    return mse + l2_reg * l2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train_step_section",
   "metadata": {},
   "source": [
    "### Define Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_step",
   "metadata": {},
   "outputs": [],
   "source": [
    "@brainstate.transform.jit\n",
    "def f_train(inputs, targets):\n",
    "    \"\"\"Perform one training step.\n",
    "    \n",
    "    Args:\n",
    "        inputs: Input sequences\n",
    "        targets: Target sequences\n",
    "        \n",
    "    Returns:\n",
    "        loss: Loss value\n",
    "    \"\"\"\n",
    "    # Compute gradients\n",
    "    grads, loss = brainstate.transform.grad(\n",
    "        f_loss, \n",
    "        weights, \n",
    "        return_value=True\n",
    "    )(inputs, targets)\n",
    "    \n",
    "    # Update parameters\n",
    "    optimizer.update(grads)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run_training",
   "metadata": {},
   "source": [
    "### Run Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training_loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\\n\")\n",
    "\n",
    "for i_epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    \n",
    "    for i_batch, (inputs, targets) in enumerate(train_data()):\n",
    "        if i_batch >= batches_per_epoch:\n",
    "            break\n",
    "        \n",
    "        loss = f_train(inputs, targets)\n",
    "        epoch_losses.append(float(loss))\n",
    "        \n",
    "        if (i_batch + 1) % 100 == 0:\n",
    "            avg_loss = np.mean(epoch_losses[-100:])\n",
    "            print(f'Epoch {i_epoch}, Batch {i_batch + 1:3d}, Loss {avg_loss:.5f}')\n",
    "    \n",
    "    avg_epoch_loss = np.mean(epoch_losses)\n",
    "    print(f'\\nEpoch {i_epoch} completed: Avg Loss = {avg_epoch_loss:.5f}\\n')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval_section",
   "metadata": {},
   "source": [
    "## Evaluation and Visualization\n",
    "\n",
    "### Test on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data\n",
    "brainstate.nn.init_all_states(model, 1)\n",
    "x_test, y_test = build_inputs_and_targets(0.025, 0.01, 1)\n",
    "predictions = f_predict(x_test)\n",
    "\n",
    "print(f\"Test data generated:\")\n",
    "print(f\"  Input shape: {x_test.shape}\")\n",
    "print(f\"  Target shape: {y_test.shape}\")\n",
    "print(f\"  Prediction shape: {predictions.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plot_results",
   "metadata": {},
   "source": [
    "### Plot Predictions vs. Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Convert to numpy for plotting\n",
    "y_true = np.asarray(y_test[:, 0]).flatten()\n",
    "y_pred = np.asarray(predictions[:, 0]).flatten()\n",
    "time_steps = np.arange(len(y_true))\n",
    "\n",
    "# Plot ground truth and predictions\n",
    "plt.plot(time_steps, y_true, 'b-', linewidth=2, label='Ground Truth', alpha=0.7)\n",
    "plt.plot(time_steps, y_pred, 'r--', linewidth=2, label='RNN Prediction', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Time Step', fontsize=12)\n",
    "plt.ylabel('Cumulative Value', fontsize=12)\n",
    "plt.title('RNN Integration Task Performance', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute final error\n",
    "mse = np.mean((y_true - y_pred) ** 2)\n",
    "print(f\"\\nTest MSE: {mse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Key Concepts Summary\n",
    "\n",
    "### Integration Task\n",
    "\n",
    "1. **Temporal Integration**: Accumulating information over time\n",
    "2. **Memory Requirement**: Network must remember all past inputs\n",
    "3. **Continuous Computation**: No distinct decision points\n",
    "\n",
    "### RNN Architecture\n",
    "\n",
    "1. **Recurrent Connections**: Hidden state feeds back into itself\n",
    "2. **Trainable Initial State**: Learn optimal starting point\n",
    "3. **Linear Output**: Direct projection from hidden to output\n",
    "\n",
    "### Training Techniques\n",
    "\n",
    "1. **L2 Regularization**: Prevent overfitting by penalizing large weights\n",
    "2. **Learning Rate Decay**: Gradually reduce learning rate\n",
    "3. **Sequence Processing**: Use `for_loop` for efficient iteration\n",
    "4. **State Initialization**: Reset states for each sequence\n",
    "\n",
    "### BrainState Features\n",
    "\n",
    "- **Custom RNN Cells**: Build specialized recurrent units\n",
    "- **Pipe Operator (>>)**: Clean sequential computation\n",
    "- **State Management**: Automatic handling of hidden states\n",
    "- **JIT Compilation**: Fast training and inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Architecture Variations**:\n",
    "   - Try LSTM or GRU cells\n",
    "   - Add multiple RNN layers\n",
    "   - Compare performance\n",
    "\n",
    "2. **Regularization Experiments**:\n",
    "   - Vary L2 coefficient\n",
    "   - Add dropout\n",
    "   - Try gradient clipping\n",
    "\n",
    "3. **Task Modifications**:\n",
    "   - Change noise levels\n",
    "   - Increase sequence length\n",
    "   - Try different input distributions\n",
    "\n",
    "4. **Advanced Analysis**:\n",
    "   - Visualize hidden state dynamics\n",
    "   - Analyze learned weight matrices\n",
    "   - Study trajectory in phase space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next_steps",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **Sequence-to-Sequence**: Encoder-decoder architectures\n",
    "- **Attention Mechanisms**: Transformers and self-attention\n",
    "- **Spiking RNNs**: Biologically plausible recurrent networks\n",
    "- **Advanced Training**: Online learning algorithms (e-prop, RTRL)\n",
    "\n",
    "## References\n",
    "\n",
    "- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- [RNN Training Challenges](https://arxiv.org/abs/1211.5063)\n",
    "- [Reservoir Computing](https://www.nature.com/articles/s41467-021-25801-2)\n",
    "- [BrainState Documentation](https://brainstate.readthedocs.io/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

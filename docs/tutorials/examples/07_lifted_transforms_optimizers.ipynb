{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4e5f6",
   "metadata": {},
   "source": [
    "# Lifted Transforms and High-Level Optimizers\n",
    "\n",
    "In the previous tutorial, we learned about the functional API with explicit state management using graph operations. While powerful, that approach requires manual state handling. This tutorial introduces **lifted transforms** - a higher-level API that automatically manages states while preserving JAX's functional programming benefits.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "- Understand the difference between functional API and lifted transforms\n",
    "- Use BrainTools optimizers for automatic state management\n",
    "- Leverage `brainstate.transform` for state-aware JAX transformations\n",
    "- Build cleaner training loops with less boilerplate code\n",
    "- Apply the lifted API to the same polynomial regression problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5f6a7",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import brainstate\n",
    "import braintools\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6a7b8c9",
   "metadata": {},
   "source": [
    "## The Same Problem: Polynomial Regression\n",
    "\n",
    "We'll use the same dataset as the previous tutorial to highlight the differences in approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data: y = 0.8 * x^2 + 0.1 + noise\n",
    "X = np.linspace(0, 1, 100)[:, None]\n",
    "Y = 0.8 * X ** 2 + 0.1 + np.random.normal(0, 0.1, size=X.shape)\n",
    "\n",
    "def dataset(batch_size):\n",
    "    \"\"\"Generator that yields random batches from the dataset.\"\"\"\n",
    "    while True:\n",
    "        idx = np.random.choice(len(X), size=batch_size)\n",
    "        yield X[idx], Y[idx]\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(X, Y, alpha=0.5, label='Data')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Synthetic Dataset for Polynomial Regression')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7b8c9d0e1",
   "metadata": {},
   "source": [
    "## Building the Model with Lifted Transforms\n",
    "\n",
    "### Step 1: Define the Model Components\n",
    "\n",
    "The model definition is similar, but we'll use it differently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8c9d0e1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(brainstate.nn.Module):\n",
    "    \"\"\"A simple linear layer: y = x @ w + b\"\"\"\n",
    "    \n",
    "    def __init__(self, din: int, dout: int):\n",
    "        super().__init__()\n",
    "        self.w = brainstate.ParamState(brainstate.random.rand(din, dout))\n",
    "        self.b = brainstate.ParamState(jnp.zeros((dout,)))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return x @ self.w.value + self.b.value\n",
    "\n",
    "\n",
    "class Count(brainstate.State):\n",
    "    \"\"\"Custom state type for tracking function calls.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class MLP(brainstate.nn.Module):\n",
    "    \"\"\"Multi-layer perceptron with call counting.\n",
    "    \n",
    "    Note: Now inherits from nn.Module instead of graph.Node.\n",
    "    This enables automatic state management.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, din, dhidden, dout):\n",
    "        super().__init__()  # Important: call parent __init__\n",
    "        \n",
    "        self.count = Count(jnp.array(0))\n",
    "        self.linear1 = Linear(din, dhidden)\n",
    "        self.linear2 = Linear(dhidden, dout)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.count.value += 1\n",
    "        x = self.linear1(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9d0e1f2a3",
   "metadata": {},
   "source": [
    "### Step 2: Create Model and Optimizer\n",
    "\n",
    "Now we create the model and use a **BrainTools optimizer** that automatically manages parameter states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0e1f2a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = MLP(din=1, dhidden=32, dout=1)\n",
    "\n",
    "# Create optimizer and register trainable weights\n",
    "optimizer = braintools.optim.SGD(lr=1e-3)\n",
    "optimizer.register_trainable_weights(model.states(brainstate.ParamState))\n",
    "\n",
    "print(\"Model created:\")\n",
    "print(model)\n",
    "print(\"\\nOptimizer registered with parameters:\")\n",
    "print(jax.tree.map(jnp.shape, optimizer.param_states.to_pytree()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1f2a3b4c5",
   "metadata": {},
   "source": [
    "**Key Differences from Functional API:**\n",
    "\n",
    "1. **No Manual State Splitting**: We don't need to call `treefy_split` - the model and optimizer manage states internally\n",
    "2. **Optimizer Handles Parameters**: The optimizer automatically tracks and updates parameters\n",
    "3. **Simpler Code**: Less boilerplate, more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4c5d6",
   "metadata": {},
   "source": [
    "## Training with Lifted Transforms\n",
    "\n",
    "### Understanding Lifted Transforms\n",
    "\n",
    "**Lifted transforms** automatically handle state management:\n",
    "- `brainstate.transform.jit`: JIT compilation with automatic state threading\n",
    "- `brainstate.transform.grad`: Gradients with automatic state differentiation\n",
    "- States are implicitly updated during function execution\n",
    "\n",
    "### Step 1: Define Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3b4c5d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@brainstate.transform.jit\n",
    "def train_step(batch):\n",
    "    \"\"\"Perform one training step.\n",
    "    \n",
    "    Notice how much simpler this is compared to the functional API!\n",
    "    No need to pass/return states explicitly.\n",
    "    \"\"\"\n",
    "    x, y = batch\n",
    "    \n",
    "    def loss_fn():\n",
    "        # Simply call the model - states are managed automatically\n",
    "        return jnp.mean((y - model(x)) ** 2)\n",
    "    \n",
    "    # Compute gradients\n",
    "    # The grad transform knows which states to differentiate\n",
    "    grads = brainstate.transform.grad(\n",
    "        loss_fn, \n",
    "        optimizer.param_states.to_pytree()\n",
    "    )()\n",
    "    \n",
    "    # Update parameters using the optimizer\n",
    "    optimizer.update(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4c5d6e7f8",
   "metadata": {},
   "source": [
    "**Compare with Functional API:**\n",
    "\n",
    "Functional API (previous tutorial):\n",
    "```python\n",
    "def train_step(params, counts, batch):\n",
    "    x, y = batch\n",
    "    def loss_fn(params):\n",
    "        model = brainstate.graph.treefy_merge(graphdef, params, counts)\n",
    "        y_pred = model(x)\n",
    "        new_counts = brainstate.graph.treefy_states(model, Count)\n",
    "        loss = jnp.mean((y - y_pred) ** 2)\n",
    "        return loss, new_counts\n",
    "    grad, counts = jax.grad(loss_fn, has_aux=True)(params)\n",
    "    params = jax.tree.map(lambda w, g: w - 0.1 * g, params, grad)\n",
    "    return params, counts\n",
    "```\n",
    "\n",
    "Lifted API (this tutorial):\n",
    "```python\n",
    "def train_step(batch):\n",
    "    x, y = batch\n",
    "    def loss_fn():\n",
    "        return jnp.mean((y - model(x)) ** 2)\n",
    "    grads = brainstate.transform.grad(loss_fn, optimizer.param_states.to_pytree())()\n",
    "    optimizer.update(grads)\n",
    "```\n",
    "\n",
    "**Much cleaner!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c5d6e7f8a9",
   "metadata": {},
   "source": [
    "### Step 2: Define Evaluation Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6e7f8a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@brainstate.transform.jit\n",
    "def eval_step(batch):\n",
    "    \"\"\"Evaluate the model on a batch.\"\"\"\n",
    "    x, y = batch\n",
    "    y_pred = model(x)\n",
    "    loss = jnp.mean((y - y_pred) ** 2)\n",
    "    return {'loss': loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e7f8a9b0c1",
   "metadata": {},
   "source": [
    "### Step 3: Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8a9b0c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "total_steps = 10_000\n",
    "\n",
    "# Training loop - notice how clean this is!\n",
    "print(\"Training the model...\\n\")\n",
    "for step, batch in enumerate(dataset(32)):\n",
    "    # Simply call train_step - no state management needed!\n",
    "    train_step(batch)\n",
    "    \n",
    "    # Log progress every 1000 steps\n",
    "    if step % 1000 == 0:\n",
    "        logs = eval_step((X, Y))\n",
    "        print(f\"Step: {step:5d}, Loss: {logs['loss']:.6f}\")\n",
    "    \n",
    "    # Stop after total_steps\n",
    "    if step >= total_steps - 1:\n",
    "        break\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a9b0c1d2e3",
   "metadata": {},
   "source": [
    "**Compare Training Loops:**\n",
    "\n",
    "Functional API:\n",
    "```python\n",
    "for step, batch in enumerate(dataset(32)):\n",
    "    params_, counts_ = train_step(params_, counts_, batch)  # Manual state threading\n",
    "    if step % 1000 == 0:\n",
    "        logs = eval_step(params_, counts_, (X, Y))  # Pass states explicitly\n",
    "```\n",
    "\n",
    "Lifted API:\n",
    "```python\n",
    "for step, batch in enumerate(dataset(32)):\n",
    "    train_step(batch)  # States handled automatically\n",
    "    if step % 1000 == 0:\n",
    "        logs = eval_step((X, Y))  # No state arguments needed\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b0c1d2e3f4",
   "metadata": {},
   "source": [
    "## Analyzing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c1d2e3f4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model already contains the trained parameters!\n",
    "# No need to merge states like in functional API\n",
    "\n",
    "print(f\"Total model calls during training: {model.count.value}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model(X)\n",
    "\n",
    "print(f\"Final predictions shape: {y_pred.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d2e3f4a5b6",
   "metadata": {},
   "source": [
    "### Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e3f4a5b6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot data points\n",
    "plt.scatter(X, Y, color='blue', alpha=0.5, label='Training Data', s=30)\n",
    "\n",
    "# Plot predictions\n",
    "plt.plot(X, y_pred, color='red', linewidth=2, label='Model Prediction')\n",
    "\n",
    "# Plot true function\n",
    "X_true = np.linspace(0, 1, 100)[:, None]\n",
    "Y_true = 0.8 * X_true ** 2 + 0.1\n",
    "plt.plot(X_true, Y_true, color='green', linewidth=2, \n",
    "         linestyle='--', label='True Function', alpha=0.7)\n",
    "\n",
    "plt.xlabel('X', fontsize=12)\n",
    "plt.ylabel('Y', fontsize=12)\n",
    "plt.title('Polynomial Regression with Lifted Transforms', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f4a5b6c7d8",
   "metadata": {},
   "source": [
    "## Advanced: Using Different Optimizers\n",
    "\n",
    "BrainTools provides many optimizers. Let's try Adam optimizer with learning rate scheduling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a5b6c7d8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model\n",
    "brainstate.random.seed(0)\n",
    "model_adam = MLP(din=1, dhidden=32, dout=1)\n",
    "\n",
    "# Create Adam optimizer with exponential decay learning rate\n",
    "lr_schedule = braintools.optim.ExponentialDecayLR(\n",
    "    lr=1e-2,           # Initial learning rate\n",
    "    decay_steps=100,   # Decay every 100 steps\n",
    "    decay_rate=0.99    # Multiply by 0.99 each decay\n",
    ")\n",
    "\n",
    "optimizer_adam = braintools.optim.Adam(lr=lr_schedule)\n",
    "optimizer_adam.register_trainable_weights(model_adam.states(brainstate.ParamState))\n",
    "\n",
    "print(\"Adam optimizer created with learning rate scheduling!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b6c7d8e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training step with the new optimizer\n",
    "@brainstate.transform.jit\n",
    "def train_step_adam(batch):\n",
    "    x, y = batch\n",
    "    \n",
    "    def loss_fn():\n",
    "        return jnp.mean((y - model_adam(x)) ** 2)\n",
    "    \n",
    "    grads = brainstate.transform.grad(\n",
    "        loss_fn, \n",
    "        optimizer_adam.param_states.to_pytree()\n",
    "    )()\n",
    "    \n",
    "    optimizer_adam.update(grads)\n",
    "\n",
    "@brainstate.transform.jit\n",
    "def eval_step_adam(batch):\n",
    "    x, y = batch\n",
    "    y_pred = model_adam(x)\n",
    "    loss = jnp.mean((y - y_pred) ** 2)\n",
    "    return {'loss': loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c7d8e9f0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with Adam\n",
    "total_steps = 5_000\n",
    "\n",
    "print(\"Training with Adam optimizer...\\n\")\n",
    "for step, batch in enumerate(dataset(32)):\n",
    "    train_step_adam(batch)\n",
    "    \n",
    "    if step % 500 == 0:\n",
    "        logs = eval_step_adam((X, Y))\n",
    "        current_lr = optimizer_adam.lr.value if hasattr(optimizer_adam.lr, 'value') else optimizer_adam.lr\n",
    "        print(f\"Step: {step:5d}, Loss: {logs['loss']:.6f}, LR: {current_lr:.6f}\")\n",
    "    \n",
    "    if step >= total_steps - 1:\n",
    "        break\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d8e9f0a1b2",
   "metadata": {},
   "source": [
    "## Comparison: Functional API vs Lifted Transforms\n",
    "\n",
    "| Aspect | Functional API | Lifted Transforms |\n",
    "|--------|----------------|-------------------|\n",
    "| **State Management** | Manual (explicit split/merge) | Automatic (implicit) |\n",
    "| **Code Complexity** | Higher (more boilerplate) | Lower (cleaner code) |\n",
    "| **Control** | Full control over states | Managed by framework |\n",
    "| **Learning Curve** | Steeper (need to understand graph ops) | Gentler (similar to PyTorch) |\n",
    "| **Use Cases** | Custom algorithms, research | Standard training, production |\n",
    "| **Performance** | Same (both compile to XLA) | Same (both compile to XLA) |\n",
    "| **Debugging** | Easier to trace states | Harder (implicit state flow) |\n",
    "| **Flexibility** | Maximum | High (sufficient for most cases) |\n",
    "\n",
    "### When to Use Each?\n",
    "\n",
    "**Use Functional API when:**\n",
    "- Implementing novel optimization algorithms\n",
    "- Need fine-grained control over state updates\n",
    "- Debugging complex state interactions\n",
    "- Research requiring custom gradient computations\n",
    "\n",
    "**Use Lifted Transforms when:**\n",
    "- Standard training workflows\n",
    "- Production deployments\n",
    "- Rapid prototyping\n",
    "- When code readability is priority"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e9f0a1b2c3",
   "metadata": {},
   "source": [
    "## Key Concepts Summary\n",
    "\n",
    "### Lifted Transforms\n",
    "\n",
    "1. **Automatic State Management**:\n",
    "   - States are implicitly threaded through computations\n",
    "   - No need for manual split/merge operations\n",
    "   - Cleaner, more readable code\n",
    "\n",
    "2. **BrainTools Optimizers**:\n",
    "   - Handle parameter registration and updates\n",
    "   - Support learning rate scheduling\n",
    "   - Implement various algorithms (SGD, Adam, AdamW, etc.)\n",
    "\n",
    "3. **BrainState Transforms**:\n",
    "   - `brainstate.transform.jit`: JIT compilation with state handling\n",
    "   - `brainstate.transform.grad`: Gradients with automatic state differentiation\n",
    "   - `brainstate.transform.for_loop`: Efficient loops over sequences\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Start with Lifted API**: Begin with lifted transforms for most projects\n",
    "2. **Use BrainTools Optimizers**: They handle state management for you\n",
    "3. **Drop to Functional API when needed**: For advanced control\n",
    "4. **Profile before optimizing**: Both APIs have same performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f0a1b2c3d4",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Try these exercises to deepen your understanding:\n",
    "\n",
    "1. **Experiment with Optimizers**:\n",
    "   - Try different optimizers (AdamW, RMSprop)\n",
    "   - Compare convergence speeds\n",
    "   - Visualize learning curves\n",
    "\n",
    "2. **Add Regularization**:\n",
    "   - Implement L2 regularization in the loss function\n",
    "   - Add dropout layers to the MLP\n",
    "   - Compare generalization performance\n",
    "\n",
    "3. **Learning Rate Scheduling**:\n",
    "   - Implement cosine annealing schedule\n",
    "   - Try step decay\n",
    "   - Plot learning rate vs. step\n",
    "\n",
    "4. **Gradient Clipping**:\n",
    "   - Add gradient clipping to prevent exploding gradients\n",
    "   - Monitor gradient norms during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a1b2c3d4e5",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you understand both functional and lifted APIs, you can:\n",
    "\n",
    "1. **Build Complex Models**: Apply these concepts to CNNs, RNNs, and Transformers\n",
    "2. **Explore Advanced Training**: Learn about mixed precision, gradient accumulation\n",
    "3. **Dive into Brain Models**: Use these techniques for spiking neural networks\n",
    "4. **Checkpointing and Deployment**: Save and load trained models\n",
    "\n",
    "## References\n",
    "\n",
    "- [BrainState Transform API](https://brainstate.readthedocs.io/en/latest/apis/transform.html)\n",
    "- [BrainTools Optimizers](https://braintools.readthedocs.io/en/latest/apis/optim.html)\n",
    "- [JAX Documentation](https://jax.readthedocs.io/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

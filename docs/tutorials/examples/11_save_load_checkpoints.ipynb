{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Saving and Loading Model Checkpoints\n",
    "\n",
    "This tutorial demonstrates how to save and load model checkpoints in BrainState, enabling you to:\n",
    "- Preserve trained models for later use\n",
    "- Resume interrupted training\n",
    "- Share models with collaborators\n",
    "- Deploy models in production\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "- Extract and save model states\n",
    "- Use Orbax for checkpointing\n",
    "- Load models with abstract initialization\n",
    "- Handle model versioning\n",
    "- Implement best practices for model persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import orbax.checkpoint as orbax  # Google's checkpointing library\n",
    "\n",
    "import brainstate\n",
    "\n",
    "# Set random seed\n",
    "brainstate.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_section",
   "metadata": {},
   "source": [
    "## Building a Simple Model\n",
    "\n",
    "Let's create a multi-layer perceptron (MLP) for demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(brainstate.nn.Module):\n",
    "    \"\"\"Multi-layer perceptron for classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, din: int, dmid: int, dout: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Two dense layers\n",
    "        self.dense1 = brainstate.nn.Linear(din, dmid)\n",
    "        self.dense2 = brainstate.nn.Linear(dmid, dout)\n",
    "    \n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        \"\"\"Forward pass through the network.\"\"\"\n",
    "        x = self.dense1(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "factory_section",
   "metadata": {},
   "source": [
    "## Model Factory Function\n",
    "\n",
    "It's good practice to define a factory function that creates models with consistent parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "factory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(seed: int) -> MLP:\n",
    "    \"\"\"Create an MLP model with a given random seed.\n",
    "    \n",
    "    Args:\n",
    "        seed: Random seed for initialization\n",
    "        \n",
    "    Returns:\n",
    "        model: Initialized MLP\n",
    "    \"\"\"\n",
    "    brainstate.random.seed(seed)\n",
    "    return MLP(din=10, dmid=20, dout=30)\n",
    "\n",
    "\n",
    "# Create and inspect a model\n",
    "test_model = create_model(42)\n",
    "print(\"Model created:\")\n",
    "print(test_model)\n",
    "print(f\"\\nParameter shapes:\")\n",
    "for name, param in test_model.states(brainstate.ParamState).to_dict_values().items():\n",
    "    print(f\"  {name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_section",
   "metadata": {},
   "source": [
    "## Saving Model Checkpoints\n",
    "\n",
    "### Method 1: Using Orbax (Recommended)\n",
    "\n",
    "Orbax is Google's official checkpointing library for JAX. It provides:\n",
    "- Efficient serialization\n",
    "- Asynchronous saving\n",
    "- Version management\n",
    "- Cloud storage support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_orbax",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_orbax(model: MLP, path: str):\n",
    "    \"\"\"Save model parameters using Orbax.\n",
    "    \n",
    "    Args:\n",
    "        model: Model to save\n",
    "        path: Directory path for checkpoint\n",
    "    \"\"\"\n",
    "    # Extract all states as a pytree\n",
    "    state_tree = brainstate.graph.treefy_states(model)\n",
    "    \n",
    "    # Create Orbax checkpointer\n",
    "    checkpointer = orbax.PyTreeCheckpointer()\n",
    "    \n",
    "    # Save the state tree\n",
    "    checkpoint_path = os.path.join(path, 'state')\n",
    "    checkpointer.save(checkpoint_path, state_tree)\n",
    "    \n",
    "    print(f\"Model saved to {checkpoint_path}\")\n",
    "\n",
    "\n",
    "# Example: Save a model\n",
    "with TemporaryDirectory() as tmpdir:\n",
    "    model = create_model(seed=42)\n",
    "    save_model_orbax(model, tmpdir)\n",
    "    \n",
    "    # Check what was saved\n",
    "    print(f\"\\nFiles in checkpoint directory:\")\n",
    "    for root, dirs, files in os.walk(tmpdir):\n",
    "        for file in files:\n",
    "            filepath = os.path.join(root, file)\n",
    "            size = os.path.getsize(filepath)\n",
    "            print(f\"  {file}: {size:,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_section",
   "metadata": {},
   "source": [
    "## Loading Model Checkpoints\n",
    "\n",
    "### Abstract Initialization\n",
    "\n",
    "BrainState provides `abstract_init` to create models with abstract shapes (no actual arrays), which is memory-efficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_orbax",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_orbax(path: str) -> MLP:\n",
    "    \"\"\"Load model parameters using Orbax.\n",
    "    \n",
    "    Args:\n",
    "        path: Directory path for checkpoint\n",
    "        \n",
    "    Returns:\n",
    "        model: Loaded model with restored parameters\n",
    "    \"\"\"\n",
    "    # Create model with abstract shapes (memory efficient)\n",
    "    model = brainstate.transform.abstract_init(lambda: create_model(0))\n",
    "    \n",
    "    # Get state tree structure\n",
    "    state_tree = brainstate.graph.treefy_states(model)\n",
    "    \n",
    "    # Load parameters from checkpoint\n",
    "    checkpointer = orbax.PyTreeCheckpointer()\n",
    "    checkpoint_path = os.path.join(path, 'state')\n",
    "    restored_state = checkpointer.restore(checkpoint_path, item=state_tree)\n",
    "    \n",
    "    # Update model with loaded parameters\n",
    "    brainstate.graph.update_states(model, restored_state)\n",
    "    \n",
    "    print(f\"Model loaded from {checkpoint_path}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Example: Save and load cycle\n",
    "with TemporaryDirectory() as tmpdir:\n",
    "    # Save a model\n",
    "    print(\"=== Saving Model ===\")\n",
    "    original_model = create_model(seed=42)\n",
    "    save_model_orbax(original_model, tmpdir)\n",
    "    \n",
    "    # Load the model\n",
    "    print(\"\\n=== Loading Model ===\")\n",
    "    loaded_model = load_model_orbax(tmpdir)\n",
    "    \n",
    "    # Verify parameters match\n",
    "    print(\"\\n=== Verification ===\")\n",
    "    original_params = brainstate.graph.treefy_states(original_model, brainstate.ParamState)\n",
    "    loaded_params = brainstate.graph.treefy_states(loaded_model, brainstate.ParamState)\n",
    "    \n",
    "    # Check if parameters are identical\n",
    "    params_match = jax.tree.map(\n",
    "        lambda x, y: jnp.allclose(x, y),\n",
    "        original_params.to_dict_values(),\n",
    "        loaded_params.to_dict_values()\n",
    "    )\n",
    "    \n",
    "    all_match = all(jax.tree.leaves(params_match))\n",
    "    print(f\"Parameters match: {all_match}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inference_section",
   "metadata": {},
   "source": [
    "## Testing the Loaded Model\n",
    "\n",
    "Verify that the loaded model produces identical outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_loaded",
   "metadata": {},
   "outputs": [],
   "source": [
    "with TemporaryDirectory() as tmpdir:\n",
    "    # Create and save a model\n",
    "    original_model = create_model(seed=42)\n",
    "    save_model_orbax(original_model, tmpdir)\n",
    "    \n",
    "    # Load the model\n",
    "    loaded_model = load_model_orbax(tmpdir)\n",
    "    \n",
    "    # Test with random input\n",
    "    test_input = jnp.ones((5, 10))  # Batch of 5 samples\n",
    "    \n",
    "    original_output = original_model(test_input)\n",
    "    loaded_output = loaded_model(test_input)\n",
    "    \n",
    "    print(\"\\n=== Model Inference Test ===\")\n",
    "    print(f\"Original output shape: {original_output.shape}\")\n",
    "    print(f\"Loaded output shape: {loaded_output.shape}\")\n",
    "    print(f\"\\nOutputs match: {jnp.allclose(original_output, loaded_output)}\")\n",
    "    print(f\"Max difference: {jnp.max(jnp.abs(original_output - loaded_output)):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced_section",
   "metadata": {},
   "source": [
    "## Advanced: Selective State Saving\n",
    "\n",
    "Sometimes you want to save only specific types of states (e.g., only parameters, not optimizer states):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective_save",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_params_only(model: MLP, path: str):\n",
    "    \"\"\"Save only trainable parameters.\n",
    "    \n",
    "    Args:\n",
    "        model: Model to save\n",
    "        path: Directory path for checkpoint\n",
    "    \"\"\"\n",
    "    # Extract only ParamState instances\n",
    "    param_tree = brainstate.graph.treefy_states(model, brainstate.ParamState)\n",
    "    \n",
    "    checkpointer = orbax.PyTreeCheckpointer()\n",
    "    checkpoint_path = os.path.join(path, 'params')\n",
    "    checkpointer.save(checkpoint_path, param_tree)\n",
    "    \n",
    "    print(f\"Parameters saved to {checkpoint_path}\")\n",
    "\n",
    "\n",
    "def load_params_only(model: MLP, path: str):\n",
    "    \"\"\"Load only trainable parameters.\n",
    "    \n",
    "    Args:\n",
    "        model: Model to update\n",
    "        path: Directory path for checkpoint\n",
    "    \"\"\"\n",
    "    # Get parameter structure\n",
    "    param_tree = brainstate.graph.treefy_states(model, brainstate.ParamState)\n",
    "    \n",
    "    checkpointer = orbax.PyTreeCheckpointer()\n",
    "    checkpoint_path = os.path.join(path, 'params')\n",
    "    restored_params = checkpointer.restore(checkpoint_path, item=param_tree)\n",
    "    \n",
    "    # Update only parameters\n",
    "    brainstate.graph.update_states(model, restored_params)\n",
    "    \n",
    "    print(f\"Parameters loaded from {checkpoint_path}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "with TemporaryDirectory() as tmpdir:\n",
    "    # Save only parameters\n",
    "    model = create_model(42)\n",
    "    save_params_only(model, tmpdir)\n",
    "    \n",
    "    # Create new model and load parameters\n",
    "    new_model = create_model(0)  # Different initialization\n",
    "    load_params_only(new_model, tmpdir)\n",
    "    \n",
    "    print(\"\\nParameter transfer successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "best_practices",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### 1. Version Your Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "versioning",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_with_metadata(model: MLP, path: str, metadata: dict):\n",
    "    \"\"\"Save model with metadata.\n",
    "    \n",
    "    Args:\n",
    "        model: Model to save\n",
    "        path: Directory path\n",
    "        metadata: Dictionary with version info, hyperparameters, etc.\n",
    "    \"\"\"\n",
    "    # Save model states\n",
    "    save_model_orbax(model, path)\n",
    "    \n",
    "    # Save metadata as JSON\n",
    "    metadata_path = os.path.join(path, 'metadata.json')\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"Metadata saved to {metadata_path}\")\n",
    "\n",
    "\n",
    "# Example with metadata\n",
    "with TemporaryDirectory() as tmpdir:\n",
    "    model = create_model(42)\n",
    "    \n",
    "    metadata = {\n",
    "        'model_version': '1.0.0',\n",
    "        'architecture': 'MLP',\n",
    "        'input_dim': 10,\n",
    "        'hidden_dim': 20,\n",
    "        'output_dim': 30,\n",
    "        'training_date': '2025-10-10',\n",
    "        'framework': 'brainstate',\n",
    "    }\n",
    "    \n",
    "    save_with_metadata(model, tmpdir, metadata)\n",
    "    \n",
    "    # Load and check metadata\n",
    "    metadata_path = os.path.join(tmpdir, 'metadata.json')\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        loaded_metadata = json.load(f)\n",
    "    \n",
    "    print(\"\\nLoaded metadata:\")\n",
    "    print(json.dumps(loaded_metadata, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic_section",
   "metadata": {},
   "source": [
    "### 2. Periodic Checkpointing During Training\n",
    "\n",
    "Example of saving checkpoints during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_checkpoints(\n",
    "    model: MLP,\n",
    "    checkpoint_dir: str,\n",
    "    num_epochs: int = 10,\n",
    "    checkpoint_every: int = 5\n",
    "):\n",
    "    \"\"\"Training loop with periodic checkpointing.\n",
    "    \n",
    "    Args:\n",
    "        model: Model to train\n",
    "        checkpoint_dir: Directory for checkpoints\n",
    "        num_epochs: Number of training epochs\n",
    "        checkpoint_every: Save checkpoint every N epochs\n",
    "    \"\"\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Simulate training\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        \n",
    "        # Save checkpoint periodically\n",
    "        if (epoch + 1) % checkpoint_every == 0:\n",
    "            epoch_dir = os.path.join(checkpoint_dir, f'epoch_{epoch + 1}')\n",
    "            save_model_orbax(model, epoch_dir)\n",
    "            print(f\"  Checkpoint saved!\\n\")\n",
    "\n",
    "\n",
    "# Example (simulated)\n",
    "with TemporaryDirectory() as tmpdir:\n",
    "    model = create_model(42)\n",
    "    print(\"Training with periodic checkpoints:\\n\")\n",
    "    train_with_checkpoints(model, tmpdir, num_epochs=10, checkpoint_every=3)\n",
    "    \n",
    "    # List saved checkpoints\n",
    "    print(\"\\nSaved checkpoints:\")\n",
    "    for item in sorted(os.listdir(tmpdir)):\n",
    "        if os.path.isdir(os.path.join(tmpdir, item)):\n",
    "            print(f\"  {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Key Concepts Summary\n",
    "\n",
    "### Checkpointing Workflow\n",
    "\n",
    "1. **Save**:\n",
    "   - Extract states with `treefy_states`\n",
    "   - Serialize with Orbax checkpointer\n",
    "   - Include metadata for versioning\n",
    "\n",
    "2. **Load**:\n",
    "   - Create model with `abstract_init` (memory efficient)\n",
    "   - Restore states from checkpoint\n",
    "   - Update model with `update_states`\n",
    "\n",
    "### Key Functions\n",
    "\n",
    "- `brainstate.graph.treefy_states`: Extract states as pytree\n",
    "- `brainstate.graph.update_states`: Update model with new states\n",
    "- `brainstate.transform.abstract_init`: Create model with abstract shapes\n",
    "- `orbax.PyTreeCheckpointer`: Efficient serialization\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Use Orbax**: Industry-standard, efficient, cloud-compatible\n",
    "2. **Version Control**: Save metadata with checkpoints\n",
    "3. **Periodic Saving**: Checkpoint during long training runs\n",
    "4. **Test Loading**: Always verify loaded models work correctly\n",
    "5. **Selective Saving**: Save only what you need (params vs. all states)\n",
    "\n",
    "### Common Use Cases\n",
    "\n",
    "- **Resume Training**: Load last checkpoint after interruption\n",
    "- **Model Deployment**: Save trained model for production\n",
    "- **Experimentation**: Save multiple versions for comparison\n",
    "- **Transfer Learning**: Load pre-trained weights\n",
    "- **Ensembling**: Save multiple models for ensemble predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Implement Early Stopping**:\n",
    "   - Save checkpoint only when validation loss improves\n",
    "   - Keep only the best checkpoint\n",
    "\n",
    "2. **Checkpoint Manager**:\n",
    "   - Create a class to manage multiple checkpoints\n",
    "   - Implement automatic cleanup of old checkpoints\n",
    "   - Add rollback functionality\n",
    "\n",
    "3. **Cloud Storage**:\n",
    "   - Use Orbax with Google Cloud Storage\n",
    "   - Implement automatic backup\n",
    "\n",
    "4. **Model Zoo**:\n",
    "   - Create a library of pre-trained models\n",
    "   - Implement model registry\n",
    "   - Add download functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next_steps",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **Distributed Training**: Checkpointing across multiple devices\n",
    "- **Model Quantization**: Save compressed models\n",
    "- **ONNX Export**: Convert to ONNX format\n",
    "- **Serving**: Deploy models with TensorFlow Serving or TorchServe\n",
    "\n",
    "## References\n",
    "\n",
    "- [Orbax Documentation](https://orbax.readthedocs.io/)\n",
    "- [JAX Checkpointing Guide](https://jax.readthedocs.io/en/latest/notebooks/thinking_in_jax.html)\n",
    "- [BrainState Graph API](https://brainstate.readthedocs.io/en/latest/apis/graph.html)\n",
    "- [Model Versioning Best Practices](https://neptune.ai/blog/version-control-for-ml-models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

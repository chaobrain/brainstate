{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e91a13480dd82aa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Training Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b8e8f3",
   "metadata": {},
   "source": [
    "In recent years, artificial neural networks have developed rapidly and play an important role in neuroscience research. As a high-performance computational framework for brain dynamics modeling, brainstate also supports the training of artificial neural networks, facilitating the integration of neural dynamics models with artificial neural networks.\n",
    "\n",
    "Here, we will introduce how to train an artificial neural network using brainstate, with an example of a simple 2-layer multilayer perceptron (MLP) for handwritten digit recognition (MNIST)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d512371",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T10:15:13.234279Z",
     "start_time": "2025-10-11T10:15:09.021121Z"
    }
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "import braintools \n",
    "import brainstate \n",
    "from braintools.metric import softmax_cross_entropy_with_integer_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd228991",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T10:15:13.245047Z",
     "start_time": "2025-10-11T10:15:13.237285Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.3'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brainstate.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9889df14",
   "metadata": {},
   "source": [
    "## Preparing the Dataset\n",
    "\n",
    "First, we need to obtain the dataset and wrap it into an iterable object that automatically samples and shuffles the data according to the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01e96ac0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T10:15:38.210421Z",
     "start_time": "2025-10-11T10:15:13.259009Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset('mnist')\n",
    "X_train = np.array(np.stack(dataset['train']['image']), dtype=np.uint8)\n",
    "X_test = np.array(np.stack(dataset['test']['image']), dtype=np.uint8)\n",
    "X_train = (X_train > 0).astype(jnp.float32)\n",
    "X_test = (X_test > 0).astype(jnp.float32)\n",
    "Y_train = np.array(dataset['train']['label'], dtype=np.int32)\n",
    "Y_test = np.array(dataset['test']['label'], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b886cbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T10:15:40.217618Z",
     "start_time": "2025-10-11T10:15:40.211580Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, X, Y, batch_size, shuffle=True):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(X))\n",
    "        self.current_index = 0\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.current_index = 0\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        # Check if all samples have been processed\n",
    "        if self.current_index >= len(self.X):\n",
    "            raise StopIteration\n",
    "\n",
    "        # Define the start and end of the current batch\n",
    "        start = self.current_index\n",
    "        end = start + self.batch_size\n",
    "        if end > len(self.X):\n",
    "            end = len(self.X)\n",
    "        \n",
    "        # Update current index\n",
    "        self.current_index = end\n",
    "\n",
    "        # Select batch samples\n",
    "        batch_indices = self.indices[start:end]\n",
    "        batch_X = self.X[batch_indices]\n",
    "        batch_Y = self.Y[batch_indices]\n",
    "\n",
    "        # Ensure batch has consistent shape\n",
    "        if batch_X.ndim == 1:\n",
    "            batch_X = np.expand_dims(batch_X, axis=0)\n",
    "\n",
    "        return batch_X, batch_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6c383f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T10:15:40.225527Z",
     "start_time": "2025-10-11T10:15:40.217618Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize training and testing datasets\n",
    "batch_size = 32\n",
    "train_dataset = Dataset(X_train, Y_train, batch_size, shuffle=True)\n",
    "test_dataset = Dataset(X_test, Y_test, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba1b50a",
   "metadata": {},
   "source": [
    "## Defining the Artificial Neural Network\n",
    "\n",
    "When defining an artificial neural network in brainstate, you need to inherit the base class ``brainstate.nn.Module``. In the class method ``__init__()``, define the layers in the network (make sure to initialize the base class first using ``super().__init__()``). In the class method ``__call__()``, define the forward pass method of the network.\n",
    "\n",
    "brainstate also supports defining operations for individual layers in the network. For these custom layers, you need to inherit from the base class ``brainstate.nn.Module``, similar to defining a network.\n",
    "\n",
    "All quantities that need to change in the model should be encapsulated in the ``State`` object. Parameters that need to be updated during training should be encapsulated in a subclass of ``State`` called ``ParamState``. Other quantities that need to be updated during training are encapsulated in another subclass of ``State`` called ``ShortTermState``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cea8b5d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T10:15:40.232020Z",
     "start_time": "2025-10-11T10:15:40.225527Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define linear layer\n",
    "class Linear(brainstate.nn.Module):\n",
    "  def __init__(self, din: int, dout: int):\n",
    "    super().__init__()\n",
    "    self.w = brainstate.ParamState(brainstate.random.rand(din, dout))  # Initialize weight parameters\n",
    "    self.b = brainstate.ParamState(jnp.zeros((dout,)))  # Initialize bias parameters\n",
    "\n",
    "  def __call__(self, x):\n",
    "    return x @ self.w.value + self.b.value    # Perform linear transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e541917",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T10:15:40.238526Z",
     "start_time": "2025-10-11T10:15:40.235377Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a short-term state for counting times called\n",
    "class Count(brainstate.ShortTermState):\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9445bd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T10:15:40.245851Z",
     "start_time": "2025-10-11T10:15:40.238526Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define MLP model\n",
    "class MLP(brainstate.nn.Module):\n",
    "  def __init__(self, din, dhidden, dout):\n",
    "    super().__init__()\n",
    "    self.count = Count(jnp.array(0))    # Count how many times model is called\n",
    "    self.linear1 = Linear(din, dhidden)          # brainstate有常规层的实现，可以直接写 self.linear1 = brainstate.nn.Linear(din, dhidden)\n",
    "    self.linear2 = Linear(dhidden, dout)\n",
    "    self.flatten = brainstate.nn.Flatten(start_axis=1)   # Flatten images to 1D\n",
    "    self.relu = brainstate.nn.ReLU()   # ReLU activation function\n",
    "\n",
    "  def __call__(self, x):\n",
    "    self.count.value += 1   # Increment call count\n",
    "\n",
    "    x = self.flatten(x)\n",
    "    x = self.linear1(x)\n",
    "    x = self.relu(x)      # 也兼容jax函数，可以直接写 x = jax.nn.relu(x)\n",
    "    x = self.linear2(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97aefeaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T10:15:40.811047Z",
     "start_time": "2025-10-11T10:15:40.245851Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize model with input, hidden, and output layer sizes\n",
    "model = MLP(din=28*28, dhidden=512, dout=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7edf945",
   "metadata": {},
   "source": [
    "## Optimizer Setup\n",
    "\n",
    "``braintools.optim`` provides various optimizers to choose from.\n",
    "\n",
    "After instantiating the optimizer, you need to specify which parameters the optimizer should update by calling ``optimizer.register_trainable_weights()``.\n",
    "\n",
    "In this case, we use ``brainstate.nn.Module.states()`` to collect all the ``State`` objects of the network nodes and their sub-nodes in the model. We restrict the types of ``State`` collected to ``brainstate.ParamState`` (in this model, ``State`` instances may also have other types like ``Count``, which do not need to be updated by the optimizer, so we apply type restrictions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8dd079f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T10:15:40.829346Z",
     "start_time": "2025-10-11T10:15:40.811047Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD(\n",
       "  momentum=0.0,\n",
       "  nesterov=False,\n",
       "  param_states=<braintools.optim.UniqueStateManager object at 0x000001A3AF83CF50>,\n",
       "  weight_decay=0.0,\n",
       "  grad_clip_norm=None,\n",
       "  grad_clip_value=None,\n",
       "  step_count=OptimState(\n",
       "    value=ShapedArray(int32[], weak_type=True)\n",
       "  ),\n",
       "  param_groups=[\n",
       "    {\n",
       "      'params': {\n",
       "        ('linear1', 'b'): ParamState(\n",
       "          value=ShapedArray(float32[512])\n",
       "        ),\n",
       "        ('linear1', 'w'): ParamState(\n",
       "          value=ShapedArray(float32[784,512])\n",
       "        ),\n",
       "        ('linear2', 'b'): ParamState(\n",
       "          value=ShapedArray(float32[10])\n",
       "        ),\n",
       "        ('linear2', 'w'): ParamState(\n",
       "          value=ShapedArray(float32[512,10])\n",
       "        )\n",
       "      },\n",
       "      'lr': OptimState(\n",
       "        value=ShapedArray(float32[], weak_type=True)\n",
       "      ),\n",
       "      'weight_decay': 0.0\n",
       "    }\n",
       "  ],\n",
       "  param_groups_opt_states=[],\n",
       "  _schedulers=[],\n",
       "  _lr_scheduler=<braintools.optim.ConstantLR object at 0x000001A3AF83EC90>,\n",
       "  _base_lr=0.001,\n",
       "  _current_lr=OptimState(...),\n",
       "  tx=GradientTransformationExtraArgs(init=<function chain.<locals>.init_fn at 0x000001A3AE36CB80>, update=<function chain.<locals>.update_fn at 0x000001A3AE36C860>),\n",
       "  opt_state=OptimState(\n",
       "    value=(ScaleByScheduleState(count=ShapedArray(int32[])),)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize optimizer and register model parameters\n",
    "optimizer = braintools.optim.SGD(lr = 1e-3)   # Initialize SGD optimizer with learning rate\n",
    "optimizer.register_trainable_weights(model.states(brainstate.ParamState))   # Register parameters for optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbbfad2",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "During model training, use the ``brainstate.transform.grad`` function to calculate gradients. This function requires the loss function and the parameters (``State``) for which gradients should be computed.\n",
    "\n",
    "Then, the gradients are passed to the previously defined optimizer via ``update()`` for the update.\n",
    "\n",
    "To improve computational efficiency and performance, use the ``brainstate.transform.jit`` function to decorate the training step function, enabling just-in-time compilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99272b18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T10:15:40.839065Z",
     "start_time": "2025-10-11T10:15:40.834479Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training step function\n",
    "@brainstate.transform.jit\n",
    "def train_step(batch):\n",
    "  x, y = batch\n",
    "  # Define loss function\n",
    "  def loss_fn():\n",
    "    return softmax_cross_entropy_with_integer_labels(model(x), y).mean()\n",
    "  \n",
    "  # Compute gradients of the loss with respect to model parameters\n",
    "  grads = brainstate.transform.grad(loss_fn, model.states(brainstate.ParamState))()\n",
    "  optimizer.update(grads)   # Update parameters using optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c2dfe0",
   "metadata": {},
   "source": [
    "## Model Testing\n",
    "\n",
    "Similarly, use the ``brainstate.transform.jit`` function to decorate the testing step function, allowing for just-in-time compilation to improve computational efficiency and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7456afa1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T10:15:40.876889Z",
     "start_time": "2025-10-11T10:15:40.872151Z"
    }
   },
   "outputs": [],
   "source": [
    "# Testing step function\n",
    "@brainstate.transform.jit\n",
    "def test_step(batch):\n",
    "  x, y = batch\n",
    "  y_pred = model(x)   # Perform forward pass\n",
    "  loss = softmax_cross_entropy_with_integer_labels(y_pred, y).mean()   # Compute loss\n",
    "  correct = (y_pred.argmax(1) == y).sum()   # Count correct predictions\n",
    "\n",
    "  return {'loss': loss, 'correct': correct}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a550e569",
   "metadata": {},
   "source": [
    "## Training Process\n",
    "\n",
    "This completes the setup and the process for training an artificial neural network with brainstate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eae5f682",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T10:16:17.389423Z",
     "start_time": "2025-10-11T10:15:40.899228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, test loss: 448.7932434082031, test accuracy: 0.26989999413490295\n",
      "epoch: 1, test loss: 150.93972778320312, test accuracy: 0.6607000231742859\n",
      "epoch: 2, test loss: 105.59516906738281, test accuracy: 0.7075999975204468\n",
      "epoch: 3, test loss: 80.24203491210938, test accuracy: 0.7462999820709229\n",
      "epoch: 4, test loss: 120.68618774414062, test accuracy: 0.7113999724388123\n",
      "epoch: 5, test loss: 39.29928207397461, test accuracy: 0.8547999858856201\n",
      "epoch: 6, test loss: 89.67916107177734, test accuracy: 0.7944999933242798\n",
      "epoch: 7, test loss: 53.42087173461914, test accuracy: 0.8274000287055969\n",
      "epoch: 8, test loss: 35.70460510253906, test accuracy: 0.8694000244140625\n",
      "epoch: 9, test loss: 37.64791488647461, test accuracy: 0.8648999929428101\n",
      "times model called: 21880\n"
     ]
    }
   ],
   "source": [
    "# Execute training and testing\n",
    "total_steps = 20\n",
    "for epoch in range(10):\n",
    "  for step, batch in enumerate(train_dataset):\n",
    "    train_step(batch)   # Perform training step for each batch\n",
    "\n",
    "  # Calculate test loss and accuracy\n",
    "  test_loss, correct = 0, 0\n",
    "  for step_, test_ in enumerate(test_dataset):\n",
    "    logs = test_step(test_)\n",
    "    test_loss += logs['loss']\n",
    "    correct += logs['correct']\n",
    "    test_loss += logs['loss']\n",
    "  test_loss = test_loss / (step_ + 1)\n",
    "  test_accuracy = correct / len(X_test)\n",
    "  print(f\"epoch: {epoch}, test loss: {test_loss}, test accuracy: {test_accuracy}\")\n",
    "\n",
    "print('times model called:', model.count.value)   # Output number of model calls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ecosystem-py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

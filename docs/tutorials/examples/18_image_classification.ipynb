{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 18: Image Classification with CNNs\n",
    "\n",
    "In this tutorial, we'll build a complete image classification system using Convolutional Neural Networks (CNNs) in BrainState.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "- Build CNN architectures for image classification\n",
    "- Load and preprocess image datasets (MNIST-like)\n",
    "- Implement complete training loops\n",
    "- Evaluate model performance\n",
    "- Visualize results and predictions\n",
    "- Apply data augmentation\n",
    "- Monitor training progress\n",
    "\n",
    "## What We'll Build\n",
    "\n",
    "We'll create:\n",
    "- A CNN architecture from scratch\n",
    "- A training pipeline with validation\n",
    "- Evaluation metrics and visualization\n",
    "- A complete end-to-end workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brainstate as bst\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Tuple, Dict, List\n",
    "import time\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "bst.random.seed(42)\n",
    "\n",
    "print(f\"JAX devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Preparation\n",
    "\n",
    "We'll create synthetic MNIST-like data for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_mnist(n_samples: int = 1000, img_size: int = 28, n_classes: int = 10):\n",
    "    \"\"\"Generate synthetic MNIST-like dataset.\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Number of samples to generate\n",
    "        img_size: Image size (height and width)\n",
    "        n_classes: Number of classes\n",
    "        \n",
    "    Returns:\n",
    "        images: Array of shape (n_samples, img_size, img_size, 1)\n",
    "        labels: Array of shape (n_samples,)\n",
    "    \"\"\"\n",
    "    # Generate random images with patterns\n",
    "    images = bst.random.randn(n_samples, img_size, img_size, 1) * 0.3\n",
    "    labels = bst.random.randint(0, n_classes, (n_samples,))\n",
    "    \n",
    "    # Add class-specific patterns\n",
    "    for i in range(n_samples):\n",
    "        label = int(labels[i])\n",
    "        # Add horizontal lines for even classes\n",
    "        if label % 2 == 0:\n",
    "            for j in range(0, img_size, 4):\n",
    "                images = images.at[i, j:j+2, :, 0].set(\n",
    "                    images[i, j:j+2, :, 0] + 0.5\n",
    "                )\n",
    "        # Add vertical lines for odd classes\n",
    "        else:\n",
    "            for j in range(0, img_size, 4):\n",
    "                images = images.at[i, :, j:j+2, 0].set(\n",
    "                    images[i, :, j:j+2, 0] + 0.5\n",
    "                )\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    images = (images - images.min()) / (images.max() - images.min())\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "# Generate datasets\n",
    "print(\"Generating synthetic dataset...\")\n",
    "X_train, y_train = generate_synthetic_mnist(n_samples=2000)\n",
    "X_val, y_val = generate_synthetic_mnist(n_samples=400)\n",
    "X_test, y_test = generate_synthetic_mnist(n_samples=400)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(10):\n",
    "    axes[i].imshow(X_train[i, :, :, 0], cmap='gray')\n",
    "    axes[i].set_title(f'Label: {y_train[i]}')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Sample Images from Training Set')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(bst.graph.Node):\n",
    "    \"\"\"Convolutional block with Conv -> BatchNorm -> ReLU -> Pool.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, pool_size=2):\n",
    "        super().__init__()\n",
    "        self.conv = bst.nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=(kernel_size, kernel_size),\n",
    "            padding='SAME'\n",
    "        )\n",
    "        self.bn = bst.nn.BatchNorm2d(out_channels)\n",
    "        self.pool_size = pool_size\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        # Manual pooling using jax.lax\n",
    "        x = jax.lax.reduce_window(\n",
    "            x, \n",
    "            -jnp.inf,\n",
    "            jax.lax.max,\n",
    "            (1, self.pool_size, self.pool_size, 1),\n",
    "            (1, self.pool_size, self.pool_size, 1),\n",
    "            'VALID'\n",
    "        )\n",
    "        return x\n",
    "\n",
    "class ImageClassifier(bst.graph.Node):\n",
    "    \"\"\"CNN for image classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = ConvBlock(in_channels=1, out_channels=32, kernel_size=3)\n",
    "        self.conv2 = ConvBlock(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        # After 2 pooling layers: 28 -> 14 -> 7\n",
    "        self.fc1 = bst.nn.Linear(64 * 7 * 7, 128)\n",
    "        self.dropout = bst.nn.Dropout(0.5)\n",
    "        self.fc2 = bst.nn.Linear(128, num_classes)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Convolutional layers\n",
    "        x = self.conv1(x)  # (batch, 14, 14, 32)\n",
    "        x = self.conv2(x)  # (batch, 7, 7, 64)\n",
    "        \n",
    "        # Flatten\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.reshape(batch_size, -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "model = ImageClassifier(num_classes=10)\n",
    "\n",
    "# Test forward pass\n",
    "test_batch = X_train[:4]\n",
    "with bst.environ.context(fit=True):\n",
    "    test_output = model(test_batch)\n",
    "\n",
    "print(f\"Input shape: {test_batch.shape}\")\n",
    "print(f\"Output shape: {test_output.shape}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.value.size for p in model.states(bst.ParamState).values())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(logits, labels):\n",
    "    \"\"\"Compute cross-entropy loss.\n",
    "    \n",
    "    Args:\n",
    "        logits: Model outputs of shape (batch_size, num_classes)\n",
    "        labels: True labels of shape (batch_size,)\n",
    "        \n",
    "    Returns:\n",
    "        Scalar loss value\n",
    "    \"\"\"\n",
    "    # Convert labels to one-hot\n",
    "    num_classes = logits.shape[-1]\n",
    "    one_hot_labels = jax.nn.one_hot(labels, num_classes)\n",
    "    \n",
    "    # Compute log softmax\n",
    "    log_probs = jax.nn.log_softmax(logits, axis=-1)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = -jnp.mean(jnp.sum(one_hot_labels * log_probs, axis=-1))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def accuracy(logits, labels):\n",
    "    \"\"\"Compute classification accuracy.\n",
    "    \n",
    "    Args:\n",
    "        logits: Model outputs of shape (batch_size, num_classes)\n",
    "        labels: True labels of shape (batch_size,)\n",
    "        \n",
    "    Returns:\n",
    "        Accuracy as a float\n",
    "    \"\"\"\n",
    "    predictions = jnp.argmax(logits, axis=-1)\n",
    "    return jnp.mean(predictions == labels)\n",
    "\n",
    "# Test loss and accuracy\n",
    "test_loss = cross_entropy_loss(test_output, y_train[:4])\n",
    "test_acc = accuracy(test_output, y_train[:4])\n",
    "\n",
    "print(f\"Test loss: {test_loss:.4f}\")\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, x_batch, y_batch, learning_rate=0.001):\n",
    "    \"\"\"Perform one training step.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to train\n",
    "        x_batch: Input batch\n",
    "        y_batch: Label batch\n",
    "        learning_rate: Learning rate\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with loss and accuracy\n",
    "    \"\"\"\n",
    "    with bst.environ.context(fit=True):\n",
    "        # Define loss function\n",
    "        def loss_fn():\n",
    "            logits = model(x_batch)\n",
    "            return cross_entropy_loss(logits, y_batch)\n",
    "        \n",
    "        # Compute loss and gradients\n",
    "        loss, grads = bst.augment.grad(\n",
    "            loss_fn,\n",
    "            model.states(bst.ParamState),\n",
    "            return_value=True\n",
    "        )()\n",
    "        \n",
    "        # Update parameters\n",
    "        for name, grad in grads.items():\n",
    "            model.states()[name].value -= learning_rate * grad\n",
    "        \n",
    "        # Compute accuracy\n",
    "        logits = model(x_batch)\n",
    "        acc = accuracy(logits, y_batch)\n",
    "        \n",
    "        return {\n",
    "            'loss': float(loss),\n",
    "            'accuracy': float(acc)\n",
    "        }\n",
    "\n",
    "def eval_step(model, x_batch, y_batch):\n",
    "    \"\"\"Perform one evaluation step.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to evaluate\n",
    "        x_batch: Input batch\n",
    "        y_batch: Label batch\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with loss and accuracy\n",
    "    \"\"\"\n",
    "    with bst.environ.context(fit=False):\n",
    "        logits = model(x_batch)\n",
    "        loss = cross_entropy_loss(logits, y_batch)\n",
    "        acc = accuracy(logits, y_batch)\n",
    "        \n",
    "        return {\n",
    "            'loss': float(loss),\n",
    "            'accuracy': float(acc)\n",
    "        }\n",
    "\n",
    "# Test training step\n",
    "batch_size = 32\n",
    "x_batch = X_train[:batch_size]\n",
    "y_batch = y_train[:batch_size]\n",
    "\n",
    "metrics = train_step(model, x_batch, y_batch, learning_rate=0.001)\n",
    "print(f\"Training step metrics: {metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Complete Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(X, y, batch_size, shuffle=True):\n",
    "    \"\"\"Create batches from dataset.\n",
    "    \n",
    "    Args:\n",
    "        X: Input data\n",
    "        y: Labels\n",
    "        batch_size: Batch size\n",
    "        shuffle: Whether to shuffle data\n",
    "        \n",
    "    Yields:\n",
    "        Tuples of (x_batch, y_batch)\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    indices = np.arange(n_samples)\n",
    "    \n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "    for start_idx in range(0, n_samples, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, n_samples)\n",
    "        batch_indices = indices[start_idx:end_idx]\n",
    "        yield X[batch_indices], y[batch_indices]\n",
    "\n",
    "def train_epoch(model, X_train, y_train, batch_size, learning_rate):\n",
    "    \"\"\"Train for one epoch.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with average metrics\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    \n",
    "    for x_batch, y_batch in create_batches(X_train, y_train, batch_size, shuffle=True):\n",
    "        metrics = train_step(model, x_batch, y_batch, learning_rate)\n",
    "        losses.append(metrics['loss'])\n",
    "        accuracies.append(metrics['accuracy'])\n",
    "    \n",
    "    return {\n",
    "        'loss': np.mean(losses),\n",
    "        'accuracy': np.mean(accuracies)\n",
    "    }\n",
    "\n",
    "def evaluate(model, X, y, batch_size):\n",
    "    \"\"\"Evaluate model on dataset.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with average metrics\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    \n",
    "    for x_batch, y_batch in create_batches(X, y, batch_size, shuffle=False):\n",
    "        metrics = eval_step(model, x_batch, y_batch)\n",
    "        losses.append(metrics['loss'])\n",
    "        accuracies.append(metrics['accuracy'])\n",
    "    \n",
    "    return {\n",
    "        'loss': np.mean(losses),\n",
    "        'accuracy': np.mean(accuracies)\n",
    "    }\n",
    "\n",
    "# Training configuration\n",
    "config = {\n",
    "    'num_epochs': 20,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 0.001,\n",
    "}\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "}\n",
    "\n",
    "# Recreate model for fresh training\n",
    "model = ImageClassifier(num_classes=10)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(config['num_epochs']):\n",
    "    # Train\n",
    "    train_metrics = train_epoch(\n",
    "        model, X_train, y_train, \n",
    "        config['batch_size'], \n",
    "        config['learning_rate']\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_metrics = evaluate(model, X_val, y_val, config['batch_size'])\n",
    "    \n",
    "    # Record history\n",
    "    history['train_loss'].append(train_metrics['loss'])\n",
    "    history['train_acc'].append(train_metrics['accuracy'])\n",
    "    history['val_loss'].append(val_metrics['loss'])\n",
    "    history['val_acc'].append(val_metrics['accuracy'])\n",
    "    \n",
    "    # Track best model\n",
    "    if val_metrics['accuracy'] > best_val_acc:\n",
    "        best_val_acc = val_metrics['accuracy']\n",
    "        best_epoch = epoch\n",
    "    \n",
    "    # Print progress\n",
    "    if epoch % 5 == 0 or epoch == config['num_epochs'] - 1:\n",
    "        print(f\"Epoch {epoch:2d}/{config['num_epochs']}: \"\n",
    "              f\"train_loss={train_metrics['loss']:.4f}, \"\n",
    "              f\"train_acc={train_metrics['accuracy']:.4f}, \"\n",
    "              f\"val_loss={val_metrics['loss']:.4f}, \"\n",
    "              f\"val_acc={val_metrics['accuracy']:.4f}\")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(\"=\" * 70)\n",
    "print(f\"Training completed in {training_time:.2f}s\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.4f} at epoch {best_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "epochs = range(len(history['train_loss']))\n",
    "ax1.plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "ax1.plot(epochs, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(epochs, history['train_acc'], 'b-', label='Train Accuracy', linewidth=2)\n",
    "ax2.plot(epochs, history['val_acc'], 'r-', label='Val Accuracy', linewidth=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_metrics = evaluate(model, X_test, y_test, batch_size=64)\n",
    "\n",
    "print(\"Test Set Performance:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Loss: {test_metrics['loss']:.4f}\")\n",
    "print(f\"Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"Error Rate: {(1 - test_metrics['accuracy']) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confusion_matrix(model, X, y, num_classes=10, batch_size=64):\n",
    "    \"\"\"Compute confusion matrix.\n",
    "    \n",
    "    Returns:\n",
    "        Confusion matrix of shape (num_classes, num_classes)\n",
    "    \"\"\"\n",
    "    confusion_matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    \n",
    "    with bst.environ.context(fit=False):\n",
    "        for x_batch, y_batch in create_batches(X, y, batch_size, shuffle=False):\n",
    "            logits = model(x_batch)\n",
    "            predictions = jnp.argmax(logits, axis=-1)\n",
    "            \n",
    "            for true_label, pred_label in zip(y_batch, predictions):\n",
    "                confusion_matrix[int(true_label), int(pred_label)] += 1\n",
    "    \n",
    "    return confusion_matrix\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = compute_confusion_matrix(model, X_test, y_test)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(cm, cmap='Blues', interpolation='nearest')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        plt.text(j, i, str(cm[i, j]), \n",
    "                ha='center', va='center',\n",
    "                color='white' if cm[i, j] > cm.max() / 2 else 'black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Per-class accuracy\n",
    "print(\"\\nPer-Class Accuracy:\")\n",
    "print(\"=\" * 50)\n",
    "for i in range(10):\n",
    "    class_total = cm[i].sum()\n",
    "    class_correct = cm[i, i]\n",
    "    class_acc = class_correct / class_total if class_total > 0 else 0\n",
    "    print(f\"Class {i}: {class_acc:.4f} ({class_correct}/{class_total})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for test samples\n",
    "n_samples = 20\n",
    "test_samples = X_test[:n_samples]\n",
    "test_labels = y_test[:n_samples]\n",
    "\n",
    "with bst.environ.context(fit=False):\n",
    "    logits = model(test_samples)\n",
    "    predictions = jnp.argmax(logits, axis=-1)\n",
    "    probabilities = jax.nn.softmax(logits, axis=-1)\n",
    "\n",
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(4, 5, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(n_samples):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Show image\n",
    "    ax.imshow(test_samples[i, :, :, 0], cmap='gray')\n",
    "    \n",
    "    # Title with prediction\n",
    "    true_label = int(test_labels[i])\n",
    "    pred_label = int(predictions[i])\n",
    "    confidence = float(probabilities[i, pred_label])\n",
    "    \n",
    "    color = 'green' if true_label == pred_label else 'red'\n",
    "    ax.set_title(\n",
    "        f'True: {true_label}, Pred: {pred_label}\\nConf: {confidence:.2f}',\n",
    "        color=color,\n",
    "        fontsize=10\n",
    "    )\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Model Predictions (Green=Correct, Red=Wrong)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Confidence Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction confidence\n",
    "with bst.environ.context(fit=False):\n",
    "    all_logits = model(X_test)\n",
    "    all_predictions = jnp.argmax(all_logits, axis=-1)\n",
    "    all_probs = jax.nn.softmax(all_logits, axis=-1)\n",
    "\n",
    "# Get max probability for each prediction\n",
    "max_probs = jnp.max(all_probs, axis=-1)\n",
    "\n",
    "# Separate correct and incorrect predictions\n",
    "correct_mask = (all_predictions == y_test)\n",
    "correct_probs = max_probs[correct_mask]\n",
    "incorrect_probs = max_probs[~correct_mask]\n",
    "\n",
    "# Plot confidence distributions\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.hist(correct_probs, bins=20, alpha=0.7, label='Correct Predictions', color='green')\n",
    "plt.hist(incorrect_probs, bins=20, alpha=0.7, label='Incorrect Predictions', color='red')\n",
    "\n",
    "plt.xlabel('Prediction Confidence')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Prediction Confidence')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average confidence for correct predictions: {jnp.mean(correct_probs):.4f}\")\n",
    "print(f\"Average confidence for incorrect predictions: {jnp.mean(incorrect_probs):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learned filters\n",
    "def visualize_conv_filters(model, layer_name='conv1'):\n",
    "    \"\"\"Visualize convolutional filters.\"\"\"\n",
    "    # Get first conv layer\n",
    "    conv_layer = getattr(model, layer_name)\n",
    "    filters = conv_layer.conv.weight.value  # Shape: (out_ch, in_ch, kh, kw)\n",
    "    \n",
    "    n_filters = min(16, filters.shape[0])\n",
    "    \n",
    "    fig, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(n_filters):\n",
    "        # Get filter for first input channel\n",
    "        filter_img = filters[i, 0, :, :]\n",
    "        \n",
    "        axes[i].imshow(filter_img, cmap='gray')\n",
    "        axes[i].set_title(f'Filter {i}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Learned Filters in {layer_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_conv_filters(model, 'conv1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we built a complete image classification system:\n",
    "\n",
    "1. **Dataset Preparation**: Generated synthetic MNIST-like data\n",
    "2. **CNN Architecture**: Built a CNN with conv blocks and FC layers\n",
    "3. **Training Setup**: Implemented loss, accuracy, and training steps\n",
    "4. **Training Loop**: Complete training with validation\n",
    "5. **Visualization**: Training curves and learning progress\n",
    "6. **Evaluation**: Test set performance and confusion matrix\n",
    "7. **Predictions**: Visualized model predictions and confidence\n",
    "8. **Feature Visualization**: Examined learned convolutional filters\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **CNNs** are powerful for image classification\n",
    "- **Training loops** require careful batch handling\n",
    "- **Validation** is crucial for monitoring overfitting\n",
    "- **Visualization** helps understand model behavior\n",
    "- **Evaluation metrics** beyond accuracy provide insights\n",
    "- BrainState makes it easy to build production-ready models\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next tutorial, we'll explore:\n",
    "- **Sequence Modeling**: RNNs, LSTMs, and Transformers\n",
    "- Text generation and language models\n",
    "- Attention mechanisms\n",
    "- Advanced sequence architectures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4e5f6",
   "metadata": {},
   "source": [
    "# Functional API and Graph Operations\n",
    "\n",
    "This tutorial demonstrates how to use BrainState's functional API for explicit state management using graph operations. This approach provides fine-grained control over model states and is particularly useful for advanced use cases like custom training loops and functional transformations.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "- Understand the functional API and graph operations in BrainState\n",
    "- Learn how to split and merge model states using `treefy_split` and `treefy_merge`\n",
    "- Build a training loop with explicit state management\n",
    "- Apply JAX transformations with separated states\n",
    "- Track custom states (e.g., function call counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5f6a7",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import brainstate\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6a7b8c9",
   "metadata": {},
   "source": [
    "## Problem: Polynomial Regression\n",
    "\n",
    "We'll solve a simple polynomial regression problem to demonstrate the functional API. Let's create a synthetic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data: y = 0.8 * x^2 + 0.1 + noise\n",
    "X = np.linspace(0, 1, 100)[:, None]\n",
    "Y = 0.8 * X ** 2 + 0.1 + np.random.normal(0, 0.1, size=X.shape)\n",
    "\n",
    "def dataset(batch_size):\n",
    "    \"\"\"Generator that yields random batches from the dataset.\"\"\"\n",
    "    while True:\n",
    "        idx = np.random.choice(len(X), size=batch_size)\n",
    "        yield X[idx], Y[idx]\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(X, Y, alpha=0.5, label='Data')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Synthetic Dataset for Polynomial Regression')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7b8c9d0e1",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "\n",
    "### Step 1: Define Basic Components\n",
    "\n",
    "First, let's create a simple `Linear` layer and a custom state type to track function calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8c9d0e1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(brainstate.nn.Module):\n",
    "    \"\"\"A simple linear layer: y = x @ w + b\"\"\"\n",
    "    \n",
    "    def __init__(self, din: int, dout: int):\n",
    "        super().__init__()\n",
    "        # Initialize weights and biases as trainable parameters\n",
    "        self.w = brainstate.ParamState(brainstate.random.rand(din, dout))\n",
    "        self.b = brainstate.ParamState(jnp.zeros((dout,)))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return x @ self.w.value + self.b.value\n",
    "\n",
    "\n",
    "class Count(brainstate.State):\n",
    "    \"\"\"Custom state type for tracking function calls.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9d0e1f2a3",
   "metadata": {},
   "source": [
    "### Step 2: Build the MLP Model\n",
    "\n",
    "Now let's create a multi-layer perceptron (MLP) with a call counter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0e1f2a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(brainstate.graph.Node):\n",
    "    \"\"\"Multi-layer perceptron with call counting.\"\"\"\n",
    "    \n",
    "    def __init__(self, din, dhidden, dout):\n",
    "        # Custom state to count how many times the model is called\n",
    "        self.count = Count(jnp.array(0))\n",
    "        \n",
    "        # Two linear layers\n",
    "        self.linear1 = Linear(din, dhidden)\n",
    "        self.linear2 = Linear(dhidden, dout)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Increment call counter\n",
    "        self.count.value += 1\n",
    "        \n",
    "        # Forward pass\n",
    "        x = self.linear1(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1f2a3b4c5",
   "metadata": {},
   "source": [
    "## Understanding Graph Operations\n",
    "\n",
    "### What are Graph Operations?\n",
    "\n",
    "BrainState models are represented as computational graphs where:\n",
    "- **Nodes** represent modules or components\n",
    "- **States** are the mutable variables within these nodes\n",
    "\n",
    "Graph operations allow you to:\n",
    "1. **Split** a model into its graph definition and separate state pytrees\n",
    "2. **Merge** a graph definition with state pytrees to reconstruct the model\n",
    "\n",
    "This is essential for functional programming with JAX, as it allows you to:\n",
    "- Pass states as explicit function arguments\n",
    "- Apply JAX transformations (jit, grad, vmap) to functions operating on states\n",
    "- Manage different types of states independently (e.g., parameters vs. counters)\n",
    "\n",
    "### Splitting the Model\n",
    "\n",
    "Let's create a model and split it into its components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2a3b4c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model_initial = MLP(din=1, dhidden=32, dout=1)\n",
    "\n",
    "# Split the model into graph definition and states\n",
    "graphdef, params_, counts_ = brainstate.graph.treefy_split(\n",
    "    model_initial, \n",
    "    brainstate.ParamState,  # Split out trainable parameters\n",
    "    Count                    # Split out call counters\n",
    ")\n",
    "\n",
    "print(\"Graph definition (model structure):\")\n",
    "print(graphdef)\n",
    "print(\"\\nParameters (trainable weights):\")\n",
    "print(jax.tree.map(jnp.shape, params_))\n",
    "print(\"\\nCounters:\")\n",
    "print(counts_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a3b4c5d6e7",
   "metadata": {},
   "source": [
    "**Key Points:**\n",
    "- `graphdef`: Contains the model structure (immutable)\n",
    "- `params_`: PyTree of trainable parameters (`ParamState`)\n",
    "- `counts_`: PyTree of counters (`Count` state)\n",
    "\n",
    "This separation is crucial because:\n",
    "1. Only `params_` needs gradients during training\n",
    "2. `counts_` needs to be updated but not differentiated\n",
    "3. `graphdef` remains constant throughout training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4c5d6e7f8",
   "metadata": {},
   "source": [
    "## Functional Training Loop\n",
    "\n",
    "### Step 1: Define Training Step\n",
    "\n",
    "With separated states, we can create a pure functional training step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5d6e7f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(params, counts, batch):\n",
    "    \"\"\"Perform one training step with explicit state management.\"\"\"\n",
    "    x, y = batch\n",
    "    \n",
    "    def loss_fn(params):\n",
    "        # Merge graph definition with states to reconstruct the model\n",
    "        model = brainstate.graph.treefy_merge(graphdef, params, counts)\n",
    "        \n",
    "        # Forward pass\n",
    "        y_pred = model(x)\n",
    "        \n",
    "        # Extract updated counters (model was called, so count changed)\n",
    "        new_counts = brainstate.graph.treefy_states(model, Count)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = jnp.mean((y - y_pred) ** 2)\n",
    "        \n",
    "        return loss, new_counts\n",
    "    \n",
    "    # Compute gradients with respect to parameters\n",
    "    grad, counts = jax.grad(loss_fn, has_aux=True)(params)\n",
    "    \n",
    "    # Simple SGD update: params = params - lr * grad\n",
    "    params = jax.tree.map(lambda w, g: w - 0.1 * g, params, grad)\n",
    "    \n",
    "    return params, counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6e7f8a9b0",
   "metadata": {},
   "source": [
    "**Understanding the Training Step:**\n",
    "\n",
    "1. **Loss Function Definition:**\n",
    "   - Takes `params` as input (what we differentiate)\n",
    "   - Merges `graphdef`, `params`, and `counts` to reconstruct the model\n",
    "   - Computes predictions and loss\n",
    "   - Returns both loss and updated counts (auxiliary output)\n",
    "\n",
    "2. **Gradient Computation:**\n",
    "   - `jax.grad` computes gradients of loss w.r.t. parameters\n",
    "   - `has_aux=True` allows returning both gradients and auxiliary values (counts)\n",
    "\n",
    "3. **Parameter Update:**\n",
    "   - Simple gradient descent: new_params = old_params - learning_rate * gradients\n",
    "   - Uses `jax.tree.map` to apply the update to all parameters in the pytree\n",
    "\n",
    "### Step 2: Define Evaluation Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7f8a9b0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def eval_step(params, counts, batch):\n",
    "    \"\"\"Evaluate the model on a batch.\"\"\"\n",
    "    x, y = batch\n",
    "    \n",
    "    # Reconstruct model\n",
    "    model = brainstate.graph.treefy_merge(graphdef, params, counts)\n",
    "    \n",
    "    # Forward pass\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = jnp.mean((y - y_pred) ** 2)\n",
    "    \n",
    "    return {'loss': loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8a9b0c1d2",
   "metadata": {},
   "source": [
    "### Step 3: Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a9b0c1d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "total_steps = 10_000\n",
    "\n",
    "# Training loop\n",
    "print(\"Training the model...\\n\")\n",
    "for step, batch in enumerate(dataset(32)):\n",
    "    # Update parameters and counters\n",
    "    params_, counts_ = train_step(params_, counts_, batch)\n",
    "    \n",
    "    # Log progress every 1000 steps\n",
    "    if step % 1000 == 0:\n",
    "        logs = eval_step(params_, counts_, (X, Y))\n",
    "        print(f\"Step: {step:5d}, Loss: {logs['loss']:.6f}\")\n",
    "    \n",
    "    # Stop after total_steps\n",
    "    if step >= total_steps - 1:\n",
    "        break\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b0c1d2e3f4",
   "metadata": {},
   "source": [
    "## Analyzing Results\n",
    "\n",
    "### Reconstruct the Final Model\n",
    "\n",
    "After training, we can merge the learned parameters back into a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c1d2e3f4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct the trained model\n",
    "model = brainstate.graph.treefy_merge(graphdef, params_, counts_)\n",
    "\n",
    "# Check how many times the model was called during training\n",
    "print(f\"Total model calls during training: {model.count.value}\")\n",
    "\n",
    "# Make predictions on the full dataset\n",
    "y_pred = model(X)\n",
    "\n",
    "print(f\"Final predictions shape: {y_pred.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d2e3f4a5b6",
   "metadata": {},
   "source": [
    "### Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e3f4a5b6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot data points\n",
    "plt.scatter(X, Y, color='blue', alpha=0.5, label='Training Data', s=30)\n",
    "\n",
    "# Plot predictions\n",
    "plt.plot(X, y_pred, color='red', linewidth=2, label='Model Prediction')\n",
    "\n",
    "# Plot true function\n",
    "X_true = np.linspace(0, 1, 100)[:, None]\n",
    "Y_true = 0.8 * X_true ** 2 + 0.1\n",
    "plt.plot(X_true, Y_true, color='green', linewidth=2, \n",
    "         linestyle='--', label='True Function', alpha=0.7)\n",
    "\n",
    "plt.xlabel('X', fontsize=12)\n",
    "plt.ylabel('Y', fontsize=12)\n",
    "plt.title('Polynomial Regression Results', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f4a5b6c7d8",
   "metadata": {},
   "source": [
    "## Key Concepts Summary\n",
    "\n",
    "### Graph Operations\n",
    "\n",
    "1. **`treefy_split(model, *state_types)`**:\n",
    "   - Splits a model into graph definition and state pytrees\n",
    "   - Returns: `(graphdef, state1, state2, ...)`\n",
    "   - Allows independent management of different state types\n",
    "\n",
    "2. **`treefy_merge(graphdef, *states)`**:\n",
    "   - Reconstructs a model from graph definition and states\n",
    "   - Returns: Complete model with all states\n",
    "   - Essential for functional API usage\n",
    "\n",
    "3. **`treefy_states(model, state_type)`**:\n",
    "   - Extracts states of a specific type from a model\n",
    "   - Useful for getting updated states after forward pass\n",
    "\n",
    "### Advantages of Functional API\n",
    "\n",
    "1. **Explicit State Management**: Full control over which states are updated and how\n",
    "2. **JAX Compatibility**: States are explicit function arguments, making JAX transformations straightforward\n",
    "3. **Flexibility**: Separate handling of parameters, hidden states, and custom states\n",
    "4. **Debugging**: Easier to track state changes and debug issues\n",
    "\n",
    "### When to Use Functional API\n",
    "\n",
    "- Custom training loops with complex state management\n",
    "- Implementing advanced optimization algorithms\n",
    "- Fine-grained control over gradient computation\n",
    "- Functional programming style with JAX transformations\n",
    "- Distributed training scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a5b6c7d8e9",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Try these exercises to deepen your understanding:\n",
    "\n",
    "1. **Add Momentum to SGD**:\n",
    "   - Create a custom state type for momentum\n",
    "   - Modify the training step to include momentum updates\n",
    "\n",
    "2. **Track More Statistics**:\n",
    "   - Add states to track training loss history\n",
    "   - Add states to track gradient norms\n",
    "\n",
    "3. **Implement Learning Rate Scheduling**:\n",
    "   - Create a state for the current learning rate\n",
    "   - Implement exponential decay or step decay\n",
    "\n",
    "4. **Multi-Task Learning**:\n",
    "   - Modify the MLP to have multiple output heads\n",
    "   - Track separate counters for each task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b6c7d8e9f0",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you understand the functional API and graph operations, you can:\n",
    "\n",
    "1. **Explore Lifted Transforms**: Learn about higher-level state management with automatic lifting\n",
    "2. **Advanced Optimizers**: Use BrainTools optimizers that handle state management for you\n",
    "3. **Complex Architectures**: Apply these concepts to recurrent networks and spiking neural networks\n",
    "4. **Checkpointing**: Learn how to save and load model states\n",
    "\n",
    "## References\n",
    "\n",
    "- [BrainState Graph API Documentation](https://brainstate.readthedocs.io/en/latest/apis/graph.html)\n",
    "- [JAX Transformations](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html)\n",
    "- [Flax Functional API](https://flax.readthedocs.io/en/latest/guides/flax_fundamentals/functional_api.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

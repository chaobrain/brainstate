{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 22: Model Deployment\n",
    "\n",
    "In this tutorial, we'll learn how to save, load, and deploy BrainState models for production use.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "- Understand how to save and load model states\n",
    "- Learn model serialization techniques\n",
    "- Implement checkpointing strategies\n",
    "- Optimize models for inference\n",
    "- Create batch processing pipelines\n",
    "- Implement model versioning\n",
    "- Deploy models for production use\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Model deployment is a critical step in the machine learning workflow. BrainState provides several mechanisms for saving, loading, and deploying models efficiently.\n",
    "\n",
    "Key concepts:\n",
    "- **State Management**: Saving and loading model parameters and states\n",
    "- **Serialization**: Converting model states to disk-friendly formats\n",
    "- **Checkpointing**: Periodic saving during training\n",
    "- **Inference Optimization**: Making models faster for production\n",
    "- **Batch Processing**: Handling multiple inputs efficiently\n",
    "- **Versioning**: Managing different model versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brainstate as bst\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, List\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "bst.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Model Saving and Loading\n",
    "\n",
    "The simplest way to save and load models is using BrainState's state dictionary system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple model\n",
    "class SimpleClassifier(bst.graph.Node):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = bst.nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = bst.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = bst.nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        x = jax.nn.relu(self.fc1(x))\n",
    "        x = jax.nn.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Create and initialize model\n",
    "model = SimpleClassifier(input_dim=784, hidden_dim=128, output_dim=10)\n",
    "\n",
    "# Initialize with dummy input\n",
    "dummy_input = bst.random.randn(1, 784)\n",
    "_ = model(dummy_input)\n",
    "\n",
    "print(f\"Model created with {sum(p.value.size for p in model.states(bst.ParamState).values())} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Saving Model States\n",
    "\n",
    "We can save the model's parameters using the state dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_states(model: bst.graph.Node, filepath: str):\n",
    "    \"\"\"\n",
    "    Save model states to a file.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to save\n",
    "        filepath: Path to save the model\n",
    "    \"\"\"\n",
    "    # Get all parameter states\n",
    "    states = model.states(bst.ParamState)\n",
    "    \n",
    "    # Convert to numpy arrays for serialization\n",
    "    state_dict = {k: np.array(v.value) for k, v in states.items()}\n",
    "    \n",
    "    # Save using numpy's compressed format\n",
    "    np.savez_compressed(filepath, **state_dict)\n",
    "    \n",
    "    print(f\"Model saved to {filepath}\")\n",
    "    print(f\"File size: {Path(filepath).stat().st_size / 1024:.2f} KB\")\n",
    "\n",
    "# Save the model\n",
    "save_model_states(model, \"simple_classifier.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Loading Model States\n",
    "\n",
    "We can load the saved states back into a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_states(model: bst.graph.Node, filepath: str):\n",
    "    \"\"\"\n",
    "    Load model states from a file.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to load states into\n",
    "        filepath: Path to load the model from\n",
    "    \"\"\"\n",
    "    # Load the state dictionary\n",
    "    loaded_data = np.load(filepath)\n",
    "    state_dict = {k: jnp.array(loaded_data[k]) for k in loaded_data.files}\n",
    "    \n",
    "    # Get model states\n",
    "    model_states = model.states(bst.ParamState)\n",
    "    \n",
    "    # Verify compatibility\n",
    "    if set(state_dict.keys()) != set(model_states.keys()):\n",
    "        raise ValueError(\"State dictionary keys don't match model states\")\n",
    "    \n",
    "    # Load states\n",
    "    for k, v in state_dict.items():\n",
    "        if model_states[k].value.shape != v.shape:\n",
    "            raise ValueError(f\"Shape mismatch for {k}: expected {model_states[k].value.shape}, got {v.shape}\")\n",
    "        model_states[k].value = v\n",
    "    \n",
    "    print(f\"Model loaded from {filepath}\")\n",
    "\n",
    "# Create a new model and load states\n",
    "new_model = SimpleClassifier(input_dim=784, hidden_dim=128, output_dim=10)\n",
    "_ = new_model(dummy_input)  # Initialize\n",
    "\n",
    "# Verify models are different before loading\n",
    "original_output = model(dummy_input)\n",
    "new_output_before = new_model(dummy_input)\n",
    "print(f\"Outputs differ before loading: {not jnp.allclose(original_output, new_output_before)}\")\n",
    "\n",
    "# Load states\n",
    "load_model_states(new_model, \"simple_classifier.npz\")\n",
    "\n",
    "# Verify models are identical after loading\n",
    "new_output_after = new_model(dummy_input)\n",
    "print(f\"Outputs match after loading: {jnp.allclose(original_output, new_output_after)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Complete Model Serialization\n",
    "\n",
    "For complete model serialization, we need to save both the model architecture and its states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSerializer:\n",
    "    \"\"\"\n",
    "    Complete model serialization with metadata.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def save(model: bst.graph.Node, \n",
    "             filepath: str, \n",
    "             metadata: Optional[Dict[str, Any]] = None):\n",
    "        \"\"\"\n",
    "        Save model with metadata.\n",
    "        \n",
    "        Args:\n",
    "            model: Model to save\n",
    "            filepath: Path to save to\n",
    "            metadata: Additional metadata (training info, hyperparameters, etc.)\n",
    "        \"\"\"\n",
    "        # Get model states\n",
    "        states = model.states(bst.ParamState)\n",
    "        state_dict = {k: np.array(v.value) for k, v in states.items()}\n",
    "        \n",
    "        # Prepare metadata\n",
    "        meta = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'model_class': model.__class__.__name__,\n",
    "            'num_parameters': sum(v.size for v in state_dict.values()),\n",
    "            'state_keys': list(state_dict.keys()),\n",
    "        }\n",
    "        \n",
    "        if metadata:\n",
    "            meta.update(metadata)\n",
    "        \n",
    "        # Save everything\n",
    "        save_data = {\n",
    "            'states': state_dict,\n",
    "            'metadata': meta\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(save_data, f)\n",
    "        \n",
    "        print(f\"Model serialized to {filepath}\")\n",
    "        print(f\"Metadata: {json.dumps(meta, indent=2)}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def load(model: bst.graph.Node, filepath: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Load model and return metadata.\n",
    "        \n",
    "        Args:\n",
    "            model: Model to load states into\n",
    "            filepath: Path to load from\n",
    "            \n",
    "        Returns:\n",
    "            Metadata dictionary\n",
    "        \"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            save_data = pickle.load(f)\n",
    "        \n",
    "        state_dict = {k: jnp.array(v) for k, v in save_data['states'].items()}\n",
    "        metadata = save_data['metadata']\n",
    "        \n",
    "        # Load states\n",
    "        model_states = model.states(bst.ParamState)\n",
    "        for k, v in state_dict.items():\n",
    "            if k in model_states:\n",
    "                model_states[k].value = v\n",
    "        \n",
    "        print(f\"Model deserialized from {filepath}\")\n",
    "        print(f\"Saved: {metadata['timestamp']}\")\n",
    "        \n",
    "        return metadata\n",
    "\n",
    "# Example usage\n",
    "metadata = {\n",
    "    'accuracy': 0.95,\n",
    "    'loss': 0.15,\n",
    "    'epochs': 10,\n",
    "    'learning_rate': 0.001,\n",
    "    'dataset': 'MNIST'\n",
    "}\n",
    "\n",
    "ModelSerializer.save(model, \"model_with_metadata.pkl\", metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and verify\n",
    "loaded_model = SimpleClassifier(input_dim=784, hidden_dim=128, output_dim=10)\n",
    "_ = loaded_model(dummy_input)\n",
    "\n",
    "loaded_metadata = ModelSerializer.load(loaded_model, \"model_with_metadata.pkl\")\n",
    "print(f\"\\nLoaded metadata: {json.dumps(loaded_metadata, indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Checkpoint Management\n",
    "\n",
    "During training, we want to save checkpoints periodically and keep the best models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckpointManager:\n",
    "    \"\"\"\n",
    "    Manages model checkpoints during training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 checkpoint_dir: str,\n",
    "                 max_to_keep: int = 5,\n",
    "                 keep_best: bool = True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            checkpoint_dir: Directory to save checkpoints\n",
    "            max_to_keep: Maximum number of checkpoints to keep\n",
    "            keep_best: Whether to keep the best checkpoint separately\n",
    "        \"\"\"\n",
    "        self.checkpoint_dir = Path(checkpoint_dir)\n",
    "        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.max_to_keep = max_to_keep\n",
    "        self.keep_best = keep_best\n",
    "        self.checkpoints = []  # List of (filepath, metric) tuples\n",
    "        self.best_metric = None\n",
    "        self.best_checkpoint = None\n",
    "    \n",
    "    def save_checkpoint(self,\n",
    "                       model: bst.graph.Node,\n",
    "                       epoch: int,\n",
    "                       metric: float,\n",
    "                       metadata: Optional[Dict[str, Any]] = None) -> str:\n",
    "        \"\"\"\n",
    "        Save a checkpoint.\n",
    "        \n",
    "        Args:\n",
    "            model: Model to save\n",
    "            epoch: Current epoch\n",
    "            metric: Metric value (lower is better)\n",
    "            metadata: Additional metadata\n",
    "            \n",
    "        Returns:\n",
    "            Path to saved checkpoint\n",
    "        \"\"\"\n",
    "        # Create checkpoint filename\n",
    "        checkpoint_name = f\"checkpoint_epoch_{epoch:04d}_metric_{metric:.4f}.pkl\"\n",
    "        checkpoint_path = self.checkpoint_dir / checkpoint_name\n",
    "        \n",
    "        # Prepare metadata\n",
    "        meta = {\n",
    "            'epoch': epoch,\n",
    "            'metric': metric,\n",
    "        }\n",
    "        if metadata:\n",
    "            meta.update(metadata)\n",
    "        \n",
    "        # Save checkpoint\n",
    "        ModelSerializer.save(model, str(checkpoint_path), meta)\n",
    "        \n",
    "        # Add to checkpoint list\n",
    "        self.checkpoints.append((str(checkpoint_path), metric))\n",
    "        \n",
    "        # Update best checkpoint\n",
    "        if self.keep_best and (self.best_metric is None or metric < self.best_metric):\n",
    "            self.best_metric = metric\n",
    "            best_path = self.checkpoint_dir / \"best_model.pkl\"\n",
    "            ModelSerializer.save(model, str(best_path), meta)\n",
    "            self.best_checkpoint = str(best_path)\n",
    "            print(f\"New best model saved with metric: {metric:.4f}\")\n",
    "        \n",
    "        # Remove old checkpoints if necessary\n",
    "        if len(self.checkpoints) > self.max_to_keep:\n",
    "            # Sort by metric (ascending)\n",
    "            self.checkpoints.sort(key=lambda x: x[1])\n",
    "            # Remove worst checkpoint\n",
    "            to_remove = self.checkpoints.pop()\n",
    "            Path(to_remove[0]).unlink(missing_ok=True)\n",
    "            print(f\"Removed old checkpoint: {Path(to_remove[0]).name}\")\n",
    "        \n",
    "        return str(checkpoint_path)\n",
    "    \n",
    "    def load_best(self, model: bst.graph.Node) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Load the best checkpoint.\n",
    "        \n",
    "        Args:\n",
    "            model: Model to load into\n",
    "            \n",
    "        Returns:\n",
    "            Metadata if best checkpoint exists, None otherwise\n",
    "        \"\"\"\n",
    "        if self.best_checkpoint and Path(self.best_checkpoint).exists():\n",
    "            return ModelSerializer.load(model, self.best_checkpoint)\n",
    "        return None\n",
    "    \n",
    "    def load_latest(self, model: bst.graph.Node) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Load the latest checkpoint.\n",
    "        \n",
    "        Args:\n",
    "            model: Model to load into\n",
    "            \n",
    "        Returns:\n",
    "            Metadata if checkpoint exists, None otherwise\n",
    "        \"\"\"\n",
    "        if self.checkpoints:\n",
    "            latest_checkpoint = self.checkpoints[-1][0]\n",
    "            return ModelSerializer.load(model, latest_checkpoint)\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "checkpoint_manager = CheckpointManager(\"checkpoints\", max_to_keep=3)\n",
    "\n",
    "# Simulate training with checkpoints\n",
    "for epoch in range(10):\n",
    "    # Simulate metric improvement\n",
    "    metric = 1.0 - epoch * 0.08 + np.random.randn() * 0.05\n",
    "    \n",
    "    metadata = {\n",
    "        'train_loss': metric,\n",
    "        'learning_rate': 0.001 * (0.95 ** epoch)\n",
    "    }\n",
    "    \n",
    "    checkpoint_manager.save_checkpoint(model, epoch, metric, metadata)\n",
    "    print(f\"Epoch {epoch}: metric={metric:.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "print(\"\\nLoading best model:\")\n",
    "best_metadata = checkpoint_manager.load_best(model)\n",
    "if best_metadata:\n",
    "    print(f\"Best model from epoch {best_metadata['epoch']} with metric {best_metadata['metric']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inference Optimization\n",
    "\n",
    "For production deployment, we want to optimize models for fast inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceModel:\n",
    "    \"\"\"\n",
    "    Optimized model wrapper for inference.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: bst.graph.Node):\n",
    "        self.model = model\n",
    "        # Create JIT-compiled inference function\n",
    "        self._predict_fn = bst.transform.jit(self._predict)\n",
    "        # Warmup\n",
    "        self._warmup()\n",
    "    \n",
    "    def _predict(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"Single prediction.\"\"\"\n",
    "        return self.model(x)\n",
    "    \n",
    "    def _warmup(self):\n",
    "        \"\"\"Warmup JIT compilation.\"\"\"\n",
    "        dummy = jnp.zeros((1, 784))\n",
    "        _ = self._predict_fn(dummy)\n",
    "    \n",
    "    def predict(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"\n",
    "        Make predictions.\n",
    "        \n",
    "        Args:\n",
    "            x: Input array\n",
    "            \n",
    "        Returns:\n",
    "            Predictions\n",
    "        \"\"\"\n",
    "        return self._predict_fn(x)\n",
    "    \n",
    "    def predict_proba(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"\n",
    "        Predict class probabilities.\n",
    "        \n",
    "        Args:\n",
    "            x: Input array\n",
    "            \n",
    "        Returns:\n",
    "            Class probabilities\n",
    "        \"\"\"\n",
    "        logits = self.predict(x)\n",
    "        return jax.nn.softmax(logits, axis=-1)\n",
    "    \n",
    "    def predict_class(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"\n",
    "        Predict class labels.\n",
    "        \n",
    "        Args:\n",
    "            x: Input array\n",
    "            \n",
    "        Returns:\n",
    "            Class labels\n",
    "        \"\"\"\n",
    "        logits = self.predict(x)\n",
    "        return jnp.argmax(logits, axis=-1)\n",
    "\n",
    "# Create inference model\n",
    "inference_model = InferenceModel(model)\n",
    "\n",
    "# Test predictions\n",
    "test_input = bst.random.randn(5, 784)\n",
    "predictions = inference_model.predict_class(test_input)\n",
    "probabilities = inference_model.predict_proba(test_input)\n",
    "\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"\\nTop-3 probabilities for first sample:\")\n",
    "top_3_idx = jnp.argsort(probabilities[0])[-3:][::-1]\n",
    "for idx in top_3_idx:\n",
    "    print(f\"  Class {idx}: {probabilities[0, idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Benchmarking Inference Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_inference(model_fn, input_shape, num_runs=100, batch_sizes=[1, 10, 50, 100]):\n",
    "    \"\"\"\n",
    "    Benchmark inference speed.\n",
    "    \n",
    "    Args:\n",
    "        model_fn: Function to call for inference\n",
    "        input_shape: Shape of input (excluding batch)\n",
    "        num_runs: Number of runs per batch size\n",
    "        batch_sizes: List of batch sizes to test\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        # Create test data\n",
    "        test_data = jnp.ones((batch_size,) + input_shape)\n",
    "        \n",
    "        # Warmup\n",
    "        _ = model_fn(test_data)\n",
    "        \n",
    "        # Benchmark\n",
    "        start_time = time.time()\n",
    "        for _ in range(num_runs):\n",
    "            _ = model_fn(test_data)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        total_time = end_time - start_time\n",
    "        time_per_sample = (total_time / num_runs / batch_size) * 1000  # ms\n",
    "        throughput = (num_runs * batch_size) / total_time\n",
    "        \n",
    "        results.append({\n",
    "            'batch_size': batch_size,\n",
    "            'time_per_sample_ms': time_per_sample,\n",
    "            'throughput_samples_per_sec': throughput\n",
    "        })\n",
    "        \n",
    "        print(f\"Batch size {batch_size:3d}: {time_per_sample:.3f} ms/sample, \"\n",
    "              f\"{throughput:.1f} samples/sec\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Benchmarking inference performance:\\n\")\n",
    "benchmark_results = benchmark_inference(\n",
    "    inference_model.predict,\n",
    "    input_shape=(784,),\n",
    "    num_runs=100,\n",
    "    batch_sizes=[1, 10, 50, 100]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize benchmark results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "batch_sizes = [r['batch_size'] for r in benchmark_results]\n",
    "latencies = [r['time_per_sample_ms'] for r in benchmark_results]\n",
    "throughputs = [r['throughput_samples_per_sec'] for r in benchmark_results]\n",
    "\n",
    "ax1.plot(batch_sizes, latencies, 'o-', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Batch Size')\n",
    "ax1.set_ylabel('Latency (ms/sample)')\n",
    "ax1.set_title('Inference Latency vs Batch Size')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(batch_sizes, throughputs, 's-', linewidth=2, markersize=8, color='green')\n",
    "ax2.set_xlabel('Batch Size')\n",
    "ax2.set_ylabel('Throughput (samples/sec)')\n",
    "ax2.set_title('Inference Throughput vs Batch Size')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Batch Processing Pipeline\n",
    "\n",
    "For production systems, we often need to process large datasets efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchProcessor:\n",
    "    \"\"\"\n",
    "    Efficient batch processing for inference.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model: InferenceModel,\n",
    "                 batch_size: int = 32,\n",
    "                 show_progress: bool = True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: Inference model\n",
    "            batch_size: Batch size for processing\n",
    "            show_progress: Whether to show progress\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "        self.show_progress = show_progress\n",
    "    \n",
    "    def process(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Process data in batches.\n",
    "        \n",
    "        Args:\n",
    "            data: Input data array\n",
    "            \n",
    "        Returns:\n",
    "            Predictions for all data\n",
    "        \"\"\"\n",
    "        num_samples = len(data)\n",
    "        num_batches = (num_samples + self.batch_size - 1) // self.batch_size\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            start_idx = i * self.batch_size\n",
    "            end_idx = min(start_idx + self.batch_size, num_samples)\n",
    "            \n",
    "            batch = jnp.array(data[start_idx:end_idx])\n",
    "            batch_predictions = self.model.predict_class(batch)\n",
    "            predictions.append(np.array(batch_predictions))\n",
    "            \n",
    "            if self.show_progress:\n",
    "                print(f\"\\rProcessed {end_idx}/{num_samples} samples\", end=\"\")\n",
    "        \n",
    "        if self.show_progress:\n",
    "            print()  # New line\n",
    "        \n",
    "        return np.concatenate(predictions, axis=0)\n",
    "    \n",
    "    def process_with_probabilities(self, data: np.ndarray) -> tuple:\n",
    "        \"\"\"\n",
    "        Process data and return both predictions and probabilities.\n",
    "        \n",
    "        Args:\n",
    "            data: Input data array\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (predictions, probabilities)\n",
    "        \"\"\"\n",
    "        num_samples = len(data)\n",
    "        num_batches = (num_samples + self.batch_size - 1) // self.batch_size\n",
    "        \n",
    "        all_predictions = []\n",
    "        all_probabilities = []\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            start_idx = i * self.batch_size\n",
    "            end_idx = min(start_idx + self.batch_size, num_samples)\n",
    "            \n",
    "            batch = jnp.array(data[start_idx:end_idx])\n",
    "            batch_probabilities = self.model.predict_proba(batch)\n",
    "            batch_predictions = jnp.argmax(batch_probabilities, axis=-1)\n",
    "            \n",
    "            all_predictions.append(np.array(batch_predictions))\n",
    "            all_probabilities.append(np.array(batch_probabilities))\n",
    "            \n",
    "            if self.show_progress:\n",
    "                print(f\"\\rProcessed {end_idx}/{num_samples} samples\", end=\"\")\n",
    "        \n",
    "        if self.show_progress:\n",
    "            print()\n",
    "        \n",
    "        predictions = np.concatenate(all_predictions, axis=0)\n",
    "        probabilities = np.concatenate(all_probabilities, axis=0)\n",
    "        \n",
    "        return predictions, probabilities\n",
    "\n",
    "# Example usage\n",
    "processor = BatchProcessor(inference_model, batch_size=32)\n",
    "\n",
    "# Generate test dataset\n",
    "test_dataset = np.random.randn(1000, 784)\n",
    "\n",
    "# Process all data\n",
    "start_time = time.time()\n",
    "predictions, probabilities = processor.process_with_probabilities(test_dataset)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\nProcessed {len(test_dataset)} samples in {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Throughput: {len(test_dataset) / (end_time - start_time):.1f} samples/sec\")\n",
    "print(f\"\\nPrediction distribution:\")\n",
    "for class_id in range(10):\n",
    "    count = np.sum(predictions == class_id)\n",
    "    print(f\"  Class {class_id}: {count} samples ({count/len(predictions)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Versioning\n",
    "\n",
    "Managing multiple versions of models is crucial for production systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelRegistry:\n",
    "    \"\"\"\n",
    "    Model version registry for managing multiple model versions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, registry_dir: str):\n",
    "        self.registry_dir = Path(registry_dir)\n",
    "        self.registry_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.index_file = self.registry_dir / \"registry.json\"\n",
    "        self.versions = self._load_index()\n",
    "    \n",
    "    def _load_index(self) -> Dict:\n",
    "        \"\"\"Load version index.\"\"\"\n",
    "        if self.index_file.exists():\n",
    "            with open(self.index_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "    \n",
    "    def _save_index(self):\n",
    "        \"\"\"Save version index.\"\"\"\n",
    "        with open(self.index_file, 'w') as f:\n",
    "            json.dump(self.versions, f, indent=2)\n",
    "    \n",
    "    def register_model(self,\n",
    "                      model: bst.graph.Node,\n",
    "                      version: str,\n",
    "                      description: str = \"\",\n",
    "                      metrics: Optional[Dict[str, float]] = None,\n",
    "                      tags: Optional[List[str]] = None) -> str:\n",
    "        \"\"\"\n",
    "        Register a new model version.\n",
    "        \n",
    "        Args:\n",
    "            model: Model to register\n",
    "            version: Version string (e.g., \"1.0.0\")\n",
    "            description: Description of this version\n",
    "            metrics: Performance metrics\n",
    "            tags: Tags for categorization\n",
    "            \n",
    "        Returns:\n",
    "            Path to saved model\n",
    "        \"\"\"\n",
    "        if version in self.versions:\n",
    "            raise ValueError(f\"Version {version} already exists\")\n",
    "        \n",
    "        # Create version directory\n",
    "        version_dir = self.registry_dir / f\"v_{version}\"\n",
    "        version_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Save model\n",
    "        model_path = version_dir / \"model.pkl\"\n",
    "        metadata = {\n",
    "            'version': version,\n",
    "            'description': description,\n",
    "            'metrics': metrics or {},\n",
    "            'tags': tags or [],\n",
    "            'registered_at': datetime.now().isoformat()\n",
    "        }\n",
    "        ModelSerializer.save(model, str(model_path), metadata)\n",
    "        \n",
    "        # Update registry\n",
    "        self.versions[version] = {\n",
    "            'path': str(model_path),\n",
    "            'description': description,\n",
    "            'metrics': metrics or {},\n",
    "            'tags': tags or [],\n",
    "            'registered_at': metadata['registered_at']\n",
    "        }\n",
    "        self._save_index()\n",
    "        \n",
    "        print(f\"Model version {version} registered\")\n",
    "        return str(model_path)\n",
    "    \n",
    "    def load_version(self, model: bst.graph.Node, version: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Load a specific model version.\n",
    "        \n",
    "        Args:\n",
    "            model: Model to load into\n",
    "            version: Version to load\n",
    "            \n",
    "        Returns:\n",
    "            Version metadata\n",
    "        \"\"\"\n",
    "        if version not in self.versions:\n",
    "            raise ValueError(f\"Version {version} not found\")\n",
    "        \n",
    "        model_path = self.versions[version]['path']\n",
    "        metadata = ModelSerializer.load(model, model_path)\n",
    "        \n",
    "        return metadata\n",
    "    \n",
    "    def list_versions(self, tags: Optional[List[str]] = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        List all registered versions.\n",
    "        \n",
    "        Args:\n",
    "            tags: Filter by tags (optional)\n",
    "            \n",
    "        Returns:\n",
    "            List of version info dictionaries\n",
    "        \"\"\"\n",
    "        versions = []\n",
    "        for version, info in self.versions.items():\n",
    "            if tags is None or any(tag in info['tags'] for tag in tags):\n",
    "                versions.append({\n",
    "                    'version': version,\n",
    "                    **info\n",
    "                })\n",
    "        return versions\n",
    "    \n",
    "    def get_best_version(self, metric: str = 'accuracy') -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Get the version with the best metric.\n",
    "        \n",
    "        Args:\n",
    "            metric: Metric name to optimize\n",
    "            \n",
    "        Returns:\n",
    "            Best version string or None\n",
    "        \"\"\"\n",
    "        best_version = None\n",
    "        best_value = -float('inf')\n",
    "        \n",
    "        for version, info in self.versions.items():\n",
    "            if metric in info['metrics']:\n",
    "                value = info['metrics'][metric]\n",
    "                if value > best_value:\n",
    "                    best_value = value\n",
    "                    best_version = version\n",
    "        \n",
    "        return best_version\n",
    "\n",
    "# Example usage\n",
    "registry = ModelRegistry(\"model_registry\")\n",
    "\n",
    "# Register multiple versions\n",
    "versions_to_register = [\n",
    "    (\"1.0.0\", \"Initial release\", {'accuracy': 0.85, 'f1': 0.83}, ['baseline']),\n",
    "    (\"1.1.0\", \"Improved architecture\", {'accuracy': 0.89, 'f1': 0.87}, ['production']),\n",
    "    (\"1.2.0\", \"Fine-tuned hyperparameters\", {'accuracy': 0.92, 'f1': 0.90}, ['production', 'best']),\n",
    "    (\"2.0.0\", \"Major redesign\", {'accuracy': 0.91, 'f1': 0.89}, ['experimental']),\n",
    "]\n",
    "\n",
    "for version, desc, metrics, tags in versions_to_register:\n",
    "    try:\n",
    "        registry.register_model(model, version, desc, metrics, tags)\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping {version}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all versions\n",
    "print(\"\\nAll registered versions:\")\n",
    "all_versions = registry.list_versions()\n",
    "for v in all_versions:\n",
    "    print(f\"\\nVersion: {v['version']}\")\n",
    "    print(f\"  Description: {v['description']}\")\n",
    "    print(f\"  Metrics: {v['metrics']}\")\n",
    "    print(f\"  Tags: {v['tags']}\")\n",
    "    print(f\"  Registered: {v['registered_at']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best version\n",
    "best_version = registry.get_best_version('accuracy')\n",
    "print(f\"\\nBest version by accuracy: {best_version}\")\n",
    "\n",
    "# List production versions\n",
    "print(\"\\nProduction versions:\")\n",
    "prod_versions = registry.list_versions(tags=['production'])\n",
    "for v in prod_versions:\n",
    "    print(f\"  {v['version']}: accuracy={v['metrics']['accuracy']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Production Deployment Example\n",
    "\n",
    "Let's put it all together in a complete deployment scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionModel:\n",
    "    \"\"\"\n",
    "    Production-ready model wrapper.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model_class,\n",
    "                 model_kwargs: Dict[str, Any],\n",
    "                 registry: ModelRegistry,\n",
    "                 version: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_class: Model class to instantiate\n",
    "            model_kwargs: Keyword arguments for model initialization\n",
    "            registry: Model registry\n",
    "            version: Version to load (if None, loads best)\n",
    "        \"\"\"\n",
    "        # Create model\n",
    "        self.model = model_class(**model_kwargs)\n",
    "        \n",
    "        # Initialize model\n",
    "        dummy_input = jnp.zeros((1,) + (model_kwargs['input_dim'],))\n",
    "        _ = self.model(dummy_input)\n",
    "        \n",
    "        # Load version\n",
    "        if version is None:\n",
    "            version = registry.get_best_version('accuracy')\n",
    "            print(f\"Loading best version: {version}\")\n",
    "        \n",
    "        self.metadata = registry.load_version(self.model, version)\n",
    "        self.version = version\n",
    "        \n",
    "        # Create inference model\n",
    "        self.inference_model = InferenceModel(self.model)\n",
    "        \n",
    "        # Create batch processor\n",
    "        self.processor = BatchProcessor(self.inference_model, batch_size=32, show_progress=False)\n",
    "        \n",
    "        print(f\"Production model ready (version {self.version})\")\n",
    "        print(f\"Metrics: {self.metadata.get('metrics', {})}\")\n",
    "    \n",
    "    def predict(self, x: np.ndarray, return_probabilities: bool = False):\n",
    "        \"\"\"\n",
    "        Make predictions.\n",
    "        \n",
    "        Args:\n",
    "            x: Input data\n",
    "            return_probabilities: Whether to return probabilities\n",
    "            \n",
    "        Returns:\n",
    "            Predictions (and probabilities if requested)\n",
    "        \"\"\"\n",
    "        if len(x) == 1:\n",
    "            # Single prediction\n",
    "            x_jnp = jnp.array(x)\n",
    "            if return_probabilities:\n",
    "                probs = self.inference_model.predict_proba(x_jnp)\n",
    "                preds = jnp.argmax(probs, axis=-1)\n",
    "                return np.array(preds), np.array(probs)\n",
    "            else:\n",
    "                preds = self.inference_model.predict_class(x_jnp)\n",
    "                return np.array(preds)\n",
    "        else:\n",
    "            # Batch prediction\n",
    "            if return_probabilities:\n",
    "                return self.processor.process_with_probabilities(x)\n",
    "            else:\n",
    "                return self.processor.process(x)\n",
    "    \n",
    "    def get_info(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get model information.\n",
    "        \n",
    "        Returns:\n",
    "            Model info dictionary\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'version': self.version,\n",
    "            'metadata': self.metadata,\n",
    "            'num_parameters': sum(p.value.size for p in self.model.states(bst.ParamState).values())\n",
    "        }\n",
    "\n",
    "# Deploy production model\n",
    "production_model = ProductionModel(\n",
    "    model_class=SimpleClassifier,\n",
    "    model_kwargs={'input_dim': 784, 'hidden_dim': 128, 'output_dim': 10},\n",
    "    registry=registry,\n",
    "    version='1.2.0'  # Or None to load best\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test production model\n",
    "test_samples = np.random.randn(100, 784)\n",
    "\n",
    "# Single prediction\n",
    "single_pred, single_prob = production_model.predict(test_samples[:1], return_probabilities=True)\n",
    "print(f\"Single prediction: class {single_pred[0]}\")\n",
    "print(f\"Confidence: {single_prob[0, single_pred[0]]:.4f}\")\n",
    "\n",
    "# Batch predictions\n",
    "print(\"\\nBatch predictions:\")\n",
    "batch_preds = production_model.predict(test_samples)\n",
    "print(f\"Processed {len(batch_preds)} samples\")\n",
    "\n",
    "# Get model info\n",
    "info = production_model.get_info()\n",
    "print(f\"\\nModel info:\")\n",
    "print(f\"  Version: {info['version']}\")\n",
    "print(f\"  Parameters: {info['num_parameters']:,}\")\n",
    "print(f\"  Accuracy: {info['metadata'].get('metrics', {}).get('accuracy', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Monitoring and Logging\n",
    "\n",
    "Production models need monitoring and logging capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionLogger:\n",
    "    \"\"\"\n",
    "    Logger for tracking predictions and performance.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, log_dir: str):\n",
    "        self.log_dir = Path(log_dir)\n",
    "        self.log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.predictions = []\n",
    "        self.latencies = []\n",
    "        self.start_time = time.time()\n",
    "    \n",
    "    def log_prediction(self, \n",
    "                      input_data: np.ndarray,\n",
    "                      prediction: int,\n",
    "                      probability: float,\n",
    "                      latency_ms: float):\n",
    "        \"\"\"\n",
    "        Log a single prediction.\n",
    "        \n",
    "        Args:\n",
    "            input_data: Input that was predicted on\n",
    "            prediction: Predicted class\n",
    "            probability: Prediction confidence\n",
    "            latency_ms: Inference latency in milliseconds\n",
    "        \"\"\"\n",
    "        entry = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'prediction': int(prediction),\n",
    "            'probability': float(probability),\n",
    "            'latency_ms': float(latency_ms)\n",
    "        }\n",
    "        self.predictions.append(entry)\n",
    "        self.latencies.append(latency_ms)\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get statistics about logged predictions.\n",
    "        \n",
    "        Returns:\n",
    "            Statistics dictionary\n",
    "        \"\"\"\n",
    "        if not self.predictions:\n",
    "            return {}\n",
    "        \n",
    "        latencies = np.array(self.latencies)\n",
    "        predictions = [p['prediction'] for p in self.predictions]\n",
    "        probabilities = [p['probability'] for p in self.predictions]\n",
    "        \n",
    "        return {\n",
    "            'total_predictions': len(self.predictions),\n",
    "            'uptime_seconds': time.time() - self.start_time,\n",
    "            'latency_mean_ms': float(np.mean(latencies)),\n",
    "            'latency_std_ms': float(np.std(latencies)),\n",
    "            'latency_p50_ms': float(np.percentile(latencies, 50)),\n",
    "            'latency_p95_ms': float(np.percentile(latencies, 95)),\n",
    "            'latency_p99_ms': float(np.percentile(latencies, 99)),\n",
    "            'avg_confidence': float(np.mean(probabilities)),\n",
    "            'prediction_distribution': {int(i): predictions.count(i) for i in set(predictions)}\n",
    "        }\n",
    "    \n",
    "    def save_logs(self):\n",
    "        \"\"\"\n",
    "        Save logs to disk.\n",
    "        \"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        log_file = self.log_dir / f\"predictions_{timestamp}.json\"\n",
    "        \n",
    "        log_data = {\n",
    "            'predictions': self.predictions,\n",
    "            'statistics': self.get_stats()\n",
    "        }\n",
    "        \n",
    "        with open(log_file, 'w') as f:\n",
    "            json.dump(log_data, f, indent=2)\n",
    "        \n",
    "        print(f\"Logs saved to {log_file}\")\n",
    "\n",
    "# Create logger\n",
    "logger = PredictionLogger(\"prediction_logs\")\n",
    "\n",
    "# Simulate predictions with logging\n",
    "print(\"Simulating logged predictions...\\n\")\n",
    "for i in range(50):\n",
    "    # Generate test input\n",
    "    test_input = np.random.randn(1, 784)\n",
    "    \n",
    "    # Time prediction\n",
    "    start = time.time()\n",
    "    pred, prob = production_model.predict(test_input, return_probabilities=True)\n",
    "    latency = (time.time() - start) * 1000  # Convert to ms\n",
    "    \n",
    "    # Log\n",
    "    logger.log_prediction(\n",
    "        test_input,\n",
    "        pred[0],\n",
    "        prob[0, pred[0]],\n",
    "        latency\n",
    "    )\n",
    "\n",
    "# Get statistics\n",
    "stats = logger.get_stats()\n",
    "print(\"Prediction Statistics:\")\n",
    "print(f\"  Total predictions: {stats['total_predictions']}\")\n",
    "print(f\"  Average latency: {stats['latency_mean_ms']:.2f} ms\")\n",
    "print(f\"  P95 latency: {stats['latency_p95_ms']:.2f} ms\")\n",
    "print(f\"  P99 latency: {stats['latency_p99_ms']:.2f} ms\")\n",
    "print(f\"  Average confidence: {stats['avg_confidence']:.4f}\")\n",
    "print(f\"\\n  Prediction distribution:\")\n",
    "for class_id, count in sorted(stats['prediction_distribution'].items()):\n",
    "    print(f\"    Class {class_id}: {count}\")\n",
    "\n",
    "# Save logs\n",
    "logger.save_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced: Model Export\n",
    "\n",
    "For deployment to other platforms, we can export model components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelExporter:\n",
    "    \"\"\"\n",
    "    Export models to various formats.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def export_to_numpy(model: bst.graph.Node, filepath: str):\n",
    "        \"\"\"\n",
    "        Export model weights to numpy format.\n",
    "        \n",
    "        Args:\n",
    "            model: Model to export\n",
    "            filepath: Output file path\n",
    "        \"\"\"\n",
    "        states = model.states(bst.ParamState)\n",
    "        \n",
    "        # Organize weights by layer\n",
    "        weights = {}\n",
    "        for key, state in states.items():\n",
    "            # Extract layer name and parameter type from key\n",
    "            parts = key.split('/')\n",
    "            layer_name = '/'.join(parts[:-1])\n",
    "            param_name = parts[-1]\n",
    "            \n",
    "            if layer_name not in weights:\n",
    "                weights[layer_name] = {}\n",
    "            \n",
    "            weights[layer_name][param_name] = np.array(state.value)\n",
    "        \n",
    "        # Save\n",
    "        np.savez_compressed(filepath, **{k: v for layer in weights.values() for k, v in layer.items()})\n",
    "        print(f\"Model exported to {filepath}\")\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    @staticmethod\n",
    "    def export_architecture(model: bst.graph.Node, filepath: str):\n",
    "        \"\"\"\n",
    "        Export model architecture description.\n",
    "        \n",
    "        Args:\n",
    "            model: Model to export\n",
    "            filepath: Output file path\n",
    "        \"\"\"\n",
    "        # Get model structure\n",
    "        nodes = model.nodes().values()\n",
    "        \n",
    "        architecture = {\n",
    "            'model_class': model.__class__.__name__,\n",
    "            'num_parameters': sum(p.value.size for p in model.states(bst.ParamState).values()),\n",
    "            'layers': []\n",
    "        }\n",
    "        \n",
    "        for node in nodes:\n",
    "            layer_info = {\n",
    "                'name': node.__class__.__name__,\n",
    "                'type': str(type(node)),\n",
    "            }\n",
    "            \n",
    "            # Add layer-specific info\n",
    "            if hasattr(node, '__dict__'):\n",
    "                for attr, value in node.__dict__.items():\n",
    "                    if not attr.startswith('_') and isinstance(value, (int, float, str, bool)):\n",
    "                        layer_info[attr] = value\n",
    "            \n",
    "            architecture['layers'].append(layer_info)\n",
    "        \n",
    "        # Save\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(architecture, f, indent=2)\n",
    "        \n",
    "        print(f\"Architecture exported to {filepath}\")\n",
    "        return architecture\n",
    "\n",
    "# Export model\n",
    "exporter = ModelExporter()\n",
    "\n",
    "# Export weights\n",
    "weights = exporter.export_to_numpy(model, \"exported_weights.npz\")\n",
    "\n",
    "# Export architecture\n",
    "arch = exporter.export_architecture(model, \"exported_architecture.json\")\n",
    "\n",
    "print(\"\\nExported architecture:\")\n",
    "print(json.dumps(arch, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we covered:\n",
    "\n",
    "1. **Basic Saving/Loading**: Simple state serialization with NumPy\n",
    "2. **Complete Serialization**: Saving models with metadata using pickle\n",
    "3. **Checkpoint Management**: Periodic saving during training with best model tracking\n",
    "4. **Inference Optimization**: JIT-compiled inference models with benchmarking\n",
    "5. **Batch Processing**: Efficient processing of large datasets\n",
    "6. **Model Versioning**: Registry system for managing multiple model versions\n",
    "7. **Production Deployment**: Complete production-ready model wrapper\n",
    "8. **Monitoring**: Prediction logging and performance tracking\n",
    "9. **Model Export**: Exporting weights and architecture for other platforms\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- Use `model.states(bst.ParamState)` to access all model parameters\n",
    "- Save checkpoints periodically during training\n",
    "- Use JIT compilation for faster inference\n",
    "- Implement batch processing for large datasets\n",
    "- Maintain a model registry for version control\n",
    "- Log predictions for monitoring and debugging\n",
    "- Benchmark inference performance for optimization\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you've completed all BrainState tutorials, you can:\n",
    "\n",
    "1. Build production ML systems with BrainState\n",
    "2. Implement custom deployment pipelines\n",
    "3. Integrate with serving frameworks (Flask, FastAPI, etc.)\n",
    "4. Deploy to cloud platforms (AWS, GCP, Azure)\n",
    "5. Contribute to the BrainState project\n",
    "\n",
    "For more information, visit the [BrainState documentation](https://brainstate.readthedocs.io/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

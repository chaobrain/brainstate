{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Tricks and Training Techniques\n",
    "\n",
    "In this tutorial, we'll explore advanced optimization techniques and training tricks to improve model performance and training stability.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "- Implement learning rate schedules\n",
    "- Apply gradient clipping for stability\n",
    "- Use weight decay for regularization\n",
    "- Implement mixed precision training\n",
    "- Create checkpoint management systems\n",
    "- Apply early stopping\n",
    "- Use gradient accumulation\n",
    "- Monitor training with metrics\n",
    "\n",
    "## What We'll Build\n",
    "\n",
    "We'll create:\n",
    "- Various learning rate schedulers\n",
    "- Gradient clipping utilities\n",
    "- Checkpoint manager\n",
    "- Complete training pipeline with all tricks\n",
    "- Training monitoring dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brainstate as bst\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Optional, Callable\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Set random seed\n",
    "bst.random.seed(42)\n",
    "\n",
    "print(f\"JAX devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Learning Rate Schedules\n",
    "\n",
    "Learning rate schedules adjust the learning rate during training to improve convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRScheduler:\n",
    "    \"\"\"Base class for learning rate schedulers.\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_lr: float):\n",
    "        self.initial_lr = initial_lr\n",
    "        self.current_step = 0\n",
    "    \n",
    "    def step(self) -> float:\n",
    "        \"\"\"Get current learning rate and increment step.\"\"\"\n",
    "        lr = self.get_lr()\n",
    "        self.current_step += 1\n",
    "        return lr\n",
    "    \n",
    "    def get_lr(self) -> float:\n",
    "        \"\"\"Compute current learning rate.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class ConstantLR(LRScheduler):\n",
    "    \"\"\"Constant learning rate.\"\"\"\n",
    "    \n",
    "    def get_lr(self) -> float:\n",
    "        return self.initial_lr\n",
    "\n",
    "class StepLR(LRScheduler):\n",
    "    \"\"\"Decay learning rate by gamma every step_size epochs.\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_lr: float, step_size: int, gamma: float = 0.1):\n",
    "        super().__init__(initial_lr)\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def get_lr(self) -> float:\n",
    "        return self.initial_lr * (self.gamma ** (self.current_step // self.step_size))\n",
    "\n",
    "class ExponentialLR(LRScheduler):\n",
    "    \"\"\"Exponentially decay learning rate.\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_lr: float, gamma: float = 0.95):\n",
    "        super().__init__(initial_lr)\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def get_lr(self) -> float:\n",
    "        return self.initial_lr * (self.gamma ** self.current_step)\n",
    "\n",
    "class CosineAnnealingLR(LRScheduler):\n",
    "    \"\"\"Cosine annealing learning rate schedule.\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_lr: float, T_max: int, eta_min: float = 0):\n",
    "        super().__init__(initial_lr)\n",
    "        self.T_max = T_max\n",
    "        self.eta_min = eta_min\n",
    "    \n",
    "    def get_lr(self) -> float:\n",
    "        return self.eta_min + (self.initial_lr - self.eta_min) * \\\n",
    "               (1 + np.cos(np.pi * self.current_step / self.T_max)) / 2\n",
    "\n",
    "class WarmupCosineSchedule(LRScheduler):\n",
    "    \"\"\"Warmup followed by cosine annealing.\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_lr: float, warmup_steps: int, total_steps: int):\n",
    "        super().__init__(initial_lr)\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "    \n",
    "    def get_lr(self) -> float:\n",
    "        if self.current_step < self.warmup_steps:\n",
    "            # Linear warmup\n",
    "            return self.initial_lr * self.current_step / self.warmup_steps\n",
    "        else:\n",
    "            # Cosine annealing\n",
    "            progress = (self.current_step - self.warmup_steps) / \\\n",
    "                      (self.total_steps - self.warmup_steps)\n",
    "            return self.initial_lr * (1 + np.cos(np.pi * progress)) / 2\n",
    "\n",
    "# Visualize schedules\n",
    "n_steps = 100\n",
    "initial_lr = 0.1\n",
    "\n",
    "schedules = {\n",
    "    'Constant': ConstantLR(initial_lr),\n",
    "    'Step (decay every 30)': StepLR(initial_lr, step_size=30, gamma=0.5),\n",
    "    'Exponential (Î³=0.95)': ExponentialLR(initial_lr, gamma=0.95),\n",
    "    'Cosine Annealing': CosineAnnealingLR(initial_lr, T_max=n_steps),\n",
    "    'Warmup + Cosine': WarmupCosineSchedule(initial_lr, warmup_steps=10, total_steps=n_steps)\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for name, scheduler in schedules.items():\n",
    "    lrs = []\n",
    "    scheduler.current_step = 0\n",
    "    for _ in range(n_steps):\n",
    "        lrs.append(scheduler.step())\n",
    "    plt.plot(lrs, label=name, linewidth=2)\n",
    "\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Schedules Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gradient Clipping\n",
    "\n",
    "Gradient clipping prevents exploding gradients by limiting gradient magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradients_by_norm(grads: Dict, max_norm: float) -> Tuple[Dict, float]:\n",
    "    \"\"\"Clip gradients by global norm.\n",
    "    \n",
    "    Args:\n",
    "        grads: Dictionary of gradients\n",
    "        max_norm: Maximum allowed norm\n",
    "        \n",
    "    Returns:\n",
    "        clipped_grads: Clipped gradients\n",
    "        global_norm: Original global norm\n",
    "    \"\"\"\n",
    "    # Compute global norm\n",
    "    global_norm = jnp.sqrt(\n",
    "        sum(jnp.sum(g ** 2) for g in grads.values())\n",
    "    )\n",
    "    \n",
    "    # Compute clipping factor\n",
    "    clip_factor = jnp.minimum(1.0, max_norm / (global_norm + 1e-6))\n",
    "    \n",
    "    # Clip gradients\n",
    "    clipped_grads = {\n",
    "        k: g * clip_factor for k, g in grads.items()\n",
    "    }\n",
    "    \n",
    "    return clipped_grads, float(global_norm)\n",
    "\n",
    "def clip_gradients_by_value(grads: Dict, clip_value: float) -> Dict:\n",
    "    \"\"\"Clip gradients by value (element-wise).\n",
    "    \n",
    "    Args:\n",
    "        grads: Dictionary of gradients\n",
    "        clip_value: Maximum absolute value\n",
    "        \n",
    "    Returns:\n",
    "        clipped_grads: Clipped gradients\n",
    "    \"\"\"\n",
    "    return {\n",
    "        k: jnp.clip(g, -clip_value, clip_value)\n",
    "        for k, g in grads.items()\n",
    "    }\n",
    "\n",
    "# Demonstrate gradient clipping\n",
    "# Create artificial gradients with some very large values\n",
    "test_grads = {\n",
    "    'weight1': jnp.array([[1.0, 2.0], [3.0, 100.0]]),  # Contains large gradient\n",
    "    'weight2': jnp.array([0.5, 0.3, 0.1]),\n",
    "    'bias': jnp.array([0.1])\n",
    "}\n",
    "\n",
    "# Compute original norm\n",
    "original_norm = jnp.sqrt(sum(jnp.sum(g ** 2) for g in test_grads.values()))\n",
    "print(f\"Original gradient norm: {original_norm:.4f}\")\n",
    "\n",
    "# Clip by norm\n",
    "clipped_by_norm, norm = clip_gradients_by_norm(test_grads, max_norm=5.0)\n",
    "clipped_norm = jnp.sqrt(sum(jnp.sum(g ** 2) for g in clipped_by_norm.values()))\n",
    "print(f\"\\nAfter norm clipping (max_norm=5.0):\")\n",
    "print(f\"  Gradient norm: {clipped_norm:.4f}\")\n",
    "print(f\"  weight1: {clipped_by_norm['weight1']}\")\n",
    "\n",
    "# Clip by value\n",
    "clipped_by_value = clip_gradients_by_value(test_grads, clip_value=2.0)\n",
    "print(f\"\\nAfter value clipping (clip_value=2.0):\")\n",
    "print(f\"  weight1: {clipped_by_value['weight1']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Weight Decay (L2 Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_weight_decay(params: Dict, weight_decay: float) -> Dict:\n",
    "    \"\"\"Apply weight decay (L2 regularization) to parameters.\n",
    "    \n",
    "    Args:\n",
    "        params: Dictionary of parameters\n",
    "        weight_decay: Weight decay coefficient\n",
    "        \n",
    "    Returns:\n",
    "        Updated parameters\n",
    "    \"\"\"\n",
    "    return {\n",
    "        k: p * (1 - weight_decay)\n",
    "        for k, p in params.items()\n",
    "    }\n",
    "\n",
    "def compute_l2_loss(params: Dict) -> float:\n",
    "    \"\"\"Compute L2 regularization loss.\"\"\"\n",
    "    return sum(jnp.sum(p ** 2) for p in params.values())\n",
    "\n",
    "# Demonstrate weight decay\n",
    "test_params = {\n",
    "    'weight': jnp.array([[1.0, 2.0], [3.0, 4.0]]),\n",
    "    'bias': jnp.array([0.5, 0.3])\n",
    "}\n",
    "\n",
    "l2_before = compute_l2_loss(test_params)\n",
    "print(f\"L2 loss before decay: {l2_before:.4f}\")\n",
    "\n",
    "# Apply weight decay multiple times\n",
    "l2_history = [l2_before]\n",
    "params = test_params.copy()\n",
    "\n",
    "for i in range(10):\n",
    "    params = apply_weight_decay(params, weight_decay=0.01)\n",
    "    l2_history.append(compute_l2_loss(params))\n",
    "\n",
    "print(f\"L2 loss after 10 steps: {l2_history[-1]:.4f}\")\n",
    "\n",
    "# Plot L2 loss evolution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(l2_history, 'b-o', linewidth=2, markersize=6)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('L2 Loss')\n",
    "plt.title('Effect of Weight Decay on L2 Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Checkpoint Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckpointManager:\n",
    "    \"\"\"Manage model checkpoints during training.\"\"\"\n",
    "    \n",
    "    def __init__(self, checkpoint_dir: str, max_to_keep: int = 5):\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.max_to_keep = max_to_keep\n",
    "        self.checkpoints = []  # List of (metric, path) tuples\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    def save_checkpoint(\n",
    "        self,\n",
    "        model: bst.graph.Node,\n",
    "        epoch: int,\n",
    "        metric: float,\n",
    "        metadata: Optional[Dict] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Save model checkpoint.\n",
    "        \n",
    "        Args:\n",
    "            model: Model to save\n",
    "            epoch: Current epoch\n",
    "            metric: Metric value (e.g., validation loss)\n",
    "            metadata: Optional metadata to save\n",
    "            \n",
    "        Returns:\n",
    "            Path to saved checkpoint\n",
    "        \"\"\"\n",
    "        # Create checkpoint path\n",
    "        checkpoint_path = os.path.join(\n",
    "            self.checkpoint_dir,\n",
    "            f'checkpoint_epoch_{epoch}_metric_{metric:.4f}.pkl'\n",
    "        )\n",
    "        \n",
    "        # Collect model states\n",
    "        states = {}\n",
    "        for name, state in model.states().items():\n",
    "            states[name] = np.array(state.value)\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'metric': metric,\n",
    "            'states': states,\n",
    "            'metadata': metadata or {}\n",
    "        }\n",
    "        \n",
    "        with open(checkpoint_path, 'wb') as f:\n",
    "            pickle.dump(checkpoint, f)\n",
    "        \n",
    "        # Track checkpoint\n",
    "        self.checkpoints.append((metric, checkpoint_path))\n",
    "        \n",
    "        # Remove old checkpoints if needed\n",
    "        self._cleanup_old_checkpoints()\n",
    "        \n",
    "        return checkpoint_path\n",
    "    \n",
    "    def _cleanup_old_checkpoints(self):\n",
    "        \"\"\"Remove old checkpoints, keeping only the best ones.\"\"\"\n",
    "        if len(self.checkpoints) > self.max_to_keep:\n",
    "            # Sort by metric (ascending)\n",
    "            self.checkpoints.sort(key=lambda x: x[0])\n",
    "            \n",
    "            # Remove worst checkpoints\n",
    "            while len(self.checkpoints) > self.max_to_keep:\n",
    "                _, path_to_remove = self.checkpoints.pop(-1)\n",
    "                if os.path.exists(path_to_remove):\n",
    "                    os.remove(path_to_remove)\n",
    "    \n",
    "    def load_checkpoint(self, model: bst.graph.Node, checkpoint_path: str):\n",
    "        \"\"\"Load model from checkpoint.\n",
    "        \n",
    "        Args:\n",
    "            model: Model to load into\n",
    "            checkpoint_path: Path to checkpoint file\n",
    "        \"\"\"\n",
    "        with open(checkpoint_path, 'rb') as f:\n",
    "            checkpoint = pickle.load(f)\n",
    "        \n",
    "        # Restore states\n",
    "        states = checkpoint['states']\n",
    "        for name, value in states.items():\n",
    "            if name in model.states():\n",
    "                model.states()[name].value = jnp.array(value)\n",
    "        \n",
    "        return checkpoint\n",
    "    \n",
    "    def get_best_checkpoint(self) -> Optional[str]:\n",
    "        \"\"\"Get path to best checkpoint.\"\"\"\n",
    "        if not self.checkpoints:\n",
    "            return None\n",
    "        return min(self.checkpoints, key=lambda x: x[0])[1]\n",
    "\n",
    "# Demonstrate checkpoint manager\n",
    "checkpoint_dir = './temp_checkpoints'\n",
    "manager = CheckpointManager(checkpoint_dir, max_to_keep=3)\n",
    "\n",
    "# Create simple model\n",
    "class SimpleModel(bst.graph.Node):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weight = bst.ParamState(jnp.ones((5, 3)))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return x @ self.weight.value\n",
    "\n",
    "model = SimpleModel()\n",
    "\n",
    "# Simulate saving checkpoints\n",
    "print(\"Saving checkpoints...\")\n",
    "for epoch in range(5):\n",
    "    metric = 1.0 - epoch * 0.1  # Simulated decreasing loss\n",
    "    path = manager.save_checkpoint(model, epoch, metric)\n",
    "    print(f\"Epoch {epoch}: Saved checkpoint with metric={metric:.4f}\")\n",
    "\n",
    "print(f\"\\nBest checkpoint: {manager.get_best_checkpoint()}\")\n",
    "print(f\"Number of checkpoints kept: {len(manager.checkpoints)}\")\n",
    "\n",
    "# Cleanup\n",
    "import shutil\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    shutil.rmtree(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to prevent overfitting.\"\"\"\n",
    "    \n",
    "    def __init__(self, patience: int = 10, min_delta: float = 0.0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.should_stop = False\n",
    "    \n",
    "    def __call__(self, val_loss: float) -> bool:\n",
    "        \"\"\"Check if training should stop.\n",
    "        \n",
    "        Args:\n",
    "            val_loss: Current validation loss\n",
    "            \n",
    "        Returns:\n",
    "            True if training should stop\n",
    "        \"\"\"\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            # Improvement\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            # No improvement\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.should_stop = True\n",
    "        \n",
    "        return self.should_stop\n",
    "\n",
    "# Demonstrate early stopping\n",
    "early_stop = EarlyStopping(patience=5, min_delta=0.01)\n",
    "\n",
    "# Simulated validation losses\n",
    "val_losses = [1.0, 0.8, 0.7, 0.65, 0.64, 0.64, 0.65, 0.64, 0.65, 0.66, 0.65]\n",
    "\n",
    "print(\"Early Stopping Demo:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch, loss in enumerate(val_losses):\n",
    "    should_stop = early_stop(loss)\n",
    "    print(f\"Epoch {epoch}: val_loss={loss:.2f}, \"\n",
    "          f\"counter={early_stop.counter}, \"\n",
    "          f\"best={early_stop.best_loss:.2f}\")\n",
    "    \n",
    "    if should_stop:\n",
    "        print(f\"\\nEarly stopping triggered at epoch {epoch}!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gradient Accumulation\n",
    "\n",
    "Gradient accumulation allows effective large batch sizes with limited memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientAccumulator:\n",
    "    \"\"\"Accumulate gradients over multiple mini-batches.\"\"\"\n",
    "    \n",
    "    def __init__(self, accumulation_steps: int):\n",
    "        self.accumulation_steps = accumulation_steps\n",
    "        self.accumulated_grads = None\n",
    "        self.step_count = 0\n",
    "    \n",
    "    def accumulate(self, grads: Dict) -> Tuple[bool, Optional[Dict]]:\n",
    "        \"\"\"Accumulate gradients.\n",
    "        \n",
    "        Args:\n",
    "            grads: Gradients from current mini-batch\n",
    "            \n",
    "        Returns:\n",
    "            should_update: Whether to update parameters\n",
    "            averaged_grads: Averaged gradients (if should_update=True)\n",
    "        \"\"\"\n",
    "        # Initialize accumulator\n",
    "        if self.accumulated_grads is None:\n",
    "            self.accumulated_grads = {k: jnp.zeros_like(v) for k, v in grads.items()}\n",
    "        \n",
    "        # Accumulate gradients\n",
    "        for k, v in grads.items():\n",
    "            self.accumulated_grads[k] += v\n",
    "        \n",
    "        self.step_count += 1\n",
    "        \n",
    "        # Check if we should update\n",
    "        if self.step_count % self.accumulation_steps == 0:\n",
    "            # Average accumulated gradients\n",
    "            averaged_grads = {\n",
    "                k: v / self.accumulation_steps\n",
    "                for k, v in self.accumulated_grads.items()\n",
    "            }\n",
    "            \n",
    "            # Reset accumulator\n",
    "            self.accumulated_grads = None\n",
    "            \n",
    "            return True, averaged_grads\n",
    "        else:\n",
    "            return False, None\n",
    "\n",
    "# Demonstrate gradient accumulation\n",
    "accumulator = GradientAccumulator(accumulation_steps=4)\n",
    "\n",
    "print(\"Gradient Accumulation Demo:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for step in range(10):\n",
    "    # Simulate gradients\n",
    "    grads = {'weight': jnp.ones(5) * (step + 1)}\n",
    "    \n",
    "    should_update, averaged_grads = accumulator.accumulate(grads)\n",
    "    \n",
    "    if should_update:\n",
    "        print(f\"Step {step}: UPDATE with averaged gradients: {averaged_grads['weight']}\")\n",
    "    else:\n",
    "        print(f\"Step {step}: Accumulate (no update)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Metrics Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsMonitor:\n",
    "    \"\"\"Monitor and visualize training metrics.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.history = {}\n",
    "    \n",
    "    def log(self, **metrics):\n",
    "        \"\"\"Log metrics for current step.\"\"\"\n",
    "        for name, value in metrics.items():\n",
    "            if name not in self.history:\n",
    "                self.history[name] = []\n",
    "            self.history[name].append(float(value))\n",
    "    \n",
    "    def get_latest(self, metric_name: str) -> Optional[float]:\n",
    "        \"\"\"Get latest value of a metric.\"\"\"\n",
    "        if metric_name in self.history and self.history[metric_name]:\n",
    "            return self.history[metric_name][-1]\n",
    "        return None\n",
    "    \n",
    "    def get_best(self, metric_name: str, mode: str = 'min') -> Optional[float]:\n",
    "        \"\"\"Get best value of a metric.\"\"\"\n",
    "        if metric_name not in self.history or not self.history[metric_name]:\n",
    "            return None\n",
    "        \n",
    "        if mode == 'min':\n",
    "            return min(self.history[metric_name])\n",
    "        else:\n",
    "            return max(self.history[metric_name])\n",
    "    \n",
    "    def plot(self, figsize=(14, 10)):\n",
    "        \"\"\"Plot all metrics.\"\"\"\n",
    "        n_metrics = len(self.history)\n",
    "        if n_metrics == 0:\n",
    "            print(\"No metrics to plot\")\n",
    "            return\n",
    "        \n",
    "        # Calculate grid size\n",
    "        n_cols = 2\n",
    "        n_rows = (n_metrics + 1) // 2\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "        if n_rows == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for idx, (name, values) in enumerate(self.history.items()):\n",
    "            ax = axes[idx]\n",
    "            ax.plot(values, linewidth=2)\n",
    "            ax.set_xlabel('Step')\n",
    "            ax.set_ylabel(name)\n",
    "            ax.set_title(f'{name} (best: {self.get_best(name):.4f})')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for idx in range(n_metrics, len(axes)):\n",
    "            axes[idx].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Demonstrate metrics monitor\n",
    "monitor = MetricsMonitor()\n",
    "\n",
    "# Simulate training\n",
    "for epoch in range(50):\n",
    "    train_loss = 1.0 / (epoch + 1) + np.random.randn() * 0.05\n",
    "    val_loss = 1.2 / (epoch + 1) + np.random.randn() * 0.05\n",
    "    train_acc = 1 - 1.0 / (epoch + 2) + np.random.randn() * 0.02\n",
    "    val_acc = 1 - 1.2 / (epoch + 2) + np.random.randn() * 0.02\n",
    "    \n",
    "    monitor.log(\n",
    "        train_loss=train_loss,\n",
    "        val_loss=val_loss,\n",
    "        train_acc=train_acc,\n",
    "        val_acc=val_acc\n",
    "    )\n",
    "\n",
    "# Plot metrics\n",
    "monitor.plot(figsize=(12, 8))\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Best train loss: {monitor.get_best('train_loss'):.4f}\")\n",
    "print(f\"Best val loss: {monitor.get_best('val_loss'):.4f}\")\n",
    "print(f\"Best train acc: {monitor.get_best('train_acc', mode='max'):.4f}\")\n",
    "print(f\"Best val acc: {monitor.get_best('val_acc', mode='max'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Complete Training Pipeline with All Tricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedTrainer:\n",
    "    \"\"\"Advanced trainer with all optimization tricks.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: bst.graph.Node,\n",
    "        lr_scheduler: LRScheduler,\n",
    "        gradient_clip_norm: Optional[float] = None,\n",
    "        weight_decay: float = 0.0,\n",
    "        gradient_accumulation_steps: int = 1,\n",
    "        checkpoint_dir: Optional[str] = None,\n",
    "        early_stopping_patience: Optional[int] = None,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.gradient_clip_norm = gradient_clip_norm\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # Gradient accumulation\n",
    "        self.grad_accumulator = GradientAccumulator(gradient_accumulation_steps)\n",
    "        \n",
    "        # Checkpoint manager\n",
    "        self.checkpoint_manager = CheckpointManager(checkpoint_dir) if checkpoint_dir else None\n",
    "        \n",
    "        # Early stopping\n",
    "        self.early_stopping = EarlyStopping(early_stopping_patience) if early_stopping_patience else None\n",
    "        \n",
    "        # Metrics monitor\n",
    "        self.monitor = MetricsMonitor()\n",
    "    \n",
    "    def train_step(self, x_batch, y_batch):\n",
    "        \"\"\"Single training step.\"\"\"\n",
    "        with bst.environ.context(fit=True):\n",
    "            # Define loss function\n",
    "            def loss_fn():\n",
    "                logits = self.model(jnp.array(x_batch))\n",
    "                # Simple MSE loss for demonstration\n",
    "                return jnp.mean((logits - jnp.array(y_batch)) ** 2)\n",
    "            \n",
    "            # Compute gradients\n",
    "            loss, grads = bst.augment.grad(\n",
    "                loss_fn,\n",
    "                self.model.states(bst.ParamState),\n",
    "                return_value=True\n",
    "            )()\n",
    "            \n",
    "            return float(loss), grads\n",
    "    \n",
    "    def update_parameters(self, grads):\n",
    "        \"\"\"Update model parameters with all tricks.\"\"\"\n",
    "        # Gradient clipping\n",
    "        if self.gradient_clip_norm is not None:\n",
    "            grads, grad_norm = clip_gradients_by_norm(grads, self.gradient_clip_norm)\n",
    "            self.monitor.log(grad_norm=grad_norm)\n",
    "        \n",
    "        # Get learning rate\n",
    "        lr = self.lr_scheduler.step()\n",
    "        self.monitor.log(learning_rate=lr)\n",
    "        \n",
    "        # Update parameters\n",
    "        for name, grad in grads.items():\n",
    "            self.model.states()[name].value -= lr * grad\n",
    "        \n",
    "        # Weight decay\n",
    "        if self.weight_decay > 0:\n",
    "            params = {k: v.value for k, v in self.model.states(bst.ParamState).items()}\n",
    "            params = apply_weight_decay(params, self.weight_decay * lr)\n",
    "            for name, value in params.items():\n",
    "                self.model.states()[name].value = value\n",
    "    \n",
    "    def train(\n",
    "        self,\n",
    "        train_data: Tuple,\n",
    "        val_data: Optional[Tuple] = None,\n",
    "        num_epochs: int = 10,\n",
    "        batch_size: int = 32,\n",
    "    ):\n",
    "        \"\"\"Complete training loop.\"\"\"\n",
    "        X_train, y_train = train_data\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Training\n",
    "            epoch_losses = []\n",
    "            n_samples = len(X_train)\n",
    "            \n",
    "            for start_idx in range(0, n_samples, batch_size):\n",
    "                end_idx = min(start_idx + batch_size, n_samples)\n",
    "                x_batch = X_train[start_idx:end_idx]\n",
    "                y_batch = y_train[start_idx:end_idx]\n",
    "                \n",
    "                # Train step\n",
    "                loss, grads = self.train_step(x_batch, y_batch)\n",
    "                epoch_losses.append(loss)\n",
    "                \n",
    "                # Gradient accumulation\n",
    "                should_update, avg_grads = self.grad_accumulator.accumulate(grads)\n",
    "                \n",
    "                if should_update:\n",
    "                    self.update_parameters(avg_grads)\n",
    "            \n",
    "            # Log training metrics\n",
    "            train_loss = np.mean(epoch_losses)\n",
    "            self.monitor.log(train_loss=train_loss, epoch=epoch)\n",
    "            \n",
    "            # Validation\n",
    "            if val_data is not None:\n",
    "                val_loss = self.evaluate(val_data, batch_size)\n",
    "                self.monitor.log(val_loss=val_loss)\n",
    "                \n",
    "                # Checkpointing\n",
    "                if self.checkpoint_manager is not None:\n",
    "                    self.checkpoint_manager.save_checkpoint(\n",
    "                        self.model, epoch, val_loss\n",
    "                    )\n",
    "                \n",
    "                # Early stopping\n",
    "                if self.early_stopping is not None:\n",
    "                    if self.early_stopping(val_loss):\n",
    "                        print(f\"Early stopping at epoch {epoch}\")\n",
    "                        break\n",
    "            \n",
    "            # Print progress\n",
    "            if epoch % 5 == 0:\n",
    "                print(f\"Epoch {epoch}: train_loss={train_loss:.4f}\" +\n",
    "                      (f\", val_loss={val_loss:.4f}\" if val_data else \"\"))\n",
    "    \n",
    "    def evaluate(self, data: Tuple, batch_size: int) -> float:\n",
    "        \"\"\"Evaluate model.\"\"\"\n",
    "        X, y = data\n",
    "        losses = []\n",
    "        \n",
    "        with bst.environ.context(fit=False):\n",
    "            for start_idx in range(0, len(X), batch_size):\n",
    "                end_idx = min(start_idx + batch_size, len(X))\n",
    "                x_batch = X[start_idx:end_idx]\n",
    "                y_batch = y[start_idx:end_idx]\n",
    "                \n",
    "                logits = self.model(jnp.array(x_batch))\n",
    "                loss = jnp.mean((logits - jnp.array(y_batch)) ** 2)\n",
    "                losses.append(float(loss))\n",
    "        \n",
    "        return np.mean(losses)\n",
    "\n",
    "# Demo with simple model and data\n",
    "class DemoModel(bst.graph.Node):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = bst.nn.Linear(10, 5)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Generate synthetic data\n",
    "X_train = bst.random.randn(200, 10)\n",
    "y_train = bst.random.randn(200, 5)\n",
    "X_val = bst.random.randn(40, 10)\n",
    "y_val = bst.random.randn(40, 5)\n",
    "\n",
    "# Create model and trainer\n",
    "model = DemoModel()\n",
    "scheduler = WarmupCosineSchedule(initial_lr=0.01, warmup_steps=5, total_steps=30)\n",
    "\n",
    "trainer = AdvancedTrainer(\n",
    "    model=model,\n",
    "    lr_scheduler=scheduler,\n",
    "    gradient_clip_norm=1.0,\n",
    "    weight_decay=0.0001,\n",
    "    gradient_accumulation_steps=2,\n",
    "    early_stopping_patience=10\n",
    ")\n",
    "\n",
    "# Train\n",
    "print(\"Training with all optimization tricks...\")\n",
    "print(\"=\" * 70)\n",
    "trainer.train(\n",
    "    train_data=(X_train, y_train),\n",
    "    val_data=(X_val, y_val),\n",
    "    num_epochs=30,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Visualize results\n",
    "trainer.monitor.plot(figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we covered advanced optimization techniques:\n",
    "\n",
    "1. **Learning Rate Schedules**: Constant, Step, Exponential, Cosine, Warmup\n",
    "2. **Gradient Clipping**: By norm and by value\n",
    "3. **Weight Decay**: L2 regularization\n",
    "4. **Checkpoint Management**: Save/load best models\n",
    "5. **Early Stopping**: Prevent overfitting\n",
    "6. **Gradient Accumulation**: Effective large batch sizes\n",
    "7. **Metrics Monitoring**: Track and visualize training\n",
    "8. **Complete Pipeline**: All tricks integrated\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **Learning rate schedules** improve convergence\n",
    "- **Gradient clipping** prevents exploding gradients\n",
    "- **Weight decay** helps generalization\n",
    "- **Checkpoints** save best models\n",
    "- **Early stopping** prevents overfitting\n",
    "- **Gradient accumulation** enables large batches with limited memory\n",
    "- **Monitoring** is essential for debugging training\n",
    "- Combining techniques yields best results\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. Start with warmup + cosine schedule\n",
    "2. Always clip gradients for RNNs\n",
    "3. Use weight decay for regularization\n",
    "4. Save checkpoints regularly\n",
    "5. Monitor gradient norms\n",
    "6. Use early stopping to save time\n",
    "7. Accumulate gradients for large batches\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the final tutorial, we'll cover:\n",
    "- **Model Deployment**: Saving and loading models\n",
    "- Export to different formats\n",
    "- Inference optimization\n",
    "- Batch processing for deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

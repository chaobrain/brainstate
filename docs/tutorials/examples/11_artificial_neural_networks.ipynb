{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cff08e5afff144b2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Building Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c216cfe",
   "metadata": {},
   "source": [
    "Artificial Neural Networks (ANNs), also known as neural networks (NNs), in the fields of machine learning and cognitive science, are mathematical or computational models that mimic the structure and function of biological neural networks (especially the central nervous system of animals, particularly the brain) for estimating or approximating functions. Similar to how neurons in the human brain are interconnected, neurons in an artificial neural network are also interconnected in various layers of the network.\n",
    "\n",
    "Compared to spiking neural networks, the neurons in artificial neural networks are simplified and do not have intrinsic dynamics. The information transmitted between neurons is not discrete action potentials (0 or 1), but continuous floating-point numbers, which can be understood as the firing rate of a neuron at a given time step. Although artificial neural networks were originally inspired by biological brains and can exhibit some properties of biological brains, they are primarily used as powerful models to solve specific problems without focusing on how their parts correspond to biological systems.\n",
    "\n",
    "<center>\n",
    "    <img src=\"../../_static/images/artificial-neural-network-tutorial1.png\" alt=\"bnn\" />\n",
    "</center>\n",
    "<center>\n",
    "    <img src=\"../../_static/images/artificial-neural-network-tutorial2.png\" alt=\"ann\" />\n",
    "</center>\n",
    "\n",
    "## Architecture of Artificial Neural Networks\n",
    "\n",
    "The neurons in artificial neural networks are distributed across different layers, and information propagates forward layer by layer. These layers can be categorized into three types:\n",
    "\n",
    "- Input Layer: The first layer of the artificial neural network, which receives input and passes it to the hidden layers.\n",
    "- Hidden Layers: These layers perform various computations and feature extraction on the input received from the input layer. Typically, there are multiple hidden layers through which the information flows sequentially.\n",
    "- Output Layer: Finally, the output layer receives information from the hidden layers, performs computations, and provides the final result.\n",
    "\n",
    "<center>\n",
    "    <img src=\"../../_static/images/artificial-neural-network-tutorial3.png\" alt=\"ann2\" />\n",
    "</center>\n",
    "\n",
    "Depending on the type of computation, there are many different kinds of layers in an artificial neural network. The simplest type of layer is the linear layer. After receiving the input, the linear layer computes the weighted sum of the inputs, adds a bias term, and produces the output. This can be expressed by the formula:\n",
    "$$\n",
    "\\sum_{\\mathrm{i=1}}^{\\mathrm{n}}\\mathrm{W_i}*\\mathrm{X_i}+\\mathrm{b}。\\tag{1}\n",
    "$$\n",
    "This dot product and summation are linear operations. If more layers are added but only linear operations are used, adding layers will have no effect, as they can be equivalently reduced to a single linear transformation due to the commutative and associative properties. Therefore, we need to add a nonlinear **activation function** to increase the expressive power of the model. The activation function determines whether a neuron is activated. Only activated neurons will have (non-zero) outputs, which is analogous to biological neurons. There are various types of activation functions to choose from, depending on the task. In the neural network implementation in this tutorial, we will use ReLU (Rectified Linear Unit) and Softmax activation functions, which will be explained in more detail later.\n",
    "\n",
    "## Workflow of Artificial Neural Networks\n",
    "\n",
    "Artificial neural networks are data-driven statistical models. We train the model to solve problems not by explicitly writing rules, but by providing training data and allowing the model to learn the solution method.\n",
    "\n",
    "Specifically, we provide a dataset that specifies the input-output relationship, run the model to obtain its current output, and use the **loss function** to compute the difference between the model’s output and the correct output. Then, using the **backpropagation** method, we compute the partial derivatives of the parameters (mainly weights $W$ and biases $b$) layer by layer to obtain the magnitude and direction of parameter optimization. Finally, the **optimizer** is used to optimize the parameters so that the model’s output is as close as possible to the standard output provided by the dataset."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Building Your First Artificial Neural Network\n",
    "\n",
    "Here, we will use `brainstate` to write code to construct a 3-layer Multilayer Perceptron (MLP) for a handwritten digit recognition (MNIST) task as an example.\n",
    "\n",
    "We will input handwritten digit images into the constructed MLP and have the network output the digit represented by each image.\n",
    "\n",
    "<center>\n",
    "    <img src=\"../../_static/images/artificial-neural-network-tutorial5.jpg\" alt=\"mnist mlp\" />\n",
    "</center>\n"
   ],
   "id": "565ad341044fb45d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import brainstate \n",
    "from braintools.metric import softmax_cross_entropy_with_integer_labels"
   ],
   "id": "6f4a53d1eea26ad7"
  },
  {
   "cell_type": "code",
   "id": "8bf694f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T10:12:08.531239Z",
     "start_time": "2025-10-11T10:12:08.525778Z"
    }
   },
   "source": "brainstate.__version__",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "d67a35ad",
   "metadata": {},
   "source": [
    "## Preparing the Dataset\n",
    "\n",
    "First, prepare the dataset. The dataset provides many corresponding \"input-output\" samples. In this task, the input is the image of the handwritten digit, and the output is the label of the digit.\n",
    "\n",
    "The dataset can be divided into a training set and a test set (with no overlap between the two). The model is trained on the training set, adjusting the model's parameters, while the test set is used to evaluate the model's performance without updating parameters."
   ]
  },
  {
   "cell_type": "code",
   "id": "e37df3d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T10:12:33.306922Z",
     "start_time": "2025-10-11T10:12:08.543086Z"
    }
   },
   "source": [
    "dataset = load_dataset('mnist')\n",
    "X_train = np.array(np.stack(dataset['train']['image']), dtype=np.uint8)\n",
    "X_test = np.array(np.stack(dataset['test']['image']), dtype=np.uint8)\n",
    "X_train = (X_train > 0).astype(jnp.float32)\n",
    "X_test = (X_test > 0).astype(jnp.float32)\n",
    "Y_train = np.array(dataset['train']['label'], dtype=np.int32)\n",
    "Y_test = np.array(dataset['test']['label'], dtype=np.int32)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "0cc63f9d",
   "metadata": {},
   "source": [
    "The MNIST training set contains 60,000 samples, and the test set contains 10,000 samples. Each sample is a $28 \\times 28$ grayscale image, and the output is a single label."
   ]
  },
  {
   "cell_type": "code",
   "id": "d3150ef6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T10:12:35.439987Z",
     "start_time": "2025-10-11T10:12:35.383915Z"
    }
   },
   "source": [
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(X_train[0], cmap='gray')\n",
    "plt.show()\n",
    "print(Y_train[0])\n",
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "23747623",
   "metadata": {},
   "source": [
    "For convenience, we need to wrap the dataset into a `Dataset` class for unified processing."
   ]
  },
  {
   "cell_type": "code",
   "id": "5deb09a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T10:12:35.453432Z",
     "start_time": "2025-10-11T10:12:35.444346Z"
    }
   },
   "source": [
    "class Dataset:\n",
    "    def __init__(self, X, Y, batch_size, shuffle=True):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(X))\n",
    "        self.current_index = 0\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.current_index = 0\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        # Check if all samples have been processed\n",
    "        if self.current_index >= len(self.X):\n",
    "            raise StopIteration\n",
    "\n",
    "        # Define the start and end of the current batch\n",
    "        start = self.current_index\n",
    "        end = start + self.batch_size\n",
    "        if end > len(self.X):\n",
    "            end = len(self.X)\n",
    "        \n",
    "        # Update current index\n",
    "        self.current_index = end\n",
    "\n",
    "        # Select batch samples\n",
    "        batch_indices = self.indices[start:end]\n",
    "        batch_X = self.X[batch_indices]\n",
    "        batch_Y = self.Y[batch_indices]\n",
    "\n",
    "        # Ensure batch has consistent shape\n",
    "        if batch_X.ndim == 1:\n",
    "            batch_X = np.expand_dims(batch_X, axis=0)\n",
    "\n",
    "        return batch_X, batch_Y"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "9c426971",
   "metadata": {},
   "source": [
    "During training, the data is generally divided into batches, which are input into the model. The model computes the loss for all samples in a batch and performs gradient backpropagation to optimize the parameters.\n",
    "\n",
    "Training the entire dataset at once would require excessive GPU memory, and training on just one sample at a time would lead to inefficient parallelization, excessive training time, and each parameter update containing information from only a single sample, which is not ideal for convergence over the entire dataset. Using batches is a good balance. Since the test set does not require parameter updates, a larger batch size can be used, depending on the available GPU memory.\n",
    "\n",
    "The training set is generally shuffled (`shuffle=True`) to ensure that each iteration of the training process has different sample combinations, which helps the model converge over the entire dataset. Since the test set does not update parameters, shuffling is not required."
   ]
  },
  {
   "cell_type": "code",
   "id": "8a44b4d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T10:12:35.466658Z",
     "start_time": "2025-10-11T10:12:35.460406Z"
    }
   },
   "source": [
    "# Initialize training and testing datasets\n",
    "batch_size = 32\n",
    "train_dataset = Dataset(X_train, Y_train, batch_size, shuffle=True)\n",
    "test_dataset = Dataset(X_test, Y_test, batch_size, shuffle=False)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "54a36298",
   "metadata": {},
   "source": [
    "## Defining the Model Structure\n",
    "\n",
    "A classic MLP consists of three linear layers: one input layer, one hidden layer, and one output layer.\n",
    "\n",
    "When defining each linear layer, we need to specify the input and output dimensions of the layer (this is straightforward as the size of $W$ is determined by equation (1)):\n",
    "\n",
    "- The linear layer only accepts 1D input, so we use the `flatten()` function to convert the 2D image into a 1D vector. Here, $28*28=784$ is the size of the input to the first linear layer.\n",
    "- Handwritten digit recognition is a 10-class classification task, so the output of the model should be a 10-dimensional vector, with each dimension representing the probability of each digit. Therefore, the output dimension of the final linear layer is $10$.\n",
    "- The hidden layer extracts features from the input, with larger dimensions corresponding to more features and greater expressive power. In this simple task, the hidden layer dimension can be set to a value between $784$ and $10$. If the performance is not satisfactory, the hidden layer dimension can be increased. In more complex tasks, the number of hidden layers can be increased, and the dimensions of hidden layers can exceed the input and output sizes. However, the dimensions of hidden layers generally increase first and then decrease layer by layer.\n",
    "- Note that for adjacent layers, the output dimension of the previous layer is the input dimension of the next layer.\n",
    "\n",
    "<center>\n",
    "    <img src=\"../../_static/images/artificial-neural-network-tutorial5.jpg\" alt=\"mnist mlp\" />\n",
    "</center>\n",
    "\n",
    "As mentioned earlier, activation functions need to be added between linear layers, otherwise the model would be equivalent to a single linear layer. In this example, we use the ReLU (Rectified Linear Unit) activation function, which sets negative values to zero, introducing nonlinearity. The formula for ReLU is:\n",
    "$$\n",
    "\\text{ReLU}(x) = \\max(0, x)\\tag{2}\n",
    "$$\n",
    "\n",
    "<center>\n",
    "    <img src=\"../../_static/images/artificial-neural-network-tutorial4.png\" alt=\"relu\" />\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "331a3ca7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T10:12:35.474460Z",
     "start_time": "2025-10-11T10:12:35.470670Z"
    }
   },
   "source": [
    "# Define MLP model\n",
    "class MLP(brainstate.nn.Module):\n",
    "  def __init__(self, din, dhidden, dout):\n",
    "    super().__init__()\n",
    "    self.linear1 = brainstate.nn.Linear(din, dhidden)   # Define the first linear layer, input dimension is din, output dimension is dhidden       \n",
    "    self.linear2 = brainstate.nn.Linear(dhidden, dhidden)   # Define the second linear layer, input dimension is dhidden, output dimension is dhidden\n",
    "    self.linear3 = brainstate.nn.Linear(dhidden, dout)    # Define the third linear layer, input dimension is dhidden, output dimension is dout (10 classes for MNIST)\n",
    "    self.flatten = brainstate.nn.Flatten(start_axis=1)   # Flatten images to 1D\n",
    "    self.relu = brainstate.nn.ReLU()   # ReLU activation function\n",
    "\n",
    "  def __call__(self, x):\n",
    "    x = self.flatten(x)  # Flatten the input image from 2D to 1D\n",
    "    x = self.linear1(x)  # Pass the flattened input through the first linear layer\n",
    "    x = self.relu(x)      # Alternatively, you can use jax's ReLU function: x = jax.nn.relu(x)\n",
    "    x = self.linear2(x)   # Pass the result through the second linear layer\n",
    "    x = self.relu(x)      # Apply the ReLU activation function\n",
    "    x = self.linear3(x)   # Pass the result through the third linear layer to get the final output\n",
    "\n",
    "    return x"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "347dc916",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T10:12:36.772076Z",
     "start_time": "2025-10-11T10:12:35.477601Z"
    }
   },
   "source": [
    "# Initialize model with input, hidden, and output layer sizes\n",
    "model = MLP(din=28*28, dhidden=512, dout=10)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "b9c94f0e",
   "metadata": {},
   "source": [
    "## Model Optimization\n",
    "\n",
    "After the artificial neural network receives an image input, it outputs a classification result. We need to compare this result with the ground truth and optimize the parameters to make the predicted class probabilities as close as possible to the true classes.\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "During this process, the **loss function** measures the difference between the predicted class probabilities and the true classes. There are many types of loss functions to choose from, depending on the output and task. Here, we use the common cross-entropy loss function for multi-class classification tasks. For a single sample, the formula for the cross-entropy loss is:\n",
    "\n",
    "$$\n",
    "Loss(y_i, \\hat{y_i}) = - \\sum_{i=1}^{N} y_i \\log(\\hat{y_i})\\tag{3}\n",
    "$$\n",
    "\n",
    "where $\\hat{y_i}$ is the predicted probability distribution (the sum of probabilities for all classes is 1), $y_i$ is the true class label (using One-Hot encoding, where only the correct class has a value of 1, and the others are 0), and $N$ is the number of classes. If the model predicts correctly (the probability for the true class is close to 1), the loss will be small; conversely, if the predicted probability for the true class is close to 0, the loss will be large.\n",
    "\n",
    "Here, the model output provided to the loss function is not directly the probability value (the sum of probabilities is not constrained to 1). This is because the loss function `softmax_cross_entropy_with_integer_labels` in `braintools.metric` automatically applies the softmax activation function to convert the model's output into a probability distribution. The formula is:\n",
    "\n",
    "$$\n",
    "\\sigma(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\\tag{4}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{z}$ is the input vector, $z_i$ is the $i$-th element of the input vector, and $K$ is the dimension of the input vector.\n",
    "\n",
    "At the same time, `softmax_cross_entropy_with_integer_labels` can also automatically convert a 1D true class label into One-Hot encoding.\n",
    "\n",
    "### Backpropagation Algorithm\n",
    "\n",
    "**Backpropagation** is the key algorithm used to optimize parameters during neural network training. Its main task is to compute the gradients of each parameter (mainly the weights $W$ and biases $b$) based on the value of the loss function. This algorithm traces the source of the model's prediction error to optimize the parameters. After obtaining the loss value, backpropagation uses the chain rule to compute the partial derivatives of the loss function with respect to each parameter, layer by layer. These partial derivatives (gradients) describe the direction and magnitude of the loss function's change with respect to the parameters and form the basis for optimization.\n",
    "\n",
    "### Optimizer\n",
    "\n",
    "The **optimizer** is an algorithm that determines how to update the network's parameters (mainly weights and biases) using the gradients to reduce the loss value. The basic update rule is:\n",
    "\n",
    "$$\n",
    "w=w-\\eta\\cdot\\frac{\\partial L}{\\partial w}\\tag{5}\n",
    "$$\n",
    "where $w$ is a parameter, $\\eta$ is the learning rate, and $\\frac{\\partial L}{\\partial w}$ is the gradient.\n",
    "\n",
    "There are many types of optimizers available, and here we choose the commonly used Stochastic Gradient Descent (SGD) optimizer.\n",
    "\n",
    "Here, we instantiate the model's optimizer and specify which parameters it will update."
   ]
  },
  {
   "cell_type": "code",
   "id": "375b3e54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T10:12:36.805070Z",
     "start_time": "2025-10-11T10:12:36.791276Z"
    }
   },
   "source": [
    "# Initialize optimizer and register model parameters\n",
    "optimizer = braintools.optim.SGD(lr = 1e-3)   # Initialize SGD optimizer with learning rate\n",
    "optimizer.register_trainable_weights(model.states(brainstate.ParamState))   # Register parameters for optimization"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adadu\\AppData\\Local\\Temp\\ipykernel_6680\\65507359.py:2: DeprecationWarning: braintools.optim module is deprecated and will be removed in a future version. Please use braintools.optim instead.\n",
      "  optimizer = braintools.optim.SGD(lr = 1e-3)   # Initialize SGD optimizer with learning rate\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGD(\n",
       "  momentum=0.0,\n",
       "  nesterov=False,\n",
       "  param_states=<braintools.optim.UniqueStateManager object at 0x0000014ED6874800>,\n",
       "  weight_decay=0.0,\n",
       "  grad_clip_norm=None,\n",
       "  grad_clip_value=None,\n",
       "  step_count=OptimState(\n",
       "    value=ShapedArray(int32[], weak_type=True)\n",
       "  ),\n",
       "  param_groups=[\n",
       "    {\n",
       "      'params': {\n",
       "        ('linear1', 'weight'): ParamState(\n",
       "          value={\n",
       "            'bias': ShapedArray(float32[512]),\n",
       "            'weight': ShapedArray(float32[784,512])\n",
       "          }\n",
       "        ),\n",
       "        ('linear2', 'weight'): ParamState(\n",
       "          value={\n",
       "            'bias': ShapedArray(float32[512]),\n",
       "            'weight': ShapedArray(float32[512,512])\n",
       "          }\n",
       "        ),\n",
       "        ('linear3', 'weight'): ParamState(\n",
       "          value={\n",
       "            'bias': ShapedArray(float32[10]),\n",
       "            'weight': ShapedArray(float32[512,10])\n",
       "          }\n",
       "        )\n",
       "      },\n",
       "      'lr': OptimState(\n",
       "        value=ShapedArray(float32[], weak_type=True)\n",
       "      ),\n",
       "      'weight_decay': 0.0\n",
       "    }\n",
       "  ],\n",
       "  param_groups_opt_states=[],\n",
       "  _schedulers=[],\n",
       "  _lr_scheduler=<braintools.optim.ConstantLR object at 0x0000014ED5218320>,\n",
       "  _base_lr=0.001,\n",
       "  _current_lr=OptimState(...),\n",
       "  tx=GradientTransformationExtraArgs(init=<function chain.<locals>.init_fn at 0x0000014ED6BED760>, update=<function chain.<locals>.update_fn at 0x0000014ED6BEC860>),\n",
       "  opt_state=OptimState(\n",
       "    value=(ScaleByScheduleState(count=ShapedArray(int32[])),)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "d5ecc46c",
   "metadata": {},
   "source": [
    "## Model Training & Testing\n",
    "\n",
    "During each iteration of training with a batch of data, the training process involves:\n",
    "\n",
    "- Inputting the data into the model to get the output\n",
    "- Computing the loss\n",
    "- Computing the gradients\n",
    "- Passing the gradients to the optimizer, which updates the parameters"
   ]
  },
  {
   "cell_type": "code",
   "id": "68c121df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T10:12:36.815788Z",
     "start_time": "2025-10-11T10:12:36.811589Z"
    }
   },
   "source": [
    "# Training step function\n",
    "@brainstate.compile.jit\n",
    "def train_step(batch):\n",
    "  x, y = batch\n",
    "  # Define loss function\n",
    "  def loss_fn():\n",
    "    return softmax_cross_entropy_with_integer_labels(model(x), y).mean()\n",
    "  \n",
    "  # Compute gradients of the loss with respect to model parameters\n",
    "  grads = brainstate.transform.grad(loss_fn, model.states(brainstate.ParamState))()\n",
    "  optimizer.update(grads)   # Update parameters using optimizer"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adadu\\AppData\\Local\\Temp\\ipykernel_6680\\768626274.py:2: DeprecationWarning: Accessing 'jit' from 'brainstate.compile' is deprecated and will be removed in a future version. Use 'brainstate.transform.jit' instead.\n",
      "  @brainstate.compile.jit\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "0fe15316",
   "metadata": {},
   "source": [
    "During each iteration of testing with a batch of data, the testing process does not require computing gradients or updating parameters, but we may choose to compute the accuracy to reflect the training performance:\n",
    "\n",
    "- Inputting the data into the model to get the output\n",
    "- Computing the loss\n",
    "- Computing the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "id": "9a4df640",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T10:12:36.833110Z",
     "start_time": "2025-10-11T10:12:36.828798Z"
    }
   },
   "source": [
    "# Testing step function\n",
    "@brainstate.transform.jit\n",
    "def test_step(batch):\n",
    "  x, y = batch\n",
    "  y_pred = model(x)   # Perform forward pass\n",
    "  loss = softmax_cross_entropy_with_integer_labels(y_pred, y).mean()   # Compute loss\n",
    "  correct = (y_pred.argmax(1) == y).sum()   # Count correct predictions\n",
    "\n",
    "  return {'loss': loss, 'correct': correct}"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "a0a91dfa",
   "metadata": {},
   "source": [
    "The model is typically trained for multiple epochs on the same training set, and after each epoch or several epochs, the performance on the test set is evaluated.\n",
    "\n",
    "In the following example, as the number of training iterations increases, the training loss decreases, and the test accuracy increases, indicating that we have successfully trained a multilayer perceptron to perform handwritten digit classification."
   ]
  },
  {
   "cell_type": "code",
   "id": "9964de29",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-10-11T10:12:36.842211Z"
    }
   },
   "source": [
    "# Execute training and testing\n",
    "total_steps = 20\n",
    "for epoch in range(10):\n",
    "  for step, batch in enumerate(train_dataset):\n",
    "    train_step(batch)   # Perform training step for each batch\n",
    "\n",
    "  # Calculate test loss and accuracy\n",
    "  test_loss, correct = 0, 0\n",
    "  for step_, test_ in enumerate(test_dataset):\n",
    "    logs = test_step(test_)\n",
    "    test_loss += logs['loss']\n",
    "    correct += logs['correct']\n",
    "    test_loss += logs['loss']\n",
    "  test_loss = test_loss / (step_ + 1)\n",
    "  test_accuracy = correct / len(X_test)\n",
    "  print(f\"epoch: {epoch}, test loss: {test_loss}, test accuracy: {test_accuracy}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adadu\\AppData\\Local\\Temp\\ipykernel_6680\\768626274.py:10: DeprecationWarning: Accessing 'grad' from 'brainstate.augment' is deprecated and will be removed in a future version. Use 'brainstate.transform.grad' instead.\n",
      "  grads = brainstate.augment.grad(loss_fn, model.states(brainstate.ParamState))()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, test loss: 1.2674895524978638, test accuracy: 0.8495000004768372\n",
      "epoch: 1, test loss: 0.8735711574554443, test accuracy: 0.8827999830245972\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

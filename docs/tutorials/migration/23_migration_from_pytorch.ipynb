{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 23: Migration from PyTorch to BrainState\n",
    "\n",
    "In this tutorial, we'll learn how to migrate models and code from PyTorch to BrainState.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "- Understand the key differences between PyTorch and BrainState\n",
    "- Learn API mappings between the two frameworks\n",
    "- Convert PyTorch models to BrainState\n",
    "- Migrate training loops and optimizers\n",
    "- Avoid common pitfalls during migration\n",
    "- Follow best practices for BrainState development\n",
    "\n",
    "## Introduction\n",
    "\n",
    "BrainState and PyTorch share similar goals but have different design philosophies:\n",
    "\n",
    "**PyTorch**:\n",
    "- Imperative, eager execution\n",
    "- Object-oriented with mutable state\n",
    "- Dynamic computation graphs\n",
    "- CPU/GPU with manual device management\n",
    "\n",
    "**BrainState**:\n",
    "- Built on JAX (functional + transformations)\n",
    "- Explicit state management with immutable data\n",
    "- JIT compilation for performance\n",
    "- Automatic device placement\n",
    "- Designed for brain modeling and neuroscience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brainstate as bst\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "\n",
    "# Set random seed\n",
    "bst.random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. API Comparison\n",
    "\n",
    "### 1.1 Module Definition\n",
    "\n",
    "Let's compare how to define a simple neural network module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch Version\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PyTorchMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Usage\n",
    "model = PyTorchMLP(784, 128, 10)\n",
    "output = model(torch.randn(32, 784))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BrainState Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainStateMLP(bst.graph.Node):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = bst.nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = bst.nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = jax.nn.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Usage\n",
    "model = BrainStateMLP(784, 128, 10)\n",
    "output = model(bst.random.randn(32, 784))\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Differences**:\n",
    "1. **Base class**: `nn.Module` → `bst.graph.Node`\n",
    "2. **Method name**: `forward()` → `__call__()`\n",
    "3. **Activation functions**: `torch.relu()` → `jax.nn.relu()`\n",
    "4. **Random numbers**: `torch.randn()` → `bst.random.randn()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Layer Mappings\n",
    "\n",
    "Common layer translations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping table for visualization\n",
    "layer_mappings = [\n",
    "    (\"nn.Linear\", \"bst.nn.Linear\", \"Fully connected layer\"),\n",
    "    (\"nn.Conv2d\", \"bst.nn.Conv2d\", \"2D convolution\"),\n",
    "    (\"nn.BatchNorm2d\", \"bst.nn.BatchNorm2d\", \"Batch normalization\"),\n",
    "    (\"nn.LayerNorm\", \"bst.nn.LayerNorm\", \"Layer normalization\"),\n",
    "    (\"nn.Dropout\", \"bst.nn.Dropout\", \"Dropout regularization\"),\n",
    "    (\"nn.RNN\", \"bst.nn.RNNCell\", \"Recurrent layer (use Cell)\"),\n",
    "    (\"nn.LSTM\", \"bst.nn.LSTMCell\", \"LSTM layer (use Cell)\"),\n",
    "    (\"nn.GRU\", \"bst.nn.GRUCell\", \"GRU layer (use Cell)\"),\n",
    "]\n",
    "\n",
    "print(\"PyTorch → BrainState Layer Mappings\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'PyTorch':<20} {'BrainState':<25} {'Description':<35}\")\n",
    "print(\"-\" * 80)\n",
    "for pytorch, brainstate, desc in layer_mappings:\n",
    "    print(f\"{pytorch:<20} {brainstate:<25} {desc:<35}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_mappings = [\n",
    "    (\"torch.relu\", \"jax.nn.relu\"),\n",
    "    (\"torch.sigmoid\", \"jax.nn.sigmoid\"),\n",
    "    (\"torch.tanh\", \"jax.nn.tanh\"),\n",
    "    (\"torch.softmax\", \"jax.nn.softmax\"),\n",
    "    (\"F.gelu\", \"jax.nn.gelu\"),\n",
    "    (\"F.leaky_relu\", \"jax.nn.leaky_relu\"),\n",
    "    (\"F.elu\", \"jax.nn.elu\"),\n",
    "    (\"F.silu\", \"jax.nn.silu\"),\n",
    "]\n",
    "\n",
    "print(\"\\nActivation Function Mappings\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'PyTorch':<30} {'BrainState/JAX':<30}\")\n",
    "print(\"-\" * 60)\n",
    "for pytorch, jax_fn in activation_mappings:\n",
    "    print(f\"{pytorch:<30} {jax_fn:<30}\")\n",
    "\n",
    "# Test activations\n",
    "x = jnp.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "print(\"\\nExample activations on [-2, -1, 0, 1, 2]:\")\n",
    "print(f\"ReLU: {jax.nn.relu(x)}\")\n",
    "print(f\"Sigmoid: {jax.nn.sigmoid(x)}\")\n",
    "print(f\"Tanh: {jax.nn.tanh(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Common Tensor Operations\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create sample tensors\n",
    "x_jax = jnp.array([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "operations = [\n",
    "    (\"Tensor creation\", \"torch.tensor([1,2,3])\", \"jnp.array([1,2,3])\"),\n",
    "    (\"Random normal\", \"torch.randn(2, 3)\", \"bst.random.randn(2, 3)\"),\n",
    "    (\"Zeros\", \"torch.zeros(2, 3)\", \"jnp.zeros((2, 3))\"),\n",
    "    (\"Ones\", \"torch.ones(2, 3)\", \"jnp.ones((2, 3))\"),\n",
    "    (\"Reshape\", \"x.view(3, 2)\", \"x.reshape(3, 2)\"),\n",
    "    (\"Transpose\", \"x.T\", \"x.T\"),\n",
    "    (\"Concatenate\", \"torch.cat([x, y], dim=0)\", \"jnp.concatenate([x, y], axis=0)\"),\n",
    "    (\"Stack\", \"torch.stack([x, y], dim=0)\", \"jnp.stack([x, y], axis=0)\"),\n",
    "    (\"Mean\", \"x.mean()\", \"jnp.mean(x)\"),\n",
    "    (\"Sum\", \"x.sum(dim=1)\", \"jnp.sum(x, axis=1)\"),\n",
    "    (\"Max\", \"x.max(dim=1)\", \"jnp.max(x, axis=1)\"),\n",
    "    (\"Argmax\", \"x.argmax(dim=1)\", \"jnp.argmax(x, axis=1)\"),\n",
    "]\n",
    "\n",
    "print(f\"{'Operation':<20} {'PyTorch':<30} {'JAX/BrainState':<30}\")\n",
    "print(\"-\" * 80)\n",
    "for op_name, pytorch, jax_code in operations:\n",
    "    print(f\"{op_name:<20} {pytorch:<30} {jax_code:<30}\")\n",
    "\n",
    "# Demonstrate some operations\n",
    "print(\"\\nExamples:\")\n",
    "print(f\"Original: \\n{x_jax}\")\n",
    "print(f\"\\nReshape (3, 2): \\n{x_jax.reshape(3, 2)}\")\n",
    "print(f\"\\nSum along axis 1: {jnp.sum(x_jax, axis=1)}\")\n",
    "print(f\"Argmax along axis 1: {jnp.argmax(x_jax, axis=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note**: In JAX/BrainState, use `axis` instead of `dim` for dimension specifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Converting PyTorch Models\n",
    "\n",
    "Let's convert a complete PyTorch CNN model to BrainState."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 PyTorch CNN (Reference)\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PyTorchCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Conv block 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Conv block 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Flatten and FC\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 BrainState CNN (Converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainStateCNN(bst.graph.Node):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = bst.nn.Conv2d(1, 32, kernel_size=(3, 3), padding='SAME')\n",
    "        self.bn1 = bst.nn.BatchNorm2d(32)\n",
    "        self.conv2 = bst.nn.Conv2d(32, 64, kernel_size=(3, 3), padding='SAME')\n",
    "        self.bn2 = bst.nn.BatchNorm2d(64)\n",
    "        self.dropout = bst.nn.Dropout(0.25)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = bst.nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = bst.nn.Linear(128, num_classes)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Conv block 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        x = bst.functional.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        \n",
    "        # Conv block 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        x = bst.functional.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        \n",
    "        # Flatten and FC\n",
    "        x = x.reshape(x.shape[0], -1)  # Flatten\n",
    "        x = self.dropout(x)\n",
    "        x = jax.nn.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create and test the model\n",
    "model = BrainStateCNN(num_classes=10)\n",
    "\n",
    "# Test with dummy input (batch_size=2, channels=1, height=28, width=28)\n",
    "dummy_input = bst.random.randn(2, 1, 28, 28)\n",
    "output = model(dummy_input)\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Model parameters: {sum(p.value.size for p in model.states(bst.ParamState).values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Conversion Points**:\n",
    "\n",
    "1. **Base class**: `nn.Module` → `bst.graph.Node`\n",
    "2. **Method**: `forward()` → `__call__()`\n",
    "3. **Kernel size**: `kernel_size=3` → `kernel_size=(3, 3)` (tuple required)\n",
    "4. **Padding**: `padding=1` → `padding='SAME'` (string or explicit padding)\n",
    "5. **Pooling**: `nn.MaxPool2d(2, 2)` → `bst.functional.max_pool(...)`\n",
    "6. **Flatten**: `x.view(...)` → `x.reshape(...)`\n",
    "7. **Activation**: `F.relu(x)` → `jax.nn.relu(x)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Loop Migration\n",
    "\n",
    "### 3.1 PyTorch Training Loop (Reference)\n",
    "\n",
    "```python\n",
    "import torch.optim as optim\n",
    "\n",
    "# PyTorch training\n",
    "model = PyTorchCNN()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 BrainState Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"BrainState trainer class.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: bst.graph.Node, learning_rate: float = 0.001):\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Get trainable parameters\n",
    "        self.params = model.states(bst.ParamState)\n",
    "        \n",
    "        # Initialize optimizer state (for Adam)\n",
    "        self.m = {k: jnp.zeros_like(v.value) for k, v in self.params.items()}  # First moment\n",
    "        self.v = {k: jnp.zeros_like(v.value) for k, v in self.params.items()}  # Second moment\n",
    "        self.t = 0  # Time step\n",
    "        \n",
    "        # Adam hyperparameters\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.eps = 1e-8\n",
    "    \n",
    "    def loss_fn(self, x: jnp.ndarray, y: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"Compute cross-entropy loss.\"\"\"\n",
    "        logits = self.model(x)\n",
    "        # Cross-entropy loss\n",
    "        log_probs = jax.nn.log_softmax(logits, axis=-1)\n",
    "        loss = -jnp.mean(jnp.sum(y * log_probs, axis=-1))\n",
    "        return loss\n",
    "    \n",
    "    def train_step(self, x: jnp.ndarray, y: jnp.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Single training step.\n",
    "        \n",
    "        Args:\n",
    "            x: Input batch\n",
    "            y: Target labels (one-hot encoded)\n",
    "            \n",
    "        Returns:\n",
    "            Loss value\n",
    "        \"\"\"\n",
    "        # Compute loss and gradients\n",
    "        loss, grads = bst.transform.grad(self.loss_fn, grad_states=self.params, return_value=True)(x, y)\n",
    "        \n",
    "        # Adam optimizer update\n",
    "        self.t += 1\n",
    "        for key in self.params.keys():\n",
    "            g = grads[key]\n",
    "            \n",
    "            # Update biased first moment estimate\n",
    "            self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * g\n",
    "            \n",
    "            # Update biased second moment estimate\n",
    "            self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * (g ** 2)\n",
    "            \n",
    "            # Compute bias-corrected moment estimates\n",
    "            m_hat = self.m[key] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[key] / (1 - self.beta2 ** self.t)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.params[key].value = self.params[key].value - self.learning_rate * m_hat / (jnp.sqrt(v_hat) + self.eps)\n",
    "        \n",
    "        return float(loss)\n",
    "    \n",
    "    def train_epoch(self, train_data: List[Tuple[np.ndarray, np.ndarray]]) -> float:\n",
    "        \"\"\"\n",
    "        Train for one epoch.\n",
    "        \n",
    "        Args:\n",
    "            train_data: List of (x, y) tuples\n",
    "            \n",
    "        Returns:\n",
    "            Average loss\n",
    "        \"\"\"\n",
    "        total_loss = 0.0\n",
    "        for x_batch, y_batch in train_data:\n",
    "            loss = self.train_step(jnp.array(x_batch), jnp.array(y_batch))\n",
    "            total_loss += loss\n",
    "        return total_loss / len(train_data)\n",
    "\n",
    "# Example usage\n",
    "model = BrainStateCNN(num_classes=10)\n",
    "trainer = Trainer(model, learning_rate=0.001)\n",
    "\n",
    "# Generate dummy training data\n",
    "num_batches = 5\n",
    "batch_size = 32\n",
    "train_data = [\n",
    "    (np.random.randn(batch_size, 1, 28, 28), \n",
    "     np.eye(10)[np.random.randint(0, 10, batch_size)])  # One-hot labels\n",
    "    for _ in range(num_batches)\n",
    "]\n",
    "\n",
    "# Train for a few epochs\n",
    "for epoch in range(3):\n",
    "    avg_loss = trainer.train_epoch(train_data)\n",
    "    print(f\"Epoch {epoch + 1}: Average Loss = {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Differences in Training**:\n",
    "\n",
    "1. **No `.backward()`**: Use `bst.transform.grad()` for automatic differentiation\n",
    "2. **Explicit state management**: Access parameters via `model.states(bst.ParamState)`\n",
    "3. **Manual optimizer**: Implement optimizer logic explicitly (or use optax library)\n",
    "4. **Functional approach**: Loss function takes inputs and computes output\n",
    "5. **One-hot encoding**: Labels should be one-hot encoded for cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Common Pitfalls and Solutions\n",
    "\n",
    "### 4.1 Mutable vs Immutable State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pitfall 1: Mutable vs Immutable State\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# WRONG: JAX arrays are immutable\n",
    "print(\"\\n❌ WRONG (PyTorch-style):\")\n",
    "print(\"\"\"\n",
    "x = jnp.array([1, 2, 3])\n",
    "x[0] = 10  # TypeError: JAX arrays are immutable\n",
    "\"\"\")\n",
    "\n",
    "# CORRECT: Create new array\n",
    "print(\"✓ CORRECT (BrainState way):\")\n",
    "x = jnp.array([1, 2, 3])\n",
    "x_new = x.at[0].set(10)  # Returns new array\n",
    "print(f\"Original: {x}\")\n",
    "print(f\"Modified: {x_new}\")\n",
    "\n",
    "# For model parameters, use .value assignment\n",
    "print(\"\\n✓ For model parameters:\")\n",
    "print(\"\"\"\n",
    "# Create a parameter state\n",
    "param = bst.ParamState(jnp.zeros(10))\n",
    "\n",
    "# Update parameter value\n",
    "param.value = jnp.ones(10)  # This is correct\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Device Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPitfall 2: Device Management\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# PyTorch requires explicit device management\n",
    "print(\"\\nPyTorch (manual):\")\n",
    "print(\"\"\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "x = x.to(device)\n",
    "\"\"\")\n",
    "\n",
    "# BrainState/JAX handles devices automatically\n",
    "print(\"BrainState/JAX (automatic):\")\n",
    "print(\"\"\"\n",
    "# JAX automatically uses GPU if available\n",
    "model = BrainStateCNN()\n",
    "x = bst.random.randn(32, 1, 28, 28)\n",
    "output = model(x)  # Automatically runs on best available device\n",
    "\"\"\")\n",
    "\n",
    "# Check available devices\n",
    "devices = jax.devices()\n",
    "print(f\"\\nAvailable devices: {devices}\")\n",
    "print(f\"Default device: {devices[0].device_kind}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Dimension/Axis Naming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pitfall 3: Dimension vs Axis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "x = jnp.array([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# PyTorch uses 'dim'\n",
    "print(\"\\nPyTorch:\")\n",
    "print(\"x.sum(dim=1)  # Sum along dimension 1\")\n",
    "\n",
    "# JAX uses 'axis'\n",
    "print(\"\\nJAX/BrainState:\")\n",
    "result = jnp.sum(x, axis=1)\n",
    "print(f\"jnp.sum(x, axis=1) = {result}\")\n",
    "\n",
    "# Multiple axes\n",
    "x3d = bst.random.randn(2, 3, 4)\n",
    "print(f\"\\nSum over axes (0, 2): {jnp.sum(x3d, axis=(0, 2)).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 In-place Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pitfall 4: In-place Operations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nPyTorch (in-place allowed):\")\n",
    "print(\"\"\"\n",
    "x.add_(1)  # In-place addition\n",
    "x.relu_()  # In-place ReLU\n",
    "\"\"\")\n",
    "\n",
    "print(\"JAX/BrainState (no in-place, functional):\")\n",
    "x = jnp.array([1.0, 2.0, 3.0])\n",
    "x_new = x + 1  # Returns new array\n",
    "x_relu = jax.nn.relu(x)  # Returns new array\n",
    "print(f\"Original: {x}\")\n",
    "print(f\"After +1: {x_new}\")\n",
    "print(f\"After ReLU: {x_relu}\")\n",
    "\n",
    "print(\"\\n✓ For efficient updates, JAX has .at syntax:\")\n",
    "x = jnp.array([1, 2, 3, 4, 5])\n",
    "x_updated = x.at[2:4].set(0)  # Set indices 2:4 to 0\n",
    "print(f\"Updated: {x_updated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Random Number Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pitfall 5: Random Number Generation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nPyTorch (global state):\")\n",
    "print(\"\"\"\n",
    "torch.manual_seed(42)\n",
    "x = torch.randn(2, 3)\n",
    "y = torch.randn(2, 3)  # Different from x\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nJAX (explicit PRNG key - more complex):\")\n",
    "print(\"\"\"\n",
    "key = jax.random.PRNGKey(42)\n",
    "key, subkey = jax.random.split(key)\n",
    "x = jax.random.normal(subkey, (2, 3))\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n✓ BrainState (simplified):\")\n",
    "bst.random.seed(42)\n",
    "x = bst.random.randn(2, 3)\n",
    "y = bst.random.randn(2, 3)\n",
    "print(f\"x:\\n{x}\")\n",
    "print(f\"y:\\n{y}\")\n",
    "print(\"\\nBrainState manages PRNG keys automatically!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Gradient Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pitfall 6: Gradient Computation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nPyTorch (autograd with computational graph):\")\n",
    "print(\"\"\"\n",
    "x = torch.randn(10, requires_grad=True)\n",
    "y = (x ** 2).sum()\n",
    "y.backward()\n",
    "grad = x.grad  # Access gradients\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nBrainState (functional gradient):\")\n",
    "\n",
    "# Define a simple function\n",
    "def f(x):\n",
    "    return jnp.sum(x ** 2)\n",
    "\n",
    "# Compute gradient\n",
    "x = jnp.array([1.0, 2.0, 3.0])\n",
    "grad_f = jax.grad(f)\n",
    "gradient = grad_f(x)\n",
    "\n",
    "print(f\"x = {x}\")\n",
    "print(f\"f(x) = sum(x^2) = {f(x)}\")\n",
    "print(f\"grad f(x) = 2*x = {gradient}\")\n",
    "\n",
    "# For models, use bst.transform.grad\n",
    "print(\"\\n✓ For models with parameters:\")\n",
    "print(\"\"\"\n",
    "params = model.states(bst.ParamState)\n",
    "loss, grads = bst.transform.grad(\n",
    "    loss_fn, \n",
    "    grad_states=params,\n",
    "    return_value=True\n",
    ")(x, y)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Best Practices for BrainState\n",
    "\n",
    "### 5.1 Use JIT Compilation for Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"Best Practice 1: JIT Compilation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a simple model\n",
    "model = BrainStateMLP(784, 128, 10)\n",
    "x = bst.random.randn(100, 784)\n",
    "\n",
    "# Without JIT\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    _ = model(x)\n",
    "time_no_jit = time.time() - start\n",
    "\n",
    "# With JIT\n",
    "@bst.transform.jit\n",
    "def predict_jit(x):\n",
    "    return model(x)\n",
    "\n",
    "# Warmup\n",
    "_ = predict_jit(x)\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    _ = predict_jit(x)\n",
    "time_jit = time.time() - start\n",
    "\n",
    "print(f\"Without JIT: {time_no_jit:.4f}s\")\n",
    "print(f\"With JIT: {time_jit:.4f}s\")\n",
    "print(f\"Speedup: {time_no_jit / time_jit:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Organize State Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nBest Practice 2: Organize State Management\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class WellOrganizedModel(bst.graph.Node):\n",
    "    \"\"\"Example of good state management.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Parameters (trainable)\n",
    "        self.fc1 = bst.nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = bst.nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        # Short-term state (reset between sequences)\n",
    "        self.hidden = bst.ShortTermState(jnp.zeros(hidden_dim))\n",
    "        \n",
    "        # Long-term state (accumulated statistics)\n",
    "        self.num_calls = bst.LongTermState(jnp.array(0))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Update statistics\n",
    "        self.num_calls.value = self.num_calls.value + 1\n",
    "        \n",
    "        # Use hidden state\n",
    "        x = self.fc1(x) + self.hidden.value\n",
    "        x = jax.nn.relu(x)\n",
    "        \n",
    "        # Update hidden state\n",
    "        self.hidden.value = x\n",
    "        \n",
    "        # Output\n",
    "        return self.fc2(x)\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset short-term state.\"\"\"\n",
    "        self.hidden.value = jnp.zeros_like(self.hidden.value)\n",
    "\n",
    "# Usage\n",
    "model = WellOrganizedModel(10, 20, 5)\n",
    "x = bst.random.randn(1, 10)\n",
    "\n",
    "for i in range(3):\n",
    "    y = model(x)\n",
    "    print(f\"Call {i+1}: num_calls={model.num_calls.value}\")\n",
    "\n",
    "print(\"\\nState types:\")\n",
    "print(f\"  ParamState: {len(model.states(bst.ParamState))} groups\")\n",
    "print(f\"  ShortTermState: {len(model.states(bst.ShortTermState))} states\")\n",
    "print(f\"  LongTermState: {len(model.states(bst.LongTermState))} states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Use Vectorization (vmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Practice 3: Vectorization with vmap\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Function that processes single sample\n",
    "def process_single(x):\n",
    "    return jnp.sum(x ** 2)\n",
    "\n",
    "# Process batch with loop (slow)\n",
    "batch = bst.random.randn(100, 10)\n",
    "\n",
    "start = time.time()\n",
    "results_loop = jnp.array([process_single(x) for x in batch])\n",
    "time_loop = time.time() - start\n",
    "\n",
    "# Process batch with vmap (fast)\n",
    "process_batch = jax.vmap(process_single)\n",
    "\n",
    "start = time.time()\n",
    "results_vmap = process_batch(batch)\n",
    "time_vmap = time.time() - start\n",
    "\n",
    "print(f\"Loop: {time_loop:.6f}s\")\n",
    "print(f\"vmap: {time_vmap:.6f}s\")\n",
    "print(f\"Speedup: {time_loop / time_vmap:.2f}x\")\n",
    "print(f\"Results match: {jnp.allclose(results_loop, results_vmap)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Debugging Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Practice 4: Debugging\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Enable debugging mode\n",
    "print(\"\\n1. Check for NaN/Inf:\")\n",
    "x = jnp.array([1.0, 2.0, jnp.nan, 4.0])\n",
    "print(f\"  Has NaN: {jnp.any(jnp.isnan(x))}\")\n",
    "print(f\"  Has Inf: {jnp.any(jnp.isinf(x))}\")\n",
    "\n",
    "print(\"\\n2. Use jax.debug for printing inside JIT:\")\n",
    "print(\"\"\"\n",
    "@bst.transform.jit\n",
    "def f(x):\n",
    "    jax.debug.print(\"x = {}\", x)  # Print inside JIT\n",
    "    return x ** 2\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n3. Disable JIT for debugging:\")\n",
    "print(\"\"\"\n",
    "with jax.disable_jit():\n",
    "    output = model(x)  # Run without JIT compilation\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n4. Check shapes:\")\n",
    "model = BrainStateMLP(784, 128, 10)\n",
    "x = bst.random.randn(32, 784)\n",
    "y = model(x)\n",
    "print(f\"  Input shape: {x.shape}\")\n",
    "print(f\"  Output shape: {y.shape}\")\n",
    "print(f\"  Expected: (32, 10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complete Migration Example\n",
    "\n",
    "Let's convert a complete PyTorch training script to BrainState."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete BrainState training example\n",
    "class CompleteTrainingExample:\n",
    "    \"\"\"Complete training pipeline in BrainState.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Model\n",
    "        self.model = BrainStateCNN(num_classes=10)\n",
    "        \n",
    "        # Initialize with dummy input\n",
    "        dummy = bst.random.randn(1, 1, 28, 28)\n",
    "        _ = self.model(dummy)\n",
    "        \n",
    "        # Trainer\n",
    "        self.trainer = Trainer(self.model, learning_rate=0.001)\n",
    "        \n",
    "        # Metrics\n",
    "        self.train_losses = []\n",
    "        self.val_accuracies = []\n",
    "    \n",
    "    def evaluate(self, val_data: List[Tuple[np.ndarray, np.ndarray]]) -> float:\n",
    "        \"\"\"Evaluate model accuracy.\"\"\"\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for x_batch, y_batch in val_data:\n",
    "            x = jnp.array(x_batch)\n",
    "            y = jnp.array(y_batch)\n",
    "            \n",
    "            logits = self.model(x)\n",
    "            predictions = jnp.argmax(logits, axis=-1)\n",
    "            labels = jnp.argmax(y, axis=-1)\n",
    "            \n",
    "            correct += jnp.sum(predictions == labels)\n",
    "            total += len(labels)\n",
    "        \n",
    "        return float(correct / total)\n",
    "    \n",
    "    def train(self, train_data, val_data, num_epochs=5):\n",
    "        \"\"\"Train the model.\"\"\"\n",
    "        print(\"Training started...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Train\n",
    "            train_loss = self.trainer.train_epoch(train_data)\n",
    "            self.train_losses.append(train_loss)\n",
    "            \n",
    "            # Evaluate\n",
    "            val_acc = self.evaluate(val_data)\n",
    "            self.val_accuracies.append(val_acc)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}: \"\n",
    "                  f\"Loss={train_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"Training completed!\")\n",
    "    \n",
    "    def plot_metrics(self):\n",
    "        \"\"\"Plot training metrics.\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        ax1.plot(self.train_losses, 'o-')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title('Training Loss')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        ax2.plot(self.val_accuracies, 's-', color='green')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.set_title('Validation Accuracy')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Generate dummy dataset\n",
    "def generate_dummy_data(num_batches=10, batch_size=32):\n",
    "    \"\"\"Generate dummy MNIST-like data.\"\"\"\n",
    "    return [\n",
    "        (np.random.randn(batch_size, 1, 28, 28),\n",
    "         np.eye(10)[np.random.randint(0, 10, batch_size)])\n",
    "        for _ in range(num_batches)\n",
    "    ]\n",
    "\n",
    "# Run training\n",
    "train_data = generate_dummy_data(num_batches=10, batch_size=32)\n",
    "val_data = generate_dummy_data(num_batches=5, batch_size=32)\n",
    "\n",
    "trainer_example = CompleteTrainingExample()\n",
    "trainer_example.train(train_data, val_data, num_epochs=5)\n",
    "trainer_example.plot_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways for Migration:\n",
    "\n",
    "1. **API Mappings**:\n",
    "   - `nn.Module` → `bst.graph.Node`\n",
    "   - `forward()` → `__call__()`\n",
    "   - `torch.*` → `jax.* / jnp.*`\n",
    "   - `dim` → `axis`\n",
    "\n",
    "2. **Functional Programming**:\n",
    "   - JAX arrays are immutable\n",
    "   - Use functional transformations (jit, grad, vmap)\n",
    "   - Explicit state management with State objects\n",
    "\n",
    "3. **Training**:\n",
    "   - Use `bst.transform.grad()` for gradients\n",
    "   - Implement optimizer manually or use optax\n",
    "   - Access parameters via `model.states(bst.ParamState)`\n",
    "\n",
    "4. **Best Practices**:\n",
    "   - Use JIT compilation for performance\n",
    "   - Vectorize with vmap when possible\n",
    "   - Organize state types appropriately\n",
    "   - Debug without JIT first\n",
    "\n",
    "5. **Common Pitfalls**:\n",
    "   - No in-place operations\n",
    "   - Explicit random key management (simplified in BrainState)\n",
    "   - Use `axis` not `dim`\n",
    "   - Tuple kernel sizes for Conv layers\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Practice converting your own PyTorch models\n",
    "- Explore BrainState-specific features (dynamics, brain modeling)\n",
    "- Learn BrainPy integration (next tutorial)\n",
    "- Optimize with JAX transformations (jit, vmap, pmap)\n",
    "\n",
    "For more information, visit the [BrainState documentation](https://brainstate.readthedocs.io/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

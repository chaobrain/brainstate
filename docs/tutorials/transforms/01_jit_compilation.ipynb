{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# JIT Compilation in BrainState\n",
    "\n",
    "Just-In-Time (JIT) compilation is one of JAX's most powerful features, enabling dramatic performance improvements by compiling Python functions to optimized machine code. BrainState extends JAX's `jit` with seamless support for stateful modules and the state management system.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "This tutorial covers:\n",
    "\n",
    "- 🚀 **Quick Start**: Basic JIT decoration for pure functions\n",
    "- 🔄 **Stateful Modules**: How JIT handles mutable state\n",
    "- ⚙️ **Static Arguments**: Specializing compilation for specific values\n",
    "- 🎯 **Performance**: Measuring speedups from compilation\n",
    "- 🔧 **Advanced Control**: Manual compilation and cache management\n",
    "- 💡 **Best Practices**: When and how to use JIT effectively\n",
    "\n",
    "## Why JIT Compilation?\n",
    "\n",
    "JIT compilation provides several key benefits:\n",
    "\n",
    "1. **Performance**: Functions run 10-100x faster after compilation\n",
    "2. **Fusion**: Multiple operations fuse into efficient kernels\n",
    "3. **Memory**: Reduced intermediate allocations\n",
    "4. **Specialization**: Optimized code for specific shapes and types"
   ]
  },
  {
   "cell_type": "code",
   "id": "imports",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T16:26:41.112004Z",
     "start_time": "2025-10-10T16:26:40.845819Z"
    }
   },
   "source": [
    "import time\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import brainstate\n",
    "from brainstate.transform import jit\n",
    "\n",
    "# Set random seed\n",
    "brainstate.random.seed(42)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "quick_start",
   "metadata": {},
   "source": [
    "## 1. Quick Start: Decorating Pure Functions\n",
    "\n",
    "The simplest use of JIT is decorating a pure function (no side effects). Just add `@jit` above your function definition:"
   ]
  },
  {
   "cell_type": "code",
   "id": "basic_jit",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T16:26:41.164939Z",
     "start_time": "2025-10-10T16:26:41.116010Z"
    }
   },
   "source": [
    "@jit\n",
    "def softplus(x: jax.Array) -> jax.Array:\n",
    "    \"\"\"Smooth approximation to ReLU: log(1 + exp(x))\"\"\"\n",
    "    return jnp.log1p(jnp.exp(-jnp.abs(x))) + jnp.maximum(x, 0)\n",
    "\n",
    "# First call compiles the function\n",
    "xs = jnp.linspace(-5.0, 5.0, 7)\n",
    "result = softplus(xs)\n",
    "\n",
    "print(\"Input:\", xs)\n",
    "print(\"Output:\", result)\n",
    "print(f\"\\n✅ Function compiled and executed successfully!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [-5.0000000e+00 -3.3333333e+00 -1.6666663e+00  1.1920929e-07\n",
      "  1.6666670e+00  3.3333337e+00  5.0000000e+00]\n",
      "Output: [0.00671535 0.03505242 0.17300805 0.69314724 1.839675   3.368386\n",
      " 5.0067153 ]\n",
      "\n",
      "✅ Function compiled and executed successfully!\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "reuse",
   "metadata": {},
   "source": [
    "### Compilation is Cached\n",
    "\n",
    "Subsequent calls with the same input shapes reuse the compiled executable:"
   ]
  },
  {
   "cell_type": "code",
   "id": "cache_demo",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T16:26:41.268095Z",
     "start_time": "2025-10-10T16:26:41.169944Z"
    }
   },
   "source": [
    "# Second call reuses compiled code\n",
    "second_call = softplus(xs * 2.0)\n",
    "print(\"Second call output:\", second_call)\n",
    "\n",
    "# Different shape triggers recompilation\n",
    "larger_input = jnp.linspace(-5.0, 5.0, 100)\n",
    "third_call = softplus(larger_input)\n",
    "print(f\"\\nThird call with shape {larger_input.shape}: compiled new version\")\n",
    "print(\"Output shape:\", third_call.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second call output: [4.5398901e-05 1.2718249e-03 3.5052441e-02 6.9314730e-01 3.3683863e+00\n",
      " 6.6679392e+00 1.0000046e+01]\n",
      "\n",
      "Third call with shape (100,): compiled new version\n",
      "Output shape: (100,)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "performance",
   "metadata": {},
   "source": [
    "## 2. Performance Comparison: JIT vs Non-JIT\n",
    "\n",
    "Let's measure the speedup from JIT compilation with a realistic example:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T16:27:03.660014Z",
     "start_time": "2025-10-10T16:27:03.440859Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a more complex function\n",
    "def complex_computation(x):\n",
    "    \"\"\"Multi-step computation: good candidate for JIT.\"\"\"\n",
    "    for _ in range(10):\n",
    "        x = jnp.sin(x) + jnp.cos(x)\n",
    "        x = jnp.tanh(x * 0.5)\n",
    "        x = x @ x.T\n",
    "    return x\n",
    "\n",
    "# JIT-compiled version\n",
    "complex_computation_jit = jit(complex_computation)\n",
    "\n",
    "# Test data\n",
    "test_data = brainstate.random.randn(100, 100)\n",
    "\n",
    "# Warm up (compile)\n",
    "_ = complex_computation_jit(test_data)\n",
    "\n",
    "# Benchmark non-JIT\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    result_plain = complex_computation(test_data).block_until_ready()\n",
    "time_plain = (time.time() - start) / 10\n",
    "\n",
    "# Benchmark JIT\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    result_jit = complex_computation_jit(test_data).block_until_ready()\n",
    "time_jit = (time.time() - start) / 10\n",
    "\n",
    "# Results\n",
    "speedup = time_plain / time_jit\n",
    "print(\"Performance Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Without JIT: {time_plain*1000:.2f} ms\")\n",
    "print(f\"With JIT:    {time_jit*1000:.2f} ms\")\n",
    "print(f\"\\n🚀 Speedup: {speedup:.1f}x faster with JIT!\")\n",
    "\n",
    "# Verify correctness\n",
    "print(f\"\\n✅ Results match: {jnp.allclose(result_plain, result_jit)}\")"
   ],
   "id": "c54209ad6390603c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Comparison:\n",
      "==================================================\n",
      "Without JIT: 2.60 ms\n",
      "With JIT:    2.00 ms\n",
      "\n",
      "🚀 Speedup: 1.3x faster with JIT!\n",
      "\n",
      "✅ Results match: True\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "stateful_modules",
   "metadata": {},
   "source": [
    "## 3. Stateful Modules Under JIT\n",
    "\n",
    "BrainState's JIT automatically handles modules with mutable state. The key is that `State` objects are tracked and updated correctly even inside compiled functions.\n",
    "\n",
    "### Example: Running Statistics Tracker"
   ]
  },
  {
   "cell_type": "code",
   "id": "running_mean",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T16:27:12.808100Z",
     "start_time": "2025-10-10T16:27:12.727410Z"
    }
   },
   "source": [
    "class RunningMean(brainstate.nn.Module):\n",
    "    \"\"\"Tracks running mean of data batches.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sum = brainstate.HiddenState(jnp.array(0.0))\n",
    "        self.count = brainstate.HiddenState(jnp.array(0))\n",
    "\n",
    "    def __call__(self, batch: jax.Array) -> jax.Array:\n",
    "        \"\"\"Update running mean with new batch.\"\"\"\n",
    "        self.sum.value += jnp.sum(batch)\n",
    "        self.count.value += batch.size\n",
    "        return self.sum.value / self.count.value\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset statistics.\"\"\"\n",
    "        self.sum.value = jnp.array(0.0)\n",
    "        self.count.value = jnp.array(0)\n",
    "\n",
    "\n",
    "# Create tracker\n",
    "tracker = RunningMean()\n",
    "\n",
    "# JIT-compile the update function\n",
    "@jit\n",
    "def update_running_mean(batch: jax.Array) -> jax.Array:\n",
    "    return tracker(batch)\n",
    "\n",
    "# Simulate data stream\n",
    "print(\"Streaming data batches:\")\n",
    "print(\"=\" * 50)\n",
    "for step in range(5):\n",
    "    batch = jnp.arange(4.0) + step * 2\n",
    "    mean = update_running_mean(batch)\n",
    "    print(f\"Step {step}: batch = {batch}, running mean = {float(mean):.2f}\")\n",
    "\n",
    "print(f\"\\nFinal statistics:\")\n",
    "print(f\"  Total sum: {float(tracker.sum.value):.1f}\")\n",
    "print(f\"  Sample count: {int(tracker.count.value)}\")\n",
    "print(f\"  Overall mean: {float(tracker.sum.value / tracker.count.value):.2f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming data batches:\n",
      "==================================================\n",
      "Step 0: batch = [0. 1. 2. 3.], running mean = 1.50\n",
      "Step 1: batch = [2. 3. 4. 5.], running mean = 2.50\n",
      "Step 2: batch = [4. 5. 6. 7.], running mean = 3.50\n",
      "Step 3: batch = [6. 7. 8. 9.], running mean = 4.50\n",
      "Step 4: batch = [ 8.  9. 10. 11.], running mean = 5.50\n",
      "\n",
      "Final statistics:\n",
      "  Total sum: 110.0\n",
      "  Sample count: 20\n",
      "  Overall mean: 5.50\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "batch_norm",
   "metadata": {},
   "source": [
    "### Example: Batch Normalization Layer\n",
    "\n",
    "A more realistic example with running statistics:"
   ]
  },
  {
   "cell_type": "code",
   "id": "batchnorm",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T16:27:14.635520Z",
     "start_time": "2025-10-10T16:27:14.286635Z"
    }
   },
   "source": [
    "class BatchNorm(brainstate.nn.Module):\n",
    "    \"\"\"Batch normalization with running statistics.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_features: int, momentum: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        # Learnable parameters\n",
    "        self.gamma = brainstate.ParamState(jnp.ones(num_features))\n",
    "        self.beta = brainstate.ParamState(jnp.zeros(num_features))\n",
    "        \n",
    "        # Running statistics\n",
    "        self.running_mean = brainstate.HiddenState(jnp.zeros(num_features))\n",
    "        self.running_var = brainstate.HiddenState(jnp.ones(num_features))\n",
    "        \n",
    "        # Training flag\n",
    "        self.training = True\n",
    "    \n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        if self.training:\n",
    "            # Compute batch statistics\n",
    "            mean = jnp.mean(x, axis=0)\n",
    "            var = jnp.var(x, axis=0)\n",
    "            \n",
    "            # Update running statistics\n",
    "            self.running_mean.value = (\n",
    "                (1 - self.momentum) * self.running_mean.value + \n",
    "                self.momentum * mean\n",
    "            )\n",
    "            self.running_var.value = (\n",
    "                (1 - self.momentum) * self.running_var.value + \n",
    "                self.momentum * var\n",
    "            )\n",
    "        else:\n",
    "            # Use running statistics\n",
    "            mean = self.running_mean.value\n",
    "            var = self.running_var.value\n",
    "        \n",
    "        # Normalize\n",
    "        x_norm = (x - mean) / jnp.sqrt(var + 1e-5)\n",
    "        \n",
    "        # Scale and shift\n",
    "        return self.gamma.value * x_norm + self.beta.value\n",
    "\n",
    "\n",
    "# Create batch norm layer\n",
    "bn = BatchNorm(num_features=3)\n",
    "\n",
    "@jit\n",
    "def forward_pass(x):\n",
    "    return bn(x)\n",
    "\n",
    "# Training phase\n",
    "print(\"Training phase (updating running stats):\")\n",
    "print(\"=\" * 50)\n",
    "for i in range(3):\n",
    "    x = brainstate.random.randn(32, 3) + i  # Batches with different means\n",
    "    output = forward_pass(x)\n",
    "    print(f\"Batch {i}: input mean = {jnp.mean(x, axis=0)}, \"\n",
    "          f\"output mean = {jnp.mean(output, axis=0)}\")\n",
    "\n",
    "print(f\"\\nRunning mean: {bn.running_mean.value}\")\n",
    "print(f\"Running var:  {bn.running_var.value}\")\n",
    "\n",
    "# Inference phase\n",
    "bn.training = False\n",
    "print(\"\\n✅ Switch to inference mode (using running stats)\")\n",
    "test_x = brainstate.random.randn(5, 3)\n",
    "test_out = forward_pass(test_x)\n",
    "print(f\"Test output shape: {test_out.shape}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training phase (updating running stats):\n",
      "==================================================\n",
      "Batch 0: input mean = [-0.17944504  0.16966473  0.09837674], output mean = [ 2.2351742e-08 -1.8626451e-09  2.7939677e-09]\n",
      "Batch 1: input mean = [0.98484504 1.1770446  0.93761075], output mean = [ 1.0244548e-08  2.2351742e-08 -7.4505806e-09]\n",
      "Batch 2: input mean = [2.2120051 2.0268552 2.1104944], output mean = [ 3.4831464e-07 -4.0978193e-08 -3.4645200e-07]\n",
      "\n",
      "Running mean: [0.29530153 0.3223624  0.30340293]\n",
      "Running var:  [1.0351619  0.97919095 0.9962434 ]\n",
      "\n",
      "✅ Switch to inference mode (using running stats)\n",
      "Test output shape: (5, 3)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "static_args",
   "metadata": {},
   "source": [
    "## 4. Static Arguments\n",
    "\n",
    "Use `static_argnums` or `static_argnames` to treat certain arguments as **compile-time constants**. This is crucial when:\n",
    "\n",
    "- Arguments control loop iterations or conditional branches\n",
    "- You want specialized code for different configurations\n",
    "- The argument is not a JAX array (e.g., `int`, `bool`, `str`)\n",
    "\n",
    "### Example: Polynomial Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "id": "static_args_poly",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T16:27:15.528932Z",
     "start_time": "2025-10-10T16:27:15.480555Z"
    }
   },
   "source": [
    "@jit(static_argnums=1)\n",
    "def polynomial_series(x: jax.Array, degree: int) -> jax.Array:\n",
    "    \"\"\"Compute polynomial series up to given degree.\n",
    "    \n",
    "    P(x) = 1*x + 2*x^2 + 3*x^3 + ... + degree*x^degree\n",
    "    \"\"\"\n",
    "    powers = [x ** (i + 1) for i in range(degree)]\n",
    "    coeffs = jnp.arange(1, degree + 1, dtype=x.dtype)\n",
    "    stacked = jnp.stack(powers, axis=0)\n",
    "    return jnp.tensordot(coeffs, stacked, axes=1)\n",
    "\n",
    "# Different degrees trigger different compilations\n",
    "x = jnp.array([1.0, 2.0])\n",
    "\n",
    "y3 = polynomial_series(x, 3)  # Compiles for degree=3\n",
    "y3_again = polynomial_series(x, 3)  # Reuses compilation\n",
    "y5 = polynomial_series(x, 5)  # New compilation for degree=5\n",
    "\n",
    "print(\"Polynomial evaluations:\")\n",
    "print(f\"  degree=3: {y3}\")\n",
    "print(f\"  degree=3 (cached): {y3_again}\")\n",
    "print(f\"  degree=5: {y5}\")\n",
    "print(f\"\\n✅ Each degree gets its own specialized compilation\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial evaluations:\n",
      "  degree=3: [ 6. 34.]\n",
      "  degree=3 (cached): [ 6. 34.]\n",
      "  degree=5: [ 15. 258.]\n",
      "\n",
      "✅ Each degree gets its own specialized compilation\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "static_shapes",
   "metadata": {},
   "source": [
    "### Example: Matrix Operations with Static Configuration"
   ]
  },
  {
   "cell_type": "code",
   "id": "static_config",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T16:27:16.658843Z",
     "start_time": "2025-10-10T16:27:16.438017Z"
    }
   },
   "source": [
    "@jit(static_argnames=['transpose', 'normalize'])\n",
    "def matrix_transform(x: jax.Array, transpose: bool = False, normalize: bool = False) -> jax.Array:\n",
    "    \"\"\"Apply matrix transformations based on flags.\"\"\"\n",
    "    if transpose:\n",
    "        x = x.T\n",
    "    \n",
    "    result = x @ x.T\n",
    "    \n",
    "    if normalize:\n",
    "        result = result / jnp.linalg.norm(result)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test different configurations\n",
    "mat = brainstate.random.randn(3, 4)\n",
    "\n",
    "print(\"Matrix transform configurations:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Original shape: {mat.shape}\")\n",
    "print(f\"No flags: {matrix_transform(mat, transpose=False, normalize=False).shape}\")\n",
    "print(f\"Transpose: {matrix_transform(mat, transpose=True, normalize=False).shape}\")\n",
    "print(f\"Normalize: {matrix_transform(mat, transpose=False, normalize=True).shape}\")\n",
    "print(f\"Both: {matrix_transform(mat, transpose=True, normalize=True).shape}\")\n",
    "print(f\"\\n✅ Four different compilations for four configurations\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix transform configurations:\n",
      "==================================================\n",
      "Original shape: (3, 4)\n",
      "No flags: (3, 3)\n",
      "Transpose: (4, 4)\n",
      "Normalize: (3, 3)\n",
      "Both: (4, 4)\n",
      "\n",
      "✅ Four different compilations for four configurations\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "neural_net",
   "metadata": {},
   "source": [
    "## 5. Real-World Example: Neural Network Training\n",
    "\n",
    "Let's combine everything we've learned in a complete training loop:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T16:28:00.272483Z",
     "start_time": "2025-10-10T16:28:00.235583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleNN(brainstate.nn.Module):\n",
    "    \"\"\"Simple feedforward neural network.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = brainstate.nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = brainstate.nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        # Track training steps\n",
    "        self.step_count = brainstate.HiddenState(jnp.array(0))\n",
    "    \n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        x = self.fc1(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Create model\n",
    "model = SimpleNN(input_dim=10, hidden_dim=32, output_dim=3)\n",
    "\n",
    "# Create synthetic dataset\n",
    "X_train = brainstate.random.randn(100, 10)\n",
    "y_train = jnp.argmax(brainstate.random.randn(100, 3), axis=1)\n",
    "\n",
    "print(f\"Model created:\")\n",
    "print(f\"  Parameters: {sum([jnp.size(p.value) for p in brainstate.graph.states(model, brainstate.ParamState).values()]):,}\")\n",
    "print(f\"  Training data: {X_train.shape}\")\n",
    "print(f\"  Labels: {y_train.shape}\")"
   ],
   "id": "2be5b14307abdb6e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created:\n",
      "  Parameters: 2\n",
      "  Training data: (100, 10)\n",
      "  Labels: (100,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adadu\\AppData\\Local\\Temp\\ipykernel_13924\\3288916570.py:27: DeprecationWarning: size requires ndarray or scalar arguments, got <class 'dict'> at position 0. In a future JAX release this will be an error.\n",
      "  print(f\"  Parameters: {sum([jnp.size(p.value) for p in brainstate.graph.states(model, brainstate.ParamState).values()]):,}\")\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T16:28:10.495200Z",
     "start_time": "2025-10-10T16:28:10.219795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define loss function\n",
    "def loss_fn(model, x, y):\n",
    "    logits = model(x)\n",
    "    # Simple cross-entropy\n",
    "    log_probs = jax.nn.log_softmax(logits, axis=-1)\n",
    "    loss = -jnp.mean(log_probs[jnp.arange(len(y)), y])\n",
    "    return loss\n",
    "\n",
    "# JIT-compile the training step\n",
    "@jit\n",
    "def train_step(x_batch, y_batch):\n",
    "    \"\"\"Single training step with gradient descent.\"\"\"\n",
    "    # Increment step counter\n",
    "    model.step_count.value += 1\n",
    "    \n",
    "    # Split graph for JAX transformations\n",
    "    graphdef, params, others = brainstate.graph.treefy_split(\n",
    "        model, brainstate.ParamState, ...\n",
    "    )\n",
    "    \n",
    "    # Compute loss and gradients\n",
    "    def compute_loss(params):\n",
    "        temp_model = brainstate.graph.treefy_merge(graphdef, params, others)\n",
    "        return loss_fn(temp_model, x_batch, y_batch)\n",
    "    \n",
    "    loss, grads = jax.value_and_grad(compute_loss)(params)\n",
    "    \n",
    "    # Update parameters (simple SGD)\n",
    "    learning_rate = 0.01\n",
    "    new_params = jax.tree.map(\n",
    "        lambda p, g: p - learning_rate * g,\n",
    "        params, grads\n",
    "    )\n",
    "    \n",
    "    # Update model\n",
    "    brainstate.graph.update_states(model, new_params)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Training loop\n",
    "print(\"\\nTraining:\")\n",
    "print(\"=\" * 50)\n",
    "batch_size = 32\n",
    "n_epochs = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_losses = []\n",
    "    \n",
    "    # Mini-batch training\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        x_batch = X_train[i:i+batch_size]\n",
    "        y_batch = y_train[i:i+batch_size]\n",
    "        \n",
    "        loss = train_step(x_batch, y_batch)\n",
    "        epoch_losses.append(float(loss))\n",
    "    \n",
    "    avg_loss = jnp.mean(jnp.array(epoch_losses))\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}: loss = {avg_loss:.4f}\")\n",
    "\n",
    "print(f\"\\n✅ Training complete! Total steps: {int(model.step_count.value)}\")"
   ],
   "id": "c34ea56c1b3075b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:\n",
      "==================================================\n",
      "Epoch 1/5: loss = 1.5220\n",
      "Epoch 2/5: loss = 1.5220\n",
      "Epoch 3/5: loss = 1.5220\n",
      "Epoch 4/5: loss = 1.5220\n",
      "Epoch 5/5: loss = 1.5220\n",
      "\n",
      "✅ Training complete! Total steps: 20\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "advanced_control",
   "metadata": {},
   "source": [
    "## 6. Advanced Control: Manual Compilation\n",
    "\n",
    "`JittedFunction` provides additional methods for fine-grained control:\n",
    "\n",
    "- **`compile(*args, **kwargs)`**: Pre-compile for specific inputs\n",
    "- **`clear_cache()`**: Clear all cached compilations\n",
    "- **`origin_fun`**: Access the original uncompiled function"
   ]
  },
  {
   "cell_type": "code",
   "id": "manual_compile",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T16:28:11.801508Z",
     "start_time": "2025-10-10T16:28:11.626718Z"
    }
   },
   "source": [
    "@jit(static_argnums=1)\n",
    "def power_function(x: jax.Array, n: int) -> jax.Array:\n",
    "    \"\"\"Compute x^n using repeated multiplication.\"\"\"\n",
    "    result = jnp.ones_like(x)\n",
    "    for _ in range(n):\n",
    "        result = result * x\n",
    "    return result\n",
    "\n",
    "# Pre-compile for specific cases\n",
    "print(\"Manual compilation:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_x = jnp.array([2.0, 3.0, 4.0])\n",
    "\n",
    "# Compile ahead of time\n",
    "print(\"Compiling for n=2, n=3, n=4...\")\n",
    "for n in [2, 3, 4]:\n",
    "    _ = power_function(test_x, n)  # Trigger compilation\n",
    "\n",
    "print(\"\\nCache populated. Now computing:\")\n",
    "print(f\"  2^2 = {power_function(jnp.array([2.0]), 2)}\")\n",
    "print(f\"  3^3 = {power_function(jnp.array([3.0]), 3)}\")\n",
    "print(f\"  4^4 = {power_function(jnp.array([4.0]), 4)}\")\n",
    "\n",
    "# Clear cache\n",
    "print(\"\\nClearing cache...\")\n",
    "power_function.clear_cache()\n",
    "print(\"✅ Cache cleared. Next call will recompile.\")\n",
    "\n",
    "# Access original function\n",
    "print(\"\\nOriginal (non-JIT) function:\")\n",
    "original_result = power_function.origin_fun(jnp.array([5.0]), 2)\n",
    "print(f\"  5^2 = {original_result} (computed without JIT)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual compilation:\n",
      "==================================================\n",
      "Compiling for n=2, n=3, n=4...\n",
      "\n",
      "Cache populated. Now computing:\n",
      "  2^2 = [4.]\n",
      "  3^3 = [27.]\n",
      "  4^4 = [256.]\n",
      "\n",
      "Clearing cache...\n",
      "✅ Cache cleared. Next call will recompile.\n",
      "\n",
      "Original (non-JIT) function:\n",
      "  5^2 = [25.] (computed without JIT)\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "best_practices",
   "metadata": {},
   "source": [
    "## 7. Best Practices and Common Pitfalls\n",
    "\n",
    "### ✅ When to Use JIT\n",
    "\n",
    "1. **Computational Functions**: Pure numerical computations benefit most\n",
    "2. **Training Steps**: Compile forward pass + gradient computation\n",
    "3. **Inference**: Batch prediction functions\n",
    "4. **Repeated Calls**: Functions called many times with similar shapes\n",
    "\n",
    "### ❌ When NOT to Use JIT\n",
    "\n",
    "1. **I/O Operations**: File reading, printing (use side effects carefully)\n",
    "2. **One-Time Operations**: Setup code that runs once\n",
    "3. **Debugging**: Disable JIT when debugging (errors are clearer)\n",
    "4. **Dynamic Shapes**: Functions with highly variable input shapes\n",
    "\n",
    "### Common Pitfalls"
   ]
  },
  {
   "cell_type": "code",
   "id": "pitfalls",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T16:28:13.872467Z",
     "start_time": "2025-10-10T16:28:13.834573Z"
    }
   },
   "source": [
    "# ❌ BAD: Using Python control flow with dynamic values\n",
    "@jit\n",
    "def bad_example(x):\n",
    "    # This tries to use x's VALUE for control flow\n",
    "    # Will fail or give unexpected results\n",
    "    # if x > 0:  # ❌ Don't do this!\n",
    "    #     return x * 2\n",
    "    # else:\n",
    "    #     return x / 2\n",
    "    pass\n",
    "\n",
    "# ✅ GOOD: Use jax.lax.cond for dynamic control flow\n",
    "@jit\n",
    "def good_example(x):\n",
    "    return jax.lax.cond(\n",
    "        jnp.sum(x) > 0,\n",
    "        lambda x: x * 2,\n",
    "        lambda x: x / 2,\n",
    "        x\n",
    "    )\n",
    "\n",
    "test = jnp.array([1.0, 2.0, 3.0])\n",
    "print(\"Correct conditional execution:\")\n",
    "print(f\"  Input: {test}\")\n",
    "print(f\"  Output: {good_example(test)}\")\n",
    "print(f\"\\n✅ Use jax.lax.cond/switch for conditional logic in JIT\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct conditional execution:\n",
      "  Input: [1. 2. 3.]\n",
      "  Output: [2. 4. 6.]\n",
      "\n",
      "✅ Use jax.lax.cond/switch for conditional logic in JIT\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "debugging",
   "metadata": {},
   "source": [
    "### Debugging JIT-Compiled Code\n",
    "\n",
    "When debugging, temporarily disable JIT:"
   ]
  },
  {
   "cell_type": "code",
   "id": "debug_demo",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T16:28:15.007985Z",
     "start_time": "2025-10-10T16:28:14.972044Z"
    }
   },
   "source": [
    "@jit\n",
    "def complex_function(x):\n",
    "    y = jnp.sin(x)\n",
    "    z = jnp.exp(y)\n",
    "    # In non-JIT mode, you can inspect intermediate values\n",
    "    return z\n",
    "\n",
    "# Option 1: Use origin_fun for debugging\n",
    "print(\"Debugging with origin_fun:\")\n",
    "result = complex_function.origin_fun(jnp.array([1.0, 2.0]))\n",
    "print(f\"  Result: {result}\")\n",
    "\n",
    "# Option 2: Temporarily disable JIT globally\n",
    "# jax.config.update('jax_disable_jit', True)\n",
    "# ... debug your code ...\n",
    "# jax.config.update('jax_disable_jit', False)\n",
    "\n",
    "print(\"\\n💡 Tip: Use .origin_fun or disable JIT globally when debugging\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debugging with origin_fun:\n",
      "  Result: [2.3197768 2.4825778]\n",
      "\n",
      "💡 Tip: Use .origin_fun or disable JIT globally when debugging\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This tutorial covered JIT compilation in BrainState:\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "✅ **Basic JIT**: Decorate functions with `@jit` for automatic compilation\n",
    "\n",
    "✅ **Performance**: 10-100x speedups for numerical computations\n",
    "\n",
    "✅ **Stateful Modules**: BrainState handles mutable state seamlessly\n",
    "\n",
    "✅ **Static Arguments**: Use `static_argnums`/`static_argnames` for compile-time specialization\n",
    "\n",
    "✅ **Advanced Control**: Manual compilation, cache management, debugging tools\n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "```python\n",
    "# Basic usage\n",
    "@jit\n",
    "def my_function(x):\n",
    "    return x * 2\n",
    "\n",
    "# Static arguments\n",
    "@jit(static_argnums=1)\n",
    "def my_function(x, n):\n",
    "    return x ** n\n",
    "\n",
    "# Named static arguments\n",
    "@jit(static_argnames=['mode', 'training'])\n",
    "def my_function(x, mode='train', training=True):\n",
    "    ...\n",
    "\n",
    "# Manual control\n",
    "my_function.compile(args)  # Pre-compile\n",
    "my_function.clear_cache()  # Clear cache\n",
    "my_function.origin_fun(args)  # Call without JIT\n",
    "```\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. ⚡ **JIT training steps** for maximum performance\n",
    "2. 🎯 **Use static arguments** for configuration flags\n",
    "3. 🔍 **Disable JIT when debugging** for clearer error messages\n",
    "4. 📊 **Profile before optimizing** to identify bottlenecks\n",
    "5. 🧪 **Test with and without JIT** to ensure correctness\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Vectorization (vmap)**: Automatically batch operations\n",
    "- **Automatic Differentiation (grad)**: Compute gradients\n",
    "- **Parallelization**: Distribute computation across devices\n",
    "- **Checkpointing**: Trade compute for memory in deep networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 12: Control Flow - Loops and Conditions\n",
    "\n",
    "In this tutorial, we'll explore control flow primitives in BrainState for writing efficient loops and conditional operations that work with JIT compilation.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "- Use `scan` for efficient sequential computations\n",
    "- Implement loops with `for_loop` and `while_loop`\n",
    "- Write conditional logic with `cond` and `switch`\n",
    "- Understand when to use each control flow primitive\n",
    "- Optimize recurrent neural networks with scan\n",
    "- Combine control flow with other transformations\n",
    "\n",
    "## Why Special Control Flow?\n",
    "\n",
    "Regular Python control flow (if/for/while) doesn't work well with:\n",
    "- JIT compilation (can't trace dynamic behavior)\n",
    "- Automatic differentiation (gradients through loops)\n",
    "- Hardware acceleration (GPU/TPU execution)\n",
    "\n",
    "BrainState provides functional control flow primitives that solve these problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brainstate as bst\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Set random seed\n",
    "bst.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Problem with Regular Python Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Fibonacci sequence\n",
    "def fibonacci_python(n):\n",
    "    \"\"\"Regular Python loop - doesn't JIT well.\"\"\"\n",
    "    a, b = 0, 1\n",
    "    result = []\n",
    "    for i in range(n):\n",
    "        result.append(a)\n",
    "        a, b = b, a + b\n",
    "    return jnp.array(result)\n",
    "\n",
    "# This works but can't be efficiently JIT compiled\n",
    "fib_10 = fibonacci_python(10)\n",
    "print(\"Fibonacci (Python loop):\", fib_10)\n",
    "\n",
    "# Try to JIT it - this will unroll the loop at compile time!\n",
    "@jax.jit\n",
    "def fibonacci_jit_bad(n):\n",
    "    a, b = 0, 1\n",
    "    result = []\n",
    "    for i in range(n):  # n must be static!\n",
    "        result.append(a)\n",
    "        a, b = b, a + b\n",
    "    return jnp.array(result)\n",
    "\n",
    "# This creates a separate compiled version for each n\n",
    "print(\"\\nProblem: Each different n triggers recompilation\")\n",
    "print(\"We need better control flow primitives!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scan: The Swiss Army Knife of Loops\n",
    "\n",
    "`scan` is the most versatile loop primitive. It iterates over a sequence, carrying state forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fibonacci with scan\n",
    "def fibonacci_step(carry, x):\n",
    "    \"\"\"Single step: carry = (a, b), x is unused.\"\"\"\n",
    "    a, b = carry\n",
    "    return (b, a + b), a  # new_carry, output\n",
    "\n",
    "# Use scan\n",
    "def fibonacci_scan(n):\n",
    "    carry_init = (0, 1)\n",
    "    xs = jnp.arange(n)  # Input sequence (we don't use values, just length)\n",
    "    final_carry, outputs = jax.lax.scan(fibonacci_step, carry_init, xs)\n",
    "    return outputs\n",
    "\n",
    "fib_scan = fibonacci_scan(10)\n",
    "print(\"Fibonacci (scan):\", fib_scan)\n",
    "print(\"Matches Python version:\", jnp.allclose(fib_10, fib_scan))\n",
    "\n",
    "# Scan is JIT-friendly and efficient!\n",
    "fibonacci_scan_jit = jax.jit(fibonacci_scan)\n",
    "print(\"\\nJIT-compiled version works great:\")\n",
    "print(fibonacci_scan_jit(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Scan\n",
    "\n",
    "```python\n",
    "carry, outputs = scan(f, carry_init, xs)\n",
    "```\n",
    "\n",
    "- **f(carry, x)**: Function that processes one element\n",
    "  - Input: current carry, current element x\n",
    "  - Output: (new_carry, output_value)\n",
    "- **carry_init**: Initial state\n",
    "- **xs**: Sequence to iterate over\n",
    "- Returns: (final_carry, all_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Cumulative Sum with Scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative sum\n",
    "def cumsum_step(carry, x):\n",
    "    new_sum = carry + x\n",
    "    return new_sum, new_sum  # carry is the running sum\n",
    "\n",
    "data = jnp.array([1, 2, 3, 4, 5])\n",
    "final_sum, cumulative = jax.lax.scan(cumsum_step, 0, data)\n",
    "\n",
    "print(\"Input:\", data)\n",
    "print(\"Cumulative sum:\", cumulative)\n",
    "print(\"Final sum:\", final_sum)\n",
    "print(\"Compare to jnp.cumsum:\", jnp.cumsum(data))\n",
    "print(\"Match:\", jnp.allclose(cumulative, jnp.cumsum(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scan for RNN Sequences\n",
    "\n",
    "The most common use of scan: processing sequences through recurrent networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple RNN with scan\n",
    "class SimpleRNN(bst.graph.Node):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.Wxh = bst.ParamState(bst.random.randn(input_size, hidden_size) * 0.1)\n",
    "        self.Whh = bst.ParamState(bst.random.randn(hidden_size, hidden_size) * 0.1)\n",
    "        self.bh = bst.ParamState(jnp.zeros(hidden_size))\n",
    "    \n",
    "    def step(self, h, x):\n",
    "        \"\"\"Single RNN step.\"\"\"\n",
    "        h_new = jnp.tanh(\n",
    "            x @ self.Wxh.value + \n",
    "            h @ self.Whh.value + \n",
    "            self.bh.value\n",
    "        )\n",
    "        return h_new, h_new  # (new_hidden, output)\n",
    "    \n",
    "    def __call__(self, sequence):\n",
    "        \"\"\"Process entire sequence.\"\"\"\n",
    "        h_init = jnp.zeros(self.hidden_size)\n",
    "        final_h, all_h = jax.lax.scan(self.step, h_init, sequence)\n",
    "        return all_h\n",
    "\n",
    "# Create RNN and test\n",
    "rnn = SimpleRNN(input_size=5, hidden_size=10)\n",
    "sequence = bst.random.randn(20, 5)  # 20 time steps, 5 features\n",
    "\n",
    "outputs = rnn(sequence)\n",
    "print(f\"Sequence shape: {sequence.shape}\")\n",
    "print(f\"Outputs shape: {outputs.shape}\")\n",
    "\n",
    "# Visualize hidden states\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.imshow(outputs.T, aspect='auto', cmap='RdBu_r', interpolation='nearest')\n",
    "plt.colorbar(label='Activation')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Hidden Unit')\n",
    "plt.title('RNN Hidden States Over Time')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance: Scan vs Python Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare scan vs manual loop for RNN\n",
    "long_sequence = bst.random.randn(1000, 5)\n",
    "\n",
    "# Version 1: Manual loop\n",
    "def rnn_manual_loop(rnn, sequence):\n",
    "    h = jnp.zeros(rnn.hidden_size)\n",
    "    outputs = []\n",
    "    for x in sequence:\n",
    "        h, out = rnn.step(h, x)\n",
    "        outputs.append(out)\n",
    "    return jnp.stack(outputs)\n",
    "\n",
    "# Version 2: Scan\n",
    "def rnn_scan(rnn, sequence):\n",
    "    return rnn(sequence)\n",
    "\n",
    "# Time them\n",
    "start = time.time()\n",
    "_ = rnn_manual_loop(rnn, long_sequence)\n",
    "time_loop = time.time() - start\n",
    "\n",
    "rnn_scan_jit = jax.jit(rnn_scan, static_argnums=0)\n",
    "_ = rnn_scan_jit(rnn, long_sequence)  # Warmup\n",
    "start = time.time()\n",
    "_ = rnn_scan_jit(rnn, long_sequence)\n",
    "time_scan = time.time() - start\n",
    "\n",
    "print(f\"Manual loop: {time_loop*1000:.2f} ms\")\n",
    "print(f\"Scan (JIT):  {time_scan*1000:.2f} ms\")\n",
    "print(f\"Speedup:     {time_loop/time_scan:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. While Loop\n",
    "\n",
    "`while_loop` executes until a condition becomes false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Find first power of 2 >= target\n",
    "def find_power_of_2(target):\n",
    "    def cond_fn(carry):\n",
    "        power, value = carry\n",
    "        return value < target\n",
    "    \n",
    "    def body_fn(carry):\n",
    "        power, value = carry\n",
    "        return (power + 1, value * 2)\n",
    "    \n",
    "    init_val = (0, 1)\n",
    "    final_power, final_value = jax.lax.while_loop(cond_fn, body_fn, init_val)\n",
    "    return final_power, final_value\n",
    "\n",
    "target = 1000\n",
    "power, value = find_power_of_2(target)\n",
    "print(f\"Target: {target}\")\n",
    "print(f\"First power of 2 >= target: 2^{power} = {value}\")\n",
    "\n",
    "# Works with JIT\n",
    "find_power_of_2_jit = jax.jit(find_power_of_2)\n",
    "power2, value2 = find_power_of_2_jit(500)\n",
    "print(f\"\\nTarget: 500\")\n",
    "print(f\"Result: 2^{power2} = {value2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Convergence Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteratively refine until convergence\n",
    "def sqrt_newton(x, tolerance=1e-6, max_iters=100):\n",
    "    \"\"\"Newton's method for square root.\"\"\"\n",
    "    def cond_fn(carry):\n",
    "        guess, iteration = carry\n",
    "        error = jnp.abs(guess * guess - x)\n",
    "        return (error > tolerance) & (iteration < max_iters)\n",
    "    \n",
    "    def body_fn(carry):\n",
    "        guess, iteration = carry\n",
    "        # Newton update: x_{n+1} = (x_n + a/x_n) / 2\n",
    "        new_guess = (guess + x / guess) / 2\n",
    "        return (new_guess, iteration + 1)\n",
    "    \n",
    "    init_guess = x / 2  # Initial guess\n",
    "    final_guess, num_iters = jax.lax.while_loop(cond_fn, body_fn, (init_guess, 0))\n",
    "    return final_guess, num_iters\n",
    "\n",
    "# Test\n",
    "x = 2.0\n",
    "result, iters = sqrt_newton(x)\n",
    "print(f\"sqrt({x}) ≈ {result} (converged in {iters} iterations)\")\n",
    "print(f\"True value: {jnp.sqrt(x)}\")\n",
    "print(f\"Error: {jnp.abs(result - jnp.sqrt(x))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. For Loop\n",
    "\n",
    "`for_loop` is similar to scan but with a simpler interface when you don't need outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loop example: Matrix power\n",
    "def matrix_power(A, n):\n",
    "    \"\"\"Compute A^n using for_loop.\"\"\"\n",
    "    def body_fn(i, carry):\n",
    "        return carry @ A\n",
    "    \n",
    "    result = jax.lax.fori_loop(0, n, body_fn, A)\n",
    "    return result\n",
    "\n",
    "A = jnp.array([[1, 1], [1, 0]], dtype=float)  # Fibonacci matrix\n",
    "A_power_5 = matrix_power(A, 5)\n",
    "\n",
    "print(\"Matrix A (Fibonacci matrix):\")\n",
    "print(A)\n",
    "print(f\"\\nA^5:\")\n",
    "print(A_power_5)\n",
    "print(f\"\\nNote: A^n generates Fibonacci numbers!\")\n",
    "print(f\"A_power_5[0,1] = {int(A_power_5[0,1])} (8th Fibonacci number)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conditional Execution: cond\n",
    "\n",
    "`cond` provides efficient if-else branching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple conditional\n",
    "def absolute_value(x):\n",
    "    return jax.lax.cond(\n",
    "        x >= 0,\n",
    "        lambda x: x,      # true branch\n",
    "        lambda x: -x,     # false branch\n",
    "        x\n",
    "    )\n",
    "\n",
    "print(f\"abs(5.0) = {absolute_value(5.0)}\")\n",
    "print(f\"abs(-3.0) = {absolute_value(-3.0)}\")\n",
    "\n",
    "# Works with JIT\n",
    "absolute_value_jit = jax.jit(absolute_value)\n",
    "print(f\"\\nJIT version works: {absolute_value_jit(-7.0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Piecewise Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Piecewise activation function\n",
    "def piecewise_activation(x):\n",
    "    \"\"\"Custom activation: linear below 0, quadratic 0-1, constant above 1.\"\"\"\n",
    "    return jax.lax.cond(\n",
    "        x < 0,\n",
    "        lambda x: x,                                    # Linear: x\n",
    "        lambda x: jax.lax.cond(\n",
    "            x < 1,\n",
    "            lambda x: x ** 2,                          # Quadratic: x²\n",
    "            lambda x: jnp.ones_like(x),                # Constant: 1\n",
    "            x\n",
    "        ),\n",
    "        x\n",
    "    )\n",
    "\n",
    "# Vectorize for plotting\n",
    "piecewise_vec = jax.vmap(piecewise_activation)\n",
    "\n",
    "x_vals = jnp.linspace(-2, 2, 200)\n",
    "y_vals = piecewise_vec(x_vals)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x_vals, y_vals, linewidth=2)\n",
    "plt.axhline(0, color='k', linewidth=0.5)\n",
    "plt.axvline(0, color='k', linewidth=0.5)\n",
    "plt.axvline(1, color='r', linestyle='--', alpha=0.3, label='Transition points')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Piecewise Activation Function')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Switch: Multi-Way Branching\n",
    "\n",
    "`switch` is like a switch/case statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Different operations based on index\n",
    "def apply_operation(index, x):\n",
    "    \"\"\"Apply different operations based on index.\"\"\"\n",
    "    branches = [\n",
    "        lambda x: x,              # 0: identity\n",
    "        lambda x: x ** 2,         # 1: square\n",
    "        lambda x: jnp.sqrt(jnp.abs(x)),  # 2: sqrt of abs\n",
    "        lambda x: jnp.sin(x),     # 3: sine\n",
    "    ]\n",
    "    return jax.lax.switch(index, branches, x)\n",
    "\n",
    "x = 2.0\n",
    "for i in range(4):\n",
    "    result = apply_operation(i, x)\n",
    "    print(f\"Operation {i} on {x}: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Activation Function Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic activation function selection\n",
    "def dynamic_activation(x, activation_id):\n",
    "    \"\"\"Select activation function at runtime.\"\"\"\n",
    "    activations = [\n",
    "        lambda x: x,                    # 0: linear\n",
    "        lambda x: jnp.maximum(0, x),    # 1: ReLU\n",
    "        lambda x: jnp.tanh(x),          # 2: tanh\n",
    "        lambda x: 1 / (1 + jnp.exp(-x)),  # 3: sigmoid\n",
    "    ]\n",
    "    return jax.lax.switch(activation_id, activations, x)\n",
    "\n",
    "# Visualize all activations\n",
    "x_vals = jnp.linspace(-3, 3, 100)\n",
    "activation_names = ['Linear', 'ReLU', 'Tanh', 'Sigmoid']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (ax, name) in enumerate(zip(axes, activation_names)):\n",
    "    y_vals = jax.vmap(lambda x: dynamic_activation(x, i))(x_vals)\n",
    "    ax.plot(x_vals, y_vals, linewidth=2)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('f(x)')\n",
    "    ax.set_title(f'{name} (id={i})')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(0, color='k', linewidth=0.5)\n",
    "    ax.axvline(0, color='k', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Practical Example: Sequence Classification with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN with early stopping based on confidence\n",
    "class EarlyStopRNN(bst.graph.Node):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.rnn = SimpleRNN(input_size, hidden_size)\n",
    "        self.classifier = bst.nn.Linear(hidden_size, num_classes)\n",
    "        self.confidence_threshold = 0.95\n",
    "    \n",
    "    def __call__(self, sequence, use_early_stop=False):\n",
    "        if not use_early_stop:\n",
    "            # Normal processing\n",
    "            h = self.rnn(sequence)  # All hidden states\n",
    "            logits = self.classifier(h[-1])  # Use final state\n",
    "            return logits, len(sequence)\n",
    "        else:\n",
    "            # Early stopping version\n",
    "            def process_step(carry, x):\n",
    "                h, stopped = carry\n",
    "                \n",
    "                # Update hidden state\n",
    "                h_new, _ = self.rnn.step(h, x)\n",
    "                \n",
    "                # Check confidence\n",
    "                logits = self.classifier(h_new)\n",
    "                probs = jax.nn.softmax(logits)\n",
    "                max_prob = jnp.max(probs)\n",
    "                \n",
    "                # Stop if confident\n",
    "                should_stop = max_prob > self.confidence_threshold\n",
    "                stopped_new = stopped | should_stop\n",
    "                \n",
    "                return (h_new, stopped_new), stopped_new\n",
    "            \n",
    "            h_init = jnp.zeros(self.rnn.hidden_size)\n",
    "            (final_h, _), stop_flags = jax.lax.scan(\n",
    "                process_step, \n",
    "                (h_init, False), \n",
    "                sequence\n",
    "            )\n",
    "            \n",
    "            # Count steps until stop\n",
    "            steps_used = jnp.sum(~stop_flags) + 1\n",
    "            \n",
    "            logits = self.classifier(final_h)\n",
    "            return logits, steps_used\n",
    "\n",
    "# Test\n",
    "model = EarlyStopRNN(input_size=5, hidden_size=10, num_classes=3)\n",
    "test_sequence = bst.random.randn(50, 5)\n",
    "\n",
    "logits_full, steps_full = model(test_sequence, use_early_stop=False)\n",
    "logits_early, steps_early = model(test_sequence, use_early_stop=True)\n",
    "\n",
    "print(f\"Full sequence: {steps_full} steps\")\n",
    "print(f\"Early stopping: {steps_early} steps\")\n",
    "print(f\"Savings: {(1 - steps_early/steps_full)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Combining Control Flow Primitives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Adaptive learning with scan + cond\n",
    "def adaptive_gradient_descent(loss_fn, params_init, learning_rate_init, n_steps):\n",
    "    \"\"\"Gradient descent with adaptive learning rate.\"\"\"\n",
    "    \n",
    "    def step_fn(carry, _):\n",
    "        params, lr, prev_loss = carry\n",
    "        \n",
    "        # Compute loss and gradient\n",
    "        loss, grad = jax.value_and_grad(loss_fn)(params)\n",
    "        \n",
    "        # Adaptive learning rate\n",
    "        lr_new = jax.lax.cond(\n",
    "            loss < prev_loss,\n",
    "            lambda lr: lr * 1.1,  # Increase if improving\n",
    "            lambda lr: lr * 0.5,  # Decrease if getting worse\n",
    "            lr\n",
    "        )\n",
    "        \n",
    "        # Update parameters\n",
    "        params_new = params - lr_new * grad\n",
    "        \n",
    "        return (params_new, lr_new, loss), loss\n",
    "    \n",
    "    # Run optimization\n",
    "    carry_init = (params_init, learning_rate_init, float('inf'))\n",
    "    (final_params, final_lr, _), loss_history = jax.lax.scan(\n",
    "        step_fn,\n",
    "        carry_init,\n",
    "        jnp.arange(n_steps)\n",
    "    )\n",
    "    \n",
    "    return final_params, loss_history\n",
    "\n",
    "# Test on simple quadratic\n",
    "def quadratic_loss(x):\n",
    "    return (x - 5) ** 2\n",
    "\n",
    "final_x, losses = adaptive_gradient_descent(\n",
    "    quadratic_loss,\n",
    "    params_init=0.0,\n",
    "    learning_rate_init=0.1,\n",
    "    n_steps=50\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Adaptive Gradient Descent')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final x: {final_x} (target: 5.0)\")\n",
    "print(f\"Final loss: {quadratic_loss(final_x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Performance Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison: Different loop strategies\n",
    "def benchmark_loops():\n",
    "    # Setup\n",
    "    n = 100\n",
    "    \n",
    "    # 1. Python loop (slowest, doesn't JIT well)\n",
    "    def python_loop(x):\n",
    "        result = x\n",
    "        for i in range(n):\n",
    "            result = result * 0.99 + 0.01\n",
    "        return result\n",
    "    \n",
    "    # 2. fori_loop (better)\n",
    "    def fori_version(x):\n",
    "        def body(i, val):\n",
    "            return val * 0.99 + 0.01\n",
    "        return jax.lax.fori_loop(0, n, body, x)\n",
    "    \n",
    "    # 3. scan (best for when you need intermediates)\n",
    "    def scan_version(x):\n",
    "        def step(carry, _):\n",
    "            new_val = carry * 0.99 + 0.01\n",
    "            return new_val, new_val\n",
    "        final, _ = jax.lax.scan(step, x, jnp.arange(n))\n",
    "        return final\n",
    "    \n",
    "    x = jnp.array(1.0)\n",
    "    \n",
    "    # JIT compile\n",
    "    fori_jit = jax.jit(fori_version)\n",
    "    scan_jit = jax.jit(scan_version)\n",
    "    \n",
    "    # Warmup\n",
    "    _ = fori_jit(x)\n",
    "    _ = scan_jit(x)\n",
    "    \n",
    "    # Benchmark\n",
    "    times = {}\n",
    "    \n",
    "    start = time.time()\n",
    "    for _ in range(1000):\n",
    "        _ = python_loop(x)\n",
    "    times['Python loop'] = (time.time() - start) / 1000\n",
    "    \n",
    "    start = time.time()\n",
    "    for _ in range(1000):\n",
    "        _ = fori_jit(x).block_until_ready()\n",
    "    times['fori_loop (JIT)'] = (time.time() - start) / 1000\n",
    "    \n",
    "    start = time.time()\n",
    "    for _ in range(1000):\n",
    "        _ = scan_jit(x).block_until_ready()\n",
    "    times['scan (JIT)'] = (time.time() - start) / 1000\n",
    "    \n",
    "    return times\n",
    "\n",
    "times = benchmark_loops()\n",
    "print(\"Average time per iteration (1000 runs):\")\n",
    "for name, t in times.items():\n",
    "    print(f\"{name:20s}: {t*1e6:.2f} μs\")\n",
    "\n",
    "print(\"\\nKey takeaways:\")\n",
    "print(\"  - Use scan when you need all intermediate values\")\n",
    "print(\"  - Use fori_loop when you only need final result\")\n",
    "print(\"  - Avoid Python loops in JIT-compiled code\")\n",
    "print(\"  - Always combine with JIT for best performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we covered:\n",
    "\n",
    "1. **scan**: Most versatile loop primitive for sequential operations\n",
    "2. **while_loop**: Condition-based iteration\n",
    "3. **for_loop (fori_loop)**: Fixed-count iteration\n",
    "4. **cond**: Efficient if-else branching\n",
    "5. **switch**: Multi-way branching\n",
    "6. **RNN Processing**: Using scan for recurrent networks\n",
    "7. **Early Stopping**: Combining loops with conditions\n",
    "8. **Adaptive Algorithms**: Composing control flow primitives\n",
    "9. **Performance**: Best practices for each primitive\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **Python control flow doesn't work well with JIT**\n",
    "- Use **scan** for RNNs and sequential processing\n",
    "- Use **while_loop** for convergence/iteration problems\n",
    "- Use **fori_loop** when you only need final result\n",
    "- Use **cond/switch** instead of if/elif/else\n",
    "- All primitives are **JIT-compatible and differentiable**\n",
    "- Combine primitives for complex control flow\n",
    "\n",
    "## When to Use What\n",
    "\n",
    "| Primitive | Use Case | Example |\n",
    "|-----------|----------|----------|\n",
    "| `scan` | Sequential with state | RNN, cumulative sum |\n",
    "| `fori_loop` | Fixed iteration count | Matrix power, warmup |\n",
    "| `while_loop` | Convergence/condition | Newton's method, search |\n",
    "| `cond` | If-else branching | Activation functions |\n",
    "| `switch` | Multi-way branching | Operation selection |\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next tutorial, we'll explore:\n",
    "- **Other advanced transformations**\n",
    "- Gradient checkpointing (remat)\n",
    "- Abstract initialization\n",
    "- Computation tracing with make_jaxpr\n",
    "- Progress bars for long computations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 13: Advanced Transformations\n",
    "\n",
    "In this final tutorial on program transformations, we'll explore advanced techniques for optimizing memory, debugging, and monitoring your BrainState programs.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "- Use gradient checkpointing (remat) to reduce memory usage\n",
    "- Initialize models abstractly without allocating memory\n",
    "- Inspect compiled computations with `make_jaxpr`\n",
    "- Add progress bars to long-running computations\n",
    "- Understand computational graphs\n",
    "- Profile and optimize transformation pipelines\n",
    "\n",
    "## What Are Advanced Transformations?\n",
    "\n",
    "Beyond the core transformations (JIT, grad, vmap), JAX and BrainState provide specialized tools for:\n",
    "- Memory optimization\n",
    "- Debugging and introspection\n",
    "- User experience improvements\n",
    "- Performance profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brainstate as bst\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import time\n",
    "\n",
    "# Set random seed\n",
    "bst.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gradient Checkpointing (Rematerialization)\n",
    "\n",
    "Gradient checkpointing trades computation for memory by recomputing intermediate values during backprop instead of storing them.\n",
    "\n",
    "### The Memory Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep network - stores all activations for backprop\n",
    "def deep_network(x, n_layers=10):\n",
    "    \"\"\"Each layer's activations are stored for gradient computation.\"\"\"\n",
    "    for i in range(n_layers):\n",
    "        x = jnp.tanh(x)  # Activation stored in memory\n",
    "        x = x @ jnp.eye(x.shape[-1])  # Linear transform\n",
    "    return jnp.sum(x ** 2)\n",
    "\n",
    "# Gradient function\n",
    "grad_fn = jax.grad(deep_network)\n",
    "\n",
    "x = bst.random.randn(100, 100)\n",
    "grads = grad_fn(x, n_layers=100)\n",
    "\n",
    "print(\"Without checkpointing: All intermediate activations stored in memory\")\n",
    "print(\"Memory usage grows linearly with network depth\")\n",
    "print(f\"For {100} layers, storing ~{100 * 100 * 100 * 4 / 1e6:.1f} MB of activations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution: Gradient Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use jax.checkpoint (formerly jax.remat)\n",
    "def deep_network_checkpointed(x, n_layers=10):\n",
    "    \"\"\"Checkpoint every few layers to save memory.\"\"\"\n",
    "    checkpoint_every = 10\n",
    "    \n",
    "    for i in range(n_layers):\n",
    "        x = jnp.tanh(x)\n",
    "        x = x @ jnp.eye(x.shape[-1])\n",
    "        \n",
    "        # Checkpoint at intervals\n",
    "        if (i + 1) % checkpoint_every == 0:\n",
    "            x = jax.checkpoint(lambda y: y)(x)\n",
    "    \n",
    "    return jnp.sum(x ** 2)\n",
    "\n",
    "# Gradient with checkpointing\n",
    "grad_fn_checkpoint = jax.grad(deep_network_checkpointed)\n",
    "grads_checkpoint = grad_fn_checkpoint(x, n_layers=100)\n",
    "\n",
    "print(\"With checkpointing: Only checkpoint activations stored\")\n",
    "print(\"Other activations recomputed during backprop\")\n",
    "print(\"Trade: 2x computation for ~10x less memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Example: Transformer Block with Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified transformer block\n",
    "class TransformerBlock(bst.graph.Node):\n",
    "    def __init__(self, dim, use_checkpoint=False):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.W_q = bst.ParamState(bst.random.randn(dim, dim) * 0.1)\n",
    "        self.W_k = bst.ParamState(bst.random.randn(dim, dim) * 0.1)\n",
    "        self.W_v = bst.ParamState(bst.random.randn(dim, dim) * 0.1)\n",
    "        self.W_o = bst.ParamState(bst.random.randn(dim, dim) * 0.1)\n",
    "        self.W_ff1 = bst.ParamState(bst.random.randn(dim, 4*dim) * 0.1)\n",
    "        self.W_ff2 = bst.ParamState(bst.random.randn(4*dim, dim) * 0.1)\n",
    "    \n",
    "    def attention(self, x):\n",
    "        \"\"\"Self-attention mechanism.\"\"\"\n",
    "        Q = x @ self.W_q.value\n",
    "        K = x @ self.W_k.value\n",
    "        V = x @ self.W_v.value\n",
    "        \n",
    "        scores = Q @ K.T / jnp.sqrt(self.dim)\n",
    "        attn = jax.nn.softmax(scores, axis=-1)\n",
    "        out = attn @ V\n",
    "        return out @ self.W_o.value\n",
    "    \n",
    "    def feedforward(self, x):\n",
    "        \"\"\"Feedforward network.\"\"\"\n",
    "        h = jax.nn.relu(x @ self.W_ff1.value)\n",
    "        return h @ self.W_ff2.value\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Attention with residual\n",
    "        attn_fn = self.attention\n",
    "        if self.use_checkpoint:\n",
    "            attn_fn = jax.checkpoint(self.attention)\n",
    "        x = x + attn_fn(x)\n",
    "        \n",
    "        # Feedforward with residual\n",
    "        ff_fn = self.feedforward\n",
    "        if self.use_checkpoint:\n",
    "            ff_fn = jax.checkpoint(self.feedforward)\n",
    "        x = x + ff_fn(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Compare memory usage (conceptual)\n",
    "block_normal = TransformerBlock(dim=64, use_checkpoint=False)\n",
    "block_checkpoint = TransformerBlock(dim=64, use_checkpoint=True)\n",
    "\n",
    "x = bst.random.randn(32, 64)  # (seq_len, dim)\n",
    "\n",
    "print(\"Transformer Block Comparison:\")\n",
    "print(\"  Without checkpoint: Stores all attention scores and FF activations\")\n",
    "print(\"  With checkpoint: Recomputes attention and FF during backprop\")\n",
    "print(\"  \\nRecommendation: Use for deep transformers (>12 layers)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Abstract Initialization\n",
    "\n",
    "Initialize models without allocating actual arrays - useful for inspecting shapes and structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abstract shapes without concrete values\n",
    "from jax import ShapeDtypeStruct\n",
    "\n",
    "def init_model_abstract(input_shape):\n",
    "    \"\"\"Initialize model with abstract shapes only.\"\"\"\n",
    "    # Create abstract input\n",
    "    abstract_x = ShapeDtypeStruct(input_shape, jnp.float32)\n",
    "    \n",
    "    # Trace model initialization\n",
    "    def trace_shapes(x):\n",
    "        # Simulate model\n",
    "        h1 = jnp.zeros((x.shape[0], 128))  # First layer\n",
    "        h2 = jnp.zeros((h1.shape[0], 64))  # Second layer\n",
    "        out = jnp.zeros((h2.shape[0], 10)) # Output layer\n",
    "        return {'h1': h1.shape, 'h2': h2.shape, 'out': out.shape}\n",
    "    \n",
    "    # Get shapes without allocating memory\n",
    "    shapes = jax.eval_shape(trace_shapes, abstract_x)\n",
    "    return shapes\n",
    "\n",
    "# Get model structure\n",
    "model_shapes = init_model_abstract((32, 784))\n",
    "print(\"Model architecture (shapes only, no memory allocated):\")\n",
    "for name, shape in model_shapes.items():\n",
    "    print(f\"  {name}: {shape}\")\n",
    "\n",
    "print(\"\\nUse case: Validate model architecture before training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical: Shape Inference for Complex Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex model with dynamic shapes\n",
    "class DynamicNet(bst.graph.Node):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = bst.nn.Conv2d(3, 16, kernel_size=(3, 3))\n",
    "        self.conv2 = bst.nn.Conv2d(16, 32, kernel_size=(3, 3))\n",
    "        # Linear layer size depends on input size\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = jax.nn.relu(self.conv1(x))\n",
    "        x = jax.nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        x = jax.nn.relu(self.conv2(x))\n",
    "        x = jax.nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        x = x.reshape(x.shape[0], -1)  # Flatten\n",
    "        return x\n",
    "\n",
    "model = DynamicNet()\n",
    "\n",
    "# Find output shape for different input sizes\n",
    "input_sizes = [(1, 3, 32, 32), (1, 3, 64, 64), (1, 3, 128, 128)]\n",
    "\n",
    "print(\"Output shapes for different input sizes:\")\n",
    "for input_size in input_sizes:\n",
    "    abstract_input = ShapeDtypeStruct(input_size, jnp.float32)\n",
    "    output_shape = jax.eval_shape(model, abstract_input)\n",
    "    print(f\"  Input {input_size} -> Output {output_shape.shape}\")\n",
    "    print(f\"    Flattened size: {output_shape.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Make Jaxpr: Inspecting Compiled Computations\n",
    "\n",
    "`make_jaxpr` shows you the internal representation of your computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function\n",
    "def simple_fn(x, y):\n",
    "    return x ** 2 + y ** 2\n",
    "\n",
    "# Get jaxpr (JAX expression)\n",
    "jaxpr = jax.make_jaxpr(simple_fn)(3.0, 4.0)\n",
    "print(\"Jaxpr for x² + y²:\")\n",
    "print(jaxpr)\n",
    "print(\"\\nJaxpr shows primitive operations JAX will execute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Jaxpr for Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complex example\n",
    "def neural_layer(x, W, b):\n",
    "    return jax.nn.relu(x @ W + b)\n",
    "\n",
    "# Create jaxpr\n",
    "x_ex = jnp.ones((2, 3))\n",
    "W_ex = jnp.ones((3, 4))\n",
    "b_ex = jnp.zeros(4)\n",
    "\n",
    "jaxpr_layer = jax.make_jaxpr(neural_layer)(x_ex, W_ex, b_ex)\n",
    "print(\"Jaxpr for ReLU(xW + b):\")\n",
    "print(jaxpr_layer)\n",
    "print(\"\\nUse cases:\")\n",
    "print(\"  - Verify computation is what you expect\")\n",
    "print(\"  - Debug performance issues\")\n",
    "print(\"  - Understand how transforms affect code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaxpr with Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original vs JIT vs vmap\n",
    "def square_sum(x):\n",
    "    return jnp.sum(x ** 2)\n",
    "\n",
    "x_single = jnp.array([1.0, 2.0, 3.0])\n",
    "x_batch = jnp.array([[1.0, 2.0, 3.0],\n",
    "                     [4.0, 5.0, 6.0]])\n",
    "\n",
    "print(\"Original function:\")\n",
    "print(jax.make_jaxpr(square_sum)(x_single))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"With vmap:\")\n",
    "vmapped = jax.vmap(square_sum)\n",
    "print(jax.make_jaxpr(vmapped)(x_batch))\n",
    "\n",
    "print(\"\\nNotice how vmap changes the computation graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Progress Bars for Long Computations\n",
    "\n",
    "Add progress tracking to training loops and long-running operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual progress tracking\n",
    "def train_with_progress(n_epochs):\n",
    "    \"\"\"Training loop with progress updates.\"\"\"\n",
    "    print(\"Training Progress:\")\n",
    "    print(\"[\" + \" \" * 50 + \"] 0%\", end='\\r')\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Simulate training\n",
    "        time.sleep(0.05)\n",
    "        \n",
    "        # Update progress\n",
    "        progress = (epoch + 1) / n_epochs\n",
    "        filled = int(50 * progress)\n",
    "        bar = \"#\" * filled + \" \" * (50 - filled)\n",
    "        print(f\"[{bar}] {progress*100:.0f}%\", end='\\r')\n",
    "    \n",
    "    print(\"\\nTraining complete!\")\n",
    "\n",
    "train_with_progress(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using tqdm for Better Progress Bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to import tqdm\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    has_tqdm = True\n",
    "except ImportError:\n",
    "    has_tqdm = False\n",
    "    print(\"tqdm not available. Install with: pip install tqdm\")\n",
    "\n",
    "if has_tqdm:\n",
    "    # Training with tqdm\n",
    "    def train_with_tqdm(n_epochs):\n",
    "        losses = []\n",
    "        \n",
    "        with tqdm(total=n_epochs, desc=\"Training\") as pbar:\n",
    "            for epoch in range(n_epochs):\n",
    "                # Simulate training\n",
    "                loss = 1.0 / (epoch + 1)  # Decreasing loss\n",
    "                losses.append(loss)\n",
    "                \n",
    "                # Update progress with metrics\n",
    "                pbar.set_postfix({'loss': f'{loss:.4f}'})\n",
    "                pbar.update(1)\n",
    "                \n",
    "                time.sleep(0.02)\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    losses = train_with_tqdm(30)\n",
    "    \n",
    "    # Plot training curve\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Progress')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping tqdm example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Host Callback for Debugging\n",
    "\n",
    "Use host callbacks to inspect values during JIT execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_calls": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug with host callback\n",
    "from jax.experimental import io_callback\n",
    "\n",
    "def debug_print(x, name):\n",
    "    \"\"\"Print value during JIT execution.\"\"\"\n",
    "    def _print(x):\n",
    "        print(f\"{name}: {x}\")\n",
    "    \n",
    "    io_callback(_print, None, x)\n",
    "    return x\n",
    "\n",
    "@jax.jit\n",
    "def computation_with_debug(x):\n",
    "    x = x * 2\n",
    "    x = debug_print(x, \"After multiply\")\n",
    "    \n",
    "    x = jnp.tanh(x)\n",
    "    x = debug_print(x, \"After tanh\")\n",
    "    \n",
    "    return jnp.sum(x)\n",
    "\n",
    "result = computation_with_debug(jnp.array([1.0, 2.0, 3.0]))\n",
    "print(f\"\\nFinal result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Custom Pretty Printing for Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model summary utility\n",
    "class SummaryNet(bst.graph.Node):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = bst.nn.Conv2d(3, 16, kernel_size=(3, 3))\n",
    "        self.conv2 = bst.nn.Conv2d(16, 32, kernel_size=(3, 3))\n",
    "        self.linear1 = bst.nn.Linear(32 * 6 * 6, 128)\n",
    "        self.linear2 = bst.nn.Linear(128, 10)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = jax.nn.relu(self.conv1(x))\n",
    "        x = jax.nn.max_pool(x, (2, 2), (2, 2))\n",
    "        x = jax.nn.relu(self.conv2(x))\n",
    "        x = jax.nn.max_pool(x, (2, 2), (2, 2))\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = jax.nn.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "def print_model_summary(model, input_shape):\n",
    "    \"\"\"Print model architecture summary.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Model: {model.__class__.__name__}\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"{'Layer':<20} {'Output Shape':<25} {'Param #':<15}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Get all parameters\n",
    "    total_params = 0\n",
    "    param_states = model.states(bst.ParamState)\n",
    "    \n",
    "    for name, state in param_states.items():\n",
    "        params = state.value.size\n",
    "        total_params += params\n",
    "        print(f\"{name:<20} {str(state.value.shape):<25} {params:<15,}\")\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Total memory: ~{total_params * 4 / 1024 / 1024:.2f} MB (float32)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "# Test\n",
    "model = SummaryNet()\n",
    "# Initialize model\n",
    "_ = model(bst.random.randn(1, 3, 32, 32))\n",
    "print_model_summary(model, (1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Profiling and Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple profiling utility\n",
    "class Timer:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self.start = time.time()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, *args):\n",
    "        self.elapsed = time.time() - self.start\n",
    "        print(f\"{self.name}: {self.elapsed*1000:.2f} ms\")\n",
    "\n",
    "# Profile different operations\n",
    "x = bst.random.randn(1000, 1000)\n",
    "y = bst.random.randn(1000, 1000)\n",
    "\n",
    "with Timer(\"Matrix multiplication (JIT warmup)\"):\n",
    "    @jax.jit\n",
    "    def matmul(a, b):\n",
    "        return a @ b\n",
    "    result = matmul(x, y)\n",
    "    result.block_until_ready()\n",
    "\n",
    "with Timer(\"Matrix multiplication (JIT warm)\"):\n",
    "    result = matmul(x, y)\n",
    "    result.block_until_ready()\n",
    "\n",
    "with Timer(\"Element-wise operations\"):\n",
    "    result = jnp.sin(x) ** 2 + jnp.cos(x) ** 2\n",
    "    result.block_until_ready()\n",
    "\n",
    "with Timer(\"Reduction operations\"):\n",
    "    result = jnp.sum(x, axis=0)\n",
    "    result.block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparative Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark suite\n",
    "def benchmark_transforms(size=1000, n_runs=10):\n",
    "    \"\"\"Compare performance of different transformation strategies.\"\"\"\n",
    "    x = bst.random.randn(size, size)\n",
    "    \n",
    "    # Function to test\n",
    "    def compute(x):\n",
    "        for _ in range(10):\n",
    "            x = jnp.tanh(x)\n",
    "        return jnp.sum(x)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. No JIT\n",
    "    times = []\n",
    "    for _ in range(n_runs):\n",
    "        start = time.time()\n",
    "        _ = compute(x)\n",
    "        times.append(time.time() - start)\n",
    "    results['No JIT'] = np.mean(times) * 1000\n",
    "    \n",
    "    # 2. With JIT\n",
    "    compute_jit = jax.jit(compute)\n",
    "    _ = compute_jit(x)  # Warmup\n",
    "    times = []\n",
    "    for _ in range(n_runs):\n",
    "        start = time.time()\n",
    "        result = compute_jit(x)\n",
    "        result.block_until_ready()\n",
    "        times.append(time.time() - start)\n",
    "    results['JIT'] = np.mean(times) * 1000\n",
    "    \n",
    "    # 3. JIT + vmap (for batched version)\n",
    "    x_batched = bst.random.randn(10, size, size)\n",
    "    compute_jit_vmap = jax.jit(jax.vmap(compute))\n",
    "    _ = compute_jit_vmap(x_batched)  # Warmup\n",
    "    times = []\n",
    "    for _ in range(n_runs):\n",
    "        start = time.time()\n",
    "        result = compute_jit_vmap(x_batched)\n",
    "        result.block_until_ready()\n",
    "        times.append(time.time() - start)\n",
    "    results['JIT + vmap (10 batches)'] = np.mean(times) * 1000\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run benchmark\n",
    "print(\"Running benchmarks...\")\n",
    "benchmark_results = benchmark_transforms(size=500, n_runs=10)\n",
    "\n",
    "print(\"\\nBenchmark Results (average time):\")\n",
    "print(\"-\" * 50)\n",
    "for name, time_ms in benchmark_results.items():\n",
    "    print(f\"{name:<30}: {time_ms:>8.2f} ms\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 5))\n",
    "names = list(benchmark_results.keys())\n",
    "times = list(benchmark_results.values())\n",
    "plt.bar(names, times)\n",
    "plt.ylabel('Time (ms)')\n",
    "plt.title('Transformation Performance Comparison')\n",
    "plt.xticks(rotation=15, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Putting It All Together: Production Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production-ready training loop with all optimizations\n",
    "class ProductionTrainer:\n",
    "    def __init__(self, model, use_checkpoint=True, show_progress=True):\n",
    "        self.model = model\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.show_progress = show_progress\n",
    "        \n",
    "        # Compile training step\n",
    "        self.train_step_fn = jax.jit(self._train_step)\n",
    "    \n",
    "    def _train_step(self, x, y, learning_rate):\n",
    "        \"\"\"Single training step.\"\"\"\n",
    "        def loss_fn():\n",
    "            pred = self.model(x)\n",
    "            if self.use_checkpoint:\n",
    "                pred = jax.checkpoint(lambda p: p)(pred)\n",
    "            return jnp.mean((pred - y) ** 2)\n",
    "        \n",
    "        # Compute gradients\n",
    "        loss, grads = bst.augment.grad(\n",
    "            loss_fn, \n",
    "            self.model.states(bst.ParamState),\n",
    "            return_value=True\n",
    "        )()\n",
    "        \n",
    "        # Update parameters\n",
    "        for key, grad in grads.items():\n",
    "            self.model.states()[key].value -= learning_rate * grad\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def train(self, train_data, n_epochs, learning_rate=0.01):\n",
    "        \"\"\"Full training loop.\"\"\"\n",
    "        history = {'loss': [], 'time': []}\n",
    "        \n",
    "        iterator = range(n_epochs)\n",
    "        if self.show_progress and has_tqdm:\n",
    "            iterator = tqdm(iterator, desc=\"Training\")\n",
    "        \n",
    "        for epoch in iterator:\n",
    "            epoch_start = time.time()\n",
    "            \n",
    "            # Training step\n",
    "            x, y = train_data\n",
    "            loss = self.train_step_fn(x, y, learning_rate)\n",
    "            \n",
    "            # Track metrics\n",
    "            history['loss'].append(float(loss))\n",
    "            history['time'].append(time.time() - epoch_start)\n",
    "            \n",
    "            # Update progress\n",
    "            if self.show_progress and has_tqdm:\n",
    "                iterator.set_postfix({'loss': f'{loss:.4f}'})\n",
    "        \n",
    "        return history\n",
    "\n",
    "# Demo\n",
    "class SimpleModel(bst.graph.Node):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = bst.nn.Linear(10, 5)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = SimpleModel()\n",
    "trainer = ProductionTrainer(model, use_checkpoint=True, show_progress=has_tqdm)\n",
    "\n",
    "# Generate dummy data\n",
    "x_train = bst.random.randn(32, 10)\n",
    "y_train = bst.random.randn(32, 5)\n",
    "\n",
    "# Train\n",
    "history = trainer.train((x_train, y_train), n_epochs=50, learning_rate=0.01)\n",
    "\n",
    "# Plot results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(history['loss'])\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(history['time'])\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Time (s)')\n",
    "ax2.set_title('Time per Epoch')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining completed!\")\n",
    "print(f\"Final loss: {history['loss'][-1]:.4f}\")\n",
    "print(f\"Average time per epoch: {np.mean(history['time'])*1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we covered advanced transformations:\n",
    "\n",
    "1. **Gradient Checkpointing**: Trade computation for memory in deep networks\n",
    "2. **Abstract Initialization**: Inspect shapes without allocating memory\n",
    "3. **Make Jaxpr**: Understand compiled computation graphs\n",
    "4. **Progress Bars**: Improve user experience during training\n",
    "5. **Host Callbacks**: Debug JIT-compiled code\n",
    "6. **Model Summaries**: Pretty-print model architecture\n",
    "7. **Profiling**: Measure and optimize performance\n",
    "8. **Production Training**: Combine all techniques\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **Gradient checkpointing** is essential for very deep networks\n",
    "- **Abstract initialization** helps validate architectures efficiently\n",
    "- **make_jaxpr** is invaluable for debugging transformations\n",
    "- **Progress bars** greatly improve user experience\n",
    "- **Profiling** helps identify bottlenecks\n",
    "- Combine techniques for production-ready code\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. Use gradient checkpointing for networks with >50 layers\n",
    "2. Always profile before optimizing\n",
    "3. Add progress bars for long-running operations\n",
    "4. Use abstract initialization to validate shapes\n",
    "5. Combine JIT + vmap for best performance\n",
    "6. Monitor memory usage in training loops\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You've completed all the transformation tutorials! You now know:\n",
    "- JIT compilation and optimization\n",
    "- Automatic differentiation\n",
    "- Vectorization with vmap/pmap\n",
    "- Control flow primitives\n",
    "- Advanced transformations\n",
    "\n",
    "These tools form the foundation for building efficient, scalable neural networks with BrainState!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

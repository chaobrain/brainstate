{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 10: Automatic Differentiation\n",
    "\n",
    "In this tutorial, we'll explore automatic differentiation (autodiff) in BrainState, which is essential for training neural networks and optimizing models.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "- Compute gradients using `grad`\n",
    "- Calculate Jacobian and Hessian matrices\n",
    "- Use `vector_grad` for efficient vector gradients\n",
    "- Work with the `GradientTransform` class\n",
    "- Apply gradients in training loops\n",
    "- Understand gradient flow through stateful modules\n",
    "\n",
    "## What is Automatic Differentiation?\n",
    "\n",
    "Automatic differentiation is a technique for computing derivatives of functions automatically. Unlike:\n",
    "- **Numerical differentiation**: Uses finite differences (inaccurate and slow)\n",
    "- **Symbolic differentiation**: Manipulates mathematical expressions (can be complex)\n",
    "\n",
    "Automatic differentiation uses the chain rule to compute exact derivatives efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brainstate as bst\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "bst.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Gradient Computation with `grad`\n",
    "\n",
    "The `grad` function computes the gradient of a scalar function with respect to its inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function: f(x) = x^2\n",
    "def square(x):\n",
    "    return x ** 2\n",
    "\n",
    "# Compute gradient: df/dx = 2x\n",
    "grad_square = bst.transform.grad(square)\n",
    "\n",
    "x = 3.0\n",
    "print(f\"f({x}) = {square(x)}\")\n",
    "print(f\"df/dx at x={x}: {grad_square(x)}\")\n",
    "print(f\"Expected (2*x): {2*x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients of More Complex Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f(x) = sin(x^2)\n",
    "def complex_fn(x):\n",
    "    return jnp.sin(x ** 2)\n",
    "\n",
    "# df/dx = 2x * cos(x^2)\n",
    "grad_complex = bst.transform.grad(complex_fn)\n",
    "\n",
    "# Visualize function and gradient\n",
    "x_vals = jnp.linspace(-3, 3, 100)\n",
    "y_vals = jnp.array([complex_fn(x) for x in x_vals])\n",
    "grad_vals = jnp.array([grad_complex(x) for x in x_vals])\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(x_vals, y_vals, 'b-', linewidth=2)\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('f(x) = sin(x²)')\n",
    "ax1.set_title('Function')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(x_vals, grad_vals, 'r-', linewidth=2)\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel(\"f'(x) = 2x·cos(x²)\")\n",
    "ax2.set_title('Gradient')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gradients with Multiple Arguments\n",
    "\n",
    "By default, `grad` computes the gradient with respect to the first argument. Use `argnums` to specify which argument(s) to differentiate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f(x, y) = x^2 * y + y^3\n",
    "def multi_arg_fn(x, y):\n",
    "    return x**2 * y + y**3\n",
    "\n",
    "# Gradient with respect to first argument (x)\n",
    "grad_x = bst.transform.grad(multi_arg_fn, argnums=0)\n",
    "\n",
    "# Gradient with respect to second argument (y)\n",
    "grad_y = bst.transform.grad(multi_arg_fn, argnums=1)\n",
    "\n",
    "# Gradient with respect to both arguments\n",
    "grad_both = bst.transform.grad(multi_arg_fn, argnums=(0, 1))\n",
    "\n",
    "x, y = 2.0, 3.0\n",
    "print(f\"f({x}, {y}) = {multi_arg_fn(x, y)}\")\n",
    "print(f\"∂f/∂x = {grad_x(x, y)} (expected: 2xy = {2*x*y})\")\n",
    "print(f\"∂f/∂y = {grad_y(x, y)} (expected: x² + 3y² = {x**2 + 3*y**2})\")\n",
    "print(f\"Both gradients: {grad_both(x, y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradients of Vector-Valued Functions\n",
    "\n",
    "For vector inputs, `grad` computes the gradient of a scalar output with respect to the vector input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 norm squared: f(x) = ||x||^2 = sum(x_i^2)\n",
    "def norm_squared(x):\n",
    "    return jnp.sum(x ** 2)\n",
    "\n",
    "# Gradient: df/dx_i = 2*x_i\n",
    "grad_norm = bst.transform.grad(norm_squared)\n",
    "\n",
    "x = jnp.array([1.0, 2.0, 3.0])\n",
    "print(f\"x = {x}\")\n",
    "print(f\"f(x) = {norm_squared(x)}\")\n",
    "print(f\"∇f = {grad_norm(x)}\")\n",
    "print(f\"Expected (2x): {2*x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Gradient Descent\n",
    "\n",
    "Let's use gradients to minimize a quadratic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quadratic bowl: f(x, y) = (x-1)^2 + (y-2)^2\n",
    "# Minimum at (1, 2)\n",
    "def quadratic_bowl(params):\n",
    "    x, y = params\n",
    "    return (x - 1)**2 + (y - 2)**2\n",
    "\n",
    "grad_fn = bst.transform.grad(quadratic_bowl)\n",
    "\n",
    "# Gradient descent\n",
    "params = jnp.array([5.0, -3.0])  # Start far from minimum\n",
    "learning_rate = 0.1\n",
    "history = [params]\n",
    "\n",
    "for i in range(50):\n",
    "    grads = grad_fn(params)\n",
    "    params = params - learning_rate * grads\n",
    "    history.append(params)\n",
    "\n",
    "history = jnp.array(history)\n",
    "\n",
    "# Visualize optimization path\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Create contour plot\n",
    "x_range = jnp.linspace(-1, 6, 100)\n",
    "y_range = jnp.linspace(-4, 4, 100)\n",
    "X, Y = jnp.meshgrid(x_range, y_range)\n",
    "Z = (X - 1)**2 + (Y - 2)**2\n",
    "\n",
    "contour = ax.contour(X, Y, Z, levels=20, cmap='viridis', alpha=0.6)\n",
    "ax.clabel(contour, inline=True, fontsize=8)\n",
    "\n",
    "# Plot optimization path\n",
    "ax.plot(history[:, 0], history[:, 1], 'ro-', linewidth=2, markersize=4, label='Optimization path')\n",
    "ax.plot(history[0, 0], history[0, 1], 'go', markersize=10, label='Start')\n",
    "ax.plot(history[-1, 0], history[-1, 1], 'r*', markersize=15, label='End')\n",
    "ax.plot(1, 2, 'b*', markersize=15, label='True minimum')\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('Gradient Descent Optimization')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final params: {history[-1]}\")\n",
    "print(f\"Final loss: {quadratic_bowl(history[-1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Jacobian Matrices\n",
    "\n",
    "The Jacobian is the matrix of all first-order partial derivatives of a vector-valued function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector-valued function: R^2 -> R^3\n",
    "# f([x, y]) = [x^2 + y, x*y, y^2]\n",
    "def vector_fn(xy):\n",
    "    x, y = xy\n",
    "    return jnp.array([x**2 + y, x*y, y**2])\n",
    "\n",
    "# Compute Jacobian\n",
    "jacobian_fn = bst.transform.jacobian(vector_fn)\n",
    "\n",
    "xy = jnp.array([2.0, 3.0])\n",
    "jac = jacobian_fn(xy)\n",
    "\n",
    "print(\"Input:\", xy)\n",
    "print(\"Output:\", vector_fn(xy))\n",
    "print(\"\\nJacobian matrix:\")\n",
    "print(jac)\n",
    "print(\"\\nExpected Jacobian:\")\n",
    "print(\"[[2x,  1],   [[4, 1],\")\n",
    "print(\" [ y,  x],  = [3, 2],\")\n",
    "print(\" [ 0, 2y]]    [0, 6]]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jacobian for Neural Network Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple linear transformation: y = Wx + b\n",
    "def linear_layer(x, W, b):\n",
    "    return W @ x + b\n",
    "\n",
    "# Create sample data\n",
    "W = jnp.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])  # 3x2 matrix\n",
    "b = jnp.array([0.1, 0.2, 0.3])\n",
    "x = jnp.array([1.0, 2.0])\n",
    "\n",
    "# Jacobian with respect to input x\n",
    "jacobian_x = bst.transform.jacobian(lambda x: linear_layer(x, W, b))\n",
    "\n",
    "jac_x = jacobian_x(x)\n",
    "print(\"Jacobian w.r.t. input x:\")\n",
    "print(jac_x)\n",
    "print(\"\\nThis should equal W:\")\n",
    "print(W)\n",
    "print(\"\\nMatch:\", jnp.allclose(jac_x, W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hessian Matrices\n",
    "\n",
    "The Hessian is the matrix of all second-order partial derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f(x, y) = x^3 + y^3 + 3xy\n",
    "def hessian_example(xy):\n",
    "    x, y = xy\n",
    "    return x**3 + y**3 + 3*x*y\n",
    "\n",
    "# Compute Hessian\n",
    "hessian_fn = bst.transform.hessian(hessian_example)\n",
    "\n",
    "xy = jnp.array([1.0, 2.0])\n",
    "hess = hessian_fn(xy)\n",
    "\n",
    "print(\"Hessian matrix:\")\n",
    "print(hess)\n",
    "print(\"\\nExpected Hessian:\")\n",
    "print(\"[[6x, 3 ],   [[6, 3],\")\n",
    "print(\" [3,  6y]]  = [3, 12]]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Hessian for Second-Order Optimization\n",
    "\n",
    "Newton's method uses the Hessian for faster convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rosenbrock function: f(x,y) = (1-x)^2 + 100(y-x^2)^2\n",
    "# Minimum at (1, 1)\n",
    "def rosenbrock(xy):\n",
    "    x, y = xy\n",
    "    return (1 - x)**2 + 100 * (y - x**2)**2\n",
    "\n",
    "grad_rb = bst.transform.grad(rosenbrock)\n",
    "hess_rb = bst.transform.hessian(rosenbrock)\n",
    "\n",
    "# Newton's method\n",
    "params = jnp.array([0.0, 0.0])\n",
    "history_newton = [params]\n",
    "\n",
    "for i in range(10):\n",
    "    g = grad_rb(params)\n",
    "    H = hess_rb(params)\n",
    "    # Newton update: x_new = x - H^{-1} * g\n",
    "    params = params - jnp.linalg.solve(H, g)\n",
    "    history_newton.append(params)\n",
    "\n",
    "history_newton = jnp.array(history_newton)\n",
    "\n",
    "# Compare with gradient descent\n",
    "params_gd = jnp.array([0.0, 0.0])\n",
    "history_gd = [params_gd]\n",
    "lr = 0.001\n",
    "\n",
    "for i in range(1000):\n",
    "    g = grad_rb(params_gd)\n",
    "    params_gd = params_gd - lr * g\n",
    "    if i % 100 == 0:\n",
    "        history_gd.append(params_gd)\n",
    "\n",
    "history_gd = jnp.array(history_gd)\n",
    "\n",
    "print(f\"Newton's method final: {history_newton[-1]} (in {len(history_newton)} steps)\")\n",
    "print(f\"Gradient descent final: {history_gd[-1]} (in {len(history_gd)*100} steps)\")\n",
    "print(f\"True minimum: [1, 1]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Vector Gradients with `vector_grad`\n",
    "\n",
    "`vector_grad` is more efficient for computing gradients of vector-valued loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: gradient of multiple samples\n",
    "def batch_loss(params, batch):\n",
    "    # params: model parameters\n",
    "    # batch: multiple inputs\n",
    "    # Returns: vector of losses (one per sample)\n",
    "    return jnp.sum((params - batch) ** 2, axis=1)\n",
    "\n",
    "params = jnp.array([1.0, 2.0])\n",
    "batch = jnp.array([[1.0, 1.0], \n",
    "                   [2.0, 2.0],\n",
    "                   [3.0, 3.0]])\n",
    "\n",
    "# vector_grad computes gradient for each sample\n",
    "vgrad_fn = bst.transform.vector_grad(batch_loss)\n",
    "\n",
    "grads = vgrad_fn(params, batch)\n",
    "print(\"Per-sample gradients:\")\n",
    "print(grads)\n",
    "print(\"\\nShape:\", grads.shape)  # (3, 2) - one gradient per sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. GradientTransform Class\n",
    "\n",
    "The `GradientTransform` class provides a high-level interface for gradient computation with stateful modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple neural network module\n",
    "class SimpleNet(bst.graph.Node):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.w1 = bst.ParamState(bst.random.randn(input_dim, hidden_dim) * 0.1)\n",
    "        self.b1 = bst.ParamState(jnp.zeros(hidden_dim))\n",
    "        self.w2 = bst.ParamState(bst.random.randn(hidden_dim, output_dim) * 0.1)\n",
    "        self.b2 = bst.ParamState(jnp.zeros(output_dim))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        h = jnp.tanh(x @ self.w1.value + self.b1.value)\n",
    "        return h @ self.w2.value + self.b2.value\n",
    "\n",
    "# Create network\n",
    "net = SimpleNet(input_dim=2, hidden_dim=4, output_dim=1)\n",
    "\n",
    "# Define loss function\n",
    "def loss_fn(predictions, targets):\n",
    "    return jnp.mean((predictions - targets) ** 2)\n",
    "\n",
    "# Create gradient transform\n",
    "grad_transform = bst.transform.GradientTransform(net, loss_fn)\n",
    "\n",
    "# Generate sample data\n",
    "x_train = bst.random.randn(32, 2)\n",
    "y_train = bst.random.randn(32, 1)\n",
    "\n",
    "# Compute loss and gradients\n",
    "predictions = net(x_train)\n",
    "loss, grads = grad_transform(predictions, y_train)\n",
    "\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"\\nGradients keys: {list(grads.keys())}\")\n",
    "print(f\"\\nGradient w.r.t. w1 shape: {grads['w1'].shape}\")\n",
    "print(f\"Gradient w.r.t. b1 shape: {grads['b1'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training with Gradients\n",
    "\n",
    "Let's put it all together and train a network using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a regression dataset\n",
    "def generate_data(n_samples=100):\n",
    "    x = bst.random.randn(n_samples, 2)\n",
    "    # True function: y = 3*x1 - 2*x2 + 1 + noise\n",
    "    y = 3*x[:, 0:1] - 2*x[:, 1:2] + 1 + 0.1*bst.random.randn(n_samples, 1)\n",
    "    return x, y\n",
    "\n",
    "x_train, y_train = generate_data(100)\n",
    "x_test, y_test = generate_data(20)\n",
    "\n",
    "# Create network\n",
    "class RegressionNet(bst.graph.Node):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = bst.nn.Linear(2, 8, w_init=bst.init.KaimingNormal())\n",
    "        self.linear2 = bst.nn.Linear(8, 1, w_init=bst.init.KaimingNormal())\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = jnp.tanh(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "model = RegressionNet()\n",
    "\n",
    "# Training function\n",
    "def train_step(x, y, learning_rate=0.01):\n",
    "    # Forward pass\n",
    "    def loss_fn():\n",
    "        predictions = model(x)\n",
    "        return jnp.mean((predictions - y) ** 2)\n",
    "    \n",
    "    # Compute loss and gradients\n",
    "    loss_val, grads = bst.augment.grad(loss_fn, model.states(bst.ParamState), return_value=True)()\n",
    "    \n",
    "    # Update parameters\n",
    "    for key, grad in grads.items():\n",
    "        model.states()[key].value -= learning_rate * grad\n",
    "    \n",
    "    return loss_val\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(200):\n",
    "    # Train\n",
    "    train_loss = train_step(x_train, y_train, learning_rate=0.01)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Evaluate\n",
    "    with bst.environ.context(fit=False):\n",
    "        test_pred = model(x_test)\n",
    "        test_loss = jnp.mean((test_pred - y_test) ** 2)\n",
    "        test_losses.append(test_loss)\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Test Loss = {test_loss:.4f}\")\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss', alpha=0.7)\n",
    "plt.plot(test_losses, label='Test Loss', alpha=0.7)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot predictions vs targets\n",
    "plt.subplot(1, 2, 2)\n",
    "with bst.environ.context(fit=False):\n",
    "    final_pred = model(x_test)\n",
    "plt.scatter(y_test, final_pred, alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', label='Perfect prediction')\n",
    "plt.xlabel('True values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('Test Set Predictions')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced: Custom Gradient Rules\n",
    "\n",
    "Sometimes you want to define custom gradients for specific operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "# Define a function with custom gradient\n",
    "@jax.custom_vjp\n",
    "def clip_gradient(x, threshold=1.0):\n",
    "    # Forward pass: identity function\n",
    "    return x\n",
    "\n",
    "def clip_gradient_fwd(x, threshold=1.0):\n",
    "    return x, threshold\n",
    "\n",
    "def clip_gradient_bwd(threshold, g):\n",
    "    # Backward pass: clip gradients\n",
    "    return (jnp.clip(g, -threshold, threshold), None)\n",
    "\n",
    "clip_gradient.defvjp(clip_gradient_fwd, clip_gradient_bwd)\n",
    "\n",
    "# Test it\n",
    "def loss_with_clipping(x):\n",
    "    x = clip_gradient(x, threshold=0.5)\n",
    "    return jnp.sum(x ** 3)\n",
    "\n",
    "x = jnp.array([2.0, -3.0, 1.0])\n",
    "grad_fn = bst.transform.grad(loss_with_clipping)\n",
    "gradients = grad_fn(x)\n",
    "\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"Unclipped gradients would be: {3 * x**2}\")\n",
    "print(f\"Clipped gradients: {gradients}\")\n",
    "print(\"Notice gradients are clipped to [-0.5, 0.5]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Gradient Checkpointing for Memory Efficiency\n",
    "\n",
    "For very deep networks, you can trade computation for memory using gradient checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep network that might run out of memory\n",
    "class DeepNet(bst.graph.Node):\n",
    "    def __init__(self, n_layers=50):\n",
    "        super().__init__()\n",
    "        self.layers = [bst.nn.Linear(10, 10) for _ in range(n_layers)]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = jnp.tanh(layer(x))\n",
    "        return x\n",
    "\n",
    "# With checkpointing (saves memory during backprop)\n",
    "class DeepNetCheckpointed(bst.graph.Node):\n",
    "    def __init__(self, n_layers=50, checkpoint_every=10):\n",
    "        super().__init__()\n",
    "        self.checkpoint_every = checkpoint_every\n",
    "        self.layers = [bst.nn.Linear(10, 10) for _ in range(n_layers)]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = jnp.tanh(layer(x))\n",
    "            # Checkpoint at intervals\n",
    "            if (i + 1) % self.checkpoint_every == 0:\n",
    "                x = jax.checkpoint(lambda y: y)(x)\n",
    "        return x\n",
    "\n",
    "# Note: Actual memory savings would be visible in real training\n",
    "# This is just a demonstration of the pattern\n",
    "model_checkpointed = DeepNetCheckpointed(n_layers=20, checkpoint_every=5)\n",
    "x = bst.random.randn(4, 10)\n",
    "output = model_checkpointed(x)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(\"Checkpointing allows training deeper networks with limited memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we covered:\n",
    "\n",
    "1. **Basic Gradients**: Using `grad()` for scalar functions\n",
    "2. **Multiple Arguments**: Computing gradients with respect to different arguments\n",
    "3. **Vector Inputs**: Gradients of functions with vector inputs\n",
    "4. **Jacobians**: Computing full Jacobian matrices for vector-valued functions\n",
    "5. **Hessians**: Second-order derivatives for optimization\n",
    "6. **Vector Gradients**: Efficient per-sample gradients with `vector_grad`\n",
    "7. **GradientTransform**: High-level gradient computation for modules\n",
    "8. **Training Loops**: Practical gradient-based optimization\n",
    "9. **Custom Gradients**: Defining custom backward passes\n",
    "10. **Memory Optimization**: Gradient checkpointing for deep networks\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- BrainState's automatic differentiation builds on JAX's autodiff\n",
    "- `grad()` is your primary tool for gradient computation\n",
    "- Use `jacobian()` and `hessian()` for higher-order derivatives\n",
    "- `GradientTransform` simplifies gradient computation for stateful modules\n",
    "- Custom gradients allow fine-grained control over backpropagation\n",
    "- Gradient checkpointing trades computation for memory\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next tutorial, we'll explore:\n",
    "- **Vectorization with vmap and pmap**\n",
    "- Batching operations automatically\n",
    "- Parallel computation across devices\n",
    "- StatefulMapping for stateful transformations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

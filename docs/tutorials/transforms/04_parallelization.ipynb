{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelization with ``pmap``\n",
    "\n",
    "In this tutorial, we'll explore vectorization in BrainState using `vmap` (vectorized map) and `pmap` (parallel map) to write efficient, scalable code.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "- Understand the concept of automatic vectorization\n",
    "- Use `vmap` to vectorize functions automatically\n",
    "- Specify which axes to map over\n",
    "- Handle batched operations efficiently\n",
    "- Use `pmap` for multi-device parallelism\n",
    "- Work with `StatefulMapping` for stateful transformations\n",
    "- Apply vectorization to neural networks\n",
    "\n",
    "## Why Vectorization?\n",
    "\n",
    "Instead of writing explicit loops, vectorization allows you to:\n",
    "- Write code for single examples that automatically works for batches\n",
    "- Achieve better performance through parallelization\n",
    "- Keep code clean and readable\n",
    "- Leverage hardware acceleration (GPU/TPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brainstate as bst\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Set random seed\n",
    "bst.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Problem: Manual Batching\n",
    "\n",
    "Let's start by seeing why vectorization is useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that works on a single vector\n",
    "def compute_norm_squared(x):\n",
    "    \"\"\"Compute squared L2 norm of a vector.\"\"\"\n",
    "    return jnp.sum(x ** 2)\n",
    "\n",
    "# Single example\n",
    "single_x = jnp.array([1.0, 2.0, 3.0])\n",
    "result = compute_norm_squared(single_x)\n",
    "print(f\"Single example: {single_x}\")\n",
    "print(f\"Result: {result}\")\n",
    "\n",
    "# Batch of examples - manual loop (slow!)\n",
    "batch_x = jnp.array([[1.0, 2.0, 3.0],\n",
    "                     [4.0, 5.0, 6.0],\n",
    "                     [7.0, 8.0, 9.0]])\n",
    "\n",
    "# Bad approach: Python loop\n",
    "results_loop = []\n",
    "for i in range(len(batch_x)):\n",
    "    results_loop.append(compute_norm_squared(batch_x[i]))\n",
    "results_loop = jnp.array(results_loop)\n",
    "\n",
    "print(f\"\\nBatch results (manual loop): {results_loop}\")\n",
    "print(\"Problem: Python loops are slow and don't parallelize well!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Solution: vmap (Vectorized Map)\n",
    "\n",
    "`vmap` automatically transforms a function to work on batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the function\n",
    "compute_norm_squared_batched = bst.transform.vmap(compute_norm_squared)\n",
    "\n",
    "# Now it works on batches automatically!\n",
    "results_vmap = compute_norm_squared_batched(batch_x)\n",
    "print(f\"Batch results (vmap): {results_vmap}\")\n",
    "print(f\"Results match: {jnp.allclose(results_loop, results_vmap)}\")\n",
    "\n",
    "# Performance comparison\n",
    "large_batch = bst.random.randn(1000, 100)\n",
    "\n",
    "# Time manual loop\n",
    "start = time.time()\n",
    "_ = jnp.array([compute_norm_squared(x) for x in large_batch])\n",
    "loop_time = time.time() - start\n",
    "\n",
    "# Time vmap\n",
    "vmapped_fn = bst.transform.vmap(compute_norm_squared)\n",
    "vmapped_fn = jax.jit(vmapped_fn)  # JIT for fair comparison\n",
    "_ = vmapped_fn(large_batch)  # Warmup\n",
    "start = time.time()\n",
    "_ = vmapped_fn(large_batch)\n",
    "vmap_time = time.time() - start\n",
    "\n",
    "print(f\"\\nPerformance comparison (1000 samples):\")\n",
    "print(f\"Manual loop: {loop_time*1000:.2f} ms\")\n",
    "print(f\"vmap (JIT):  {vmap_time*1000:.2f} ms\")\n",
    "print(f\"Speedup: {loop_time/vmap_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Controlling the Mapped Axis\n",
    "\n",
    "Use `in_axes` and `out_axes` to specify which axes to map over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes two arguments\n",
    "def weighted_sum(x, weight):\n",
    "    \"\"\"Compute weighted sum: sum(x * weight)\"\"\"\n",
    "    return jnp.sum(x * weight)\n",
    "\n",
    "# Batch of vectors\n",
    "batch_x = jnp.array([[1.0, 2.0],\n",
    "                     [3.0, 4.0],\n",
    "                     [5.0, 6.0]])  # Shape: (3, 2)\n",
    "\n",
    "# Single weight vector (shared across batch)\n",
    "weight = jnp.array([0.5, 1.5])  # Shape: (2,)\n",
    "\n",
    "# Map over first argument only (in_axes=(0, None))\n",
    "batched_weighted_sum = bst.transform.vmap(weighted_sum, in_axes=(0, None))\n",
    "results = batched_weighted_sum(batch_x, weight)\n",
    "\n",
    "print(\"Batch:\", batch_x)\n",
    "print(\"Weight:\", weight)\n",
    "print(\"Results:\", results)\n",
    "print(\"Expected:\", jnp.array([1.0*0.5 + 2.0*1.5,\n",
    "                               3.0*0.5 + 4.0*1.5,\n",
    "                               5.0*0.5 + 6.0*1.5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Axes Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data with batch dimension in different positions\n",
    "x_batch_first = jnp.arange(12).reshape(3, 4)  # (batch=3, features=4)\n",
    "x_batch_last = jnp.arange(12).reshape(4, 3)   # (features=4, batch=3)\n",
    "\n",
    "def sum_features(x):\n",
    "    \"\"\"Sum all features for one sample.\"\"\"\n",
    "    return jnp.sum(x)\n",
    "\n",
    "# Map over axis 0 (default)\n",
    "vmap_axis0 = bst.transform.vmap(sum_features, in_axes=0)\n",
    "result_axis0 = vmap_axis0(x_batch_first)\n",
    "\n",
    "# Map over axis 1\n",
    "vmap_axis1 = bst.transform.vmap(sum_features, in_axes=1)\n",
    "result_axis1 = vmap_axis1(x_batch_last)\n",
    "\n",
    "print(\"Batch-first shape:\", x_batch_first.shape)\n",
    "print(\"Results (axis 0):\", result_axis0)\n",
    "print(\"\\nBatch-last shape:\", x_batch_last.shape)\n",
    "print(\"Results (axis 1):\", result_axis1)\n",
    "print(\"\\nResults match:\", jnp.allclose(result_axis0, result_axis1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Nested vmap\n",
    "\n",
    "You can nest `vmap` calls for multi-dimensional operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for single pair of vectors\n",
    "def dot_product(x, y):\n",
    "    return jnp.dot(x, y)\n",
    "\n",
    "# Compute all pairwise dot products\n",
    "vectors_a = jnp.array([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]])  # 3 vectors\n",
    "vectors_b = jnp.array([[1.0, 1.0], [1.0, -1.0]])             # 2 vectors\n",
    "\n",
    "# First vmap over vectors_a, then over vectors_b\n",
    "vmap_outer = bst.transform.vmap(lambda a: bst.transform.vmap(lambda b: dot_product(a, b))(vectors_b))\n",
    "pairwise_dots = vmap_outer(vectors_a)\n",
    "\n",
    "print(\"Vectors A (3x2):\")\n",
    "print(vectors_a)\n",
    "print(\"\\nVectors B (2x2):\")\n",
    "print(vectors_b)\n",
    "print(\"\\nPairwise dot products (3x2):\")\n",
    "print(pairwise_dots)\n",
    "print(\"\\nInterpretation: pairwise_dots[i,j] = dot(vectors_a[i], vectors_b[j])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simpler Syntax for Nested vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: use in_axes with None to broadcast\n",
    "# vmap over first arg, then vmap over second arg\n",
    "compute_all_dots = bst.transform.vmap(\n",
    "    bst.transform.vmap(dot_product, in_axes=(None, 0)),\n",
    "    in_axes=(0, None)\n",
    ")\n",
    "\n",
    "pairwise_dots_v2 = compute_all_dots(vectors_a, vectors_b)\n",
    "print(\"Same result:\", jnp.allclose(pairwise_dots, pairwise_dots_v2))\n",
    "print(pairwise_dots_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. vmap with Neural Networks\n",
    "\n",
    "Let's see how `vmap` works with stateful modules in BrainState."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple MLP\n",
    "class MLP(bst.graph.Node):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.layer1 = bst.nn.Linear(input_dim, hidden_dim)\n",
    "        self.layer2 = bst.nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = jnp.tanh(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "model = MLP(input_dim=4, hidden_dim=8, output_dim=2)\n",
    "\n",
    "# Single input\n",
    "single_input = bst.random.randn(4)\n",
    "single_output = model(single_input)\n",
    "print(f\"Single input shape: {single_input.shape}\")\n",
    "print(f\"Single output shape: {single_output.shape}\")\n",
    "\n",
    "# Batch input - the model already handles batches!\n",
    "# BrainState layers are designed to work with batched inputs\n",
    "batch_input = bst.random.randn(32, 4)\n",
    "batch_output = model(batch_input)\n",
    "print(f\"\\nBatch input shape: {batch_input.shape}\")\n",
    "print(f\"Batch output shape: {batch_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using vmap for Per-Example Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-example gradients (useful for privacy, adversarial training, etc.)\n",
    "def loss_single(x, y):\n",
    "    \"\"\"Loss for a single example.\"\"\"\n",
    "    pred = model(x)\n",
    "    return jnp.sum((pred - y) ** 2)\n",
    "\n",
    "# Generate batch data\n",
    "x_batch = bst.random.randn(4, 4)  # 4 samples\n",
    "y_batch = bst.random.randn(4, 2)\n",
    "\n",
    "# Get per-example gradients\n",
    "def compute_per_example_grads(x, y):\n",
    "    # Use vector_grad which is optimized for this use case\n",
    "    grad_fn = bst.transform.vector_grad(\n",
    "        lambda xs, ys: jnp.array([loss_single(xs[i], ys[i]) for i in range(len(xs))])\n",
    "    )\n",
    "    return grad_fn(model.states(bst.ParamState), x, y)\n",
    "\n",
    "# Note: This is a simplified example. In practice, you'd use more sophisticated approaches\n",
    "print(\"Per-example gradients allow fine-grained analysis of training dynamics\")\n",
    "print(\"Useful for:\")\n",
    "print(\"  - Differential privacy\")\n",
    "print(\"  - Identifying influential examples\")\n",
    "print(\"  - Adversarial robustness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. StatefulMapping for Stateful Transformations\n",
    "\n",
    "When you need to maintain state across mapped computations, use `StatefulMapping`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN cell that maintains hidden state\n",
    "class RNNCell(bst.graph.Node):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.Wxh = bst.ParamState(bst.random.randn(input_size, hidden_size) * 0.1)\n",
    "        self.Whh = bst.ParamState(bst.random.randn(hidden_size, hidden_size) * 0.1)\n",
    "        self.bh = bst.ParamState(jnp.zeros(hidden_size))\n",
    "        self.h = bst.ShortTermState(jnp.zeros(hidden_size))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Update hidden state\n",
    "        h_new = jnp.tanh(\n",
    "            x @ self.Wxh.value + \n",
    "            self.h.value @ self.Whh.value + \n",
    "            self.bh.value\n",
    "        )\n",
    "        self.h.value = h_new\n",
    "        return h_new\n",
    "    \n",
    "    def reset(self):\n",
    "        self.h.value = jnp.zeros(self.hidden_size)\n",
    "\n",
    "# Create RNN cell\n",
    "rnn_cell = RNNCell(input_size=3, hidden_size=5)\n",
    "\n",
    "# Process a sequence\n",
    "sequence = bst.random.randn(10, 3)  # 10 time steps, 3 features\n",
    "\n",
    "rnn_cell.reset()\n",
    "outputs = []\n",
    "for t in range(len(sequence)):\n",
    "    output = rnn_cell(sequence[t])\n",
    "    outputs.append(output)\n",
    "\n",
    "outputs = jnp.stack(outputs)\n",
    "print(f\"Sequence length: {len(sequence)}\")\n",
    "print(f\"Outputs shape: {outputs.shape}\")\n",
    "print(f\"Hidden state evolves over time, maintained in rnn_cell.h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Parallel Map (pmap) for Multi-Device Execution\n",
    "\n",
    "`pmap` distributes computation across multiple devices (GPUs/TPUs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available devices\n",
    "devices = jax.devices()\n",
    "print(f\"Available devices: {len(devices)}\")\n",
    "for i, device in enumerate(devices):\n",
    "    print(f\"  Device {i}: {device}\")\n",
    "\n",
    "# Example: parallel computation across devices\n",
    "def expensive_computation(x):\n",
    "    \"\"\"Some expensive operation.\"\"\"\n",
    "    # Matrix multiplication chain\n",
    "    result = x\n",
    "    for _ in range(10):\n",
    "        result = result @ result.T @ result\n",
    "    return jnp.mean(result)\n",
    "\n",
    "# Create data with first dimension = number of devices\n",
    "n_devices = len(devices)\n",
    "data = bst.random.randn(n_devices, 50, 50)\n",
    "\n",
    "if n_devices > 1:\n",
    "    # Parallelize across devices\n",
    "    pmap_fn = jax.pmap(expensive_computation)\n",
    "    results_parallel = pmap_fn(data)\n",
    "    print(f\"\\nParallel results shape: {results_parallel.shape}\")\n",
    "    print(f\"Results computed on {n_devices} devices simultaneously\")\n",
    "else:\n",
    "    print(\"\\nOnly 1 device available. pmap would work like vmap.\")\n",
    "    print(\"On multi-GPU/TPU systems, pmap provides true parallelism.\")\n",
    "\n",
    "# vmap for comparison (sequential)\n",
    "vmap_fn = bst.transform.vmap(expensive_computation)\n",
    "results_sequential = vmap_fn(data)\n",
    "print(f\"\\nSequential (vmap) also works: {results_sequential.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Practical Example: Ensemble Predictions\n",
    "\n",
    "Use `vmap` to efficiently run multiple models in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ensemble of models with different initializations\n",
    "class SimpleClassifier(bst.graph.Node):\n",
    "    def __init__(self, seed):\n",
    "        super().__init__()\n",
    "        bst.random.seed(seed)\n",
    "        self.layer1 = bst.nn.Linear(2, 4)\n",
    "        self.layer2 = bst.nn.Linear(4, 1)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = jnp.tanh(self.layer1(x))\n",
    "        x = self.layer2(x)\n",
    "        return jnp.squeeze(x)\n",
    "\n",
    "# Create ensemble\n",
    "ensemble_size = 5\n",
    "models = [SimpleClassifier(seed=i) for i in range(ensemble_size)]\n",
    "\n",
    "# Test data\n",
    "test_x = bst.random.randn(100, 2)\n",
    "\n",
    "# Get predictions from all models\n",
    "predictions = jnp.array([model(test_x) for model in models])\n",
    "\n",
    "# Ensemble prediction: average\n",
    "ensemble_pred = jnp.mean(predictions, axis=0)\n",
    "ensemble_std = jnp.std(predictions, axis=0)\n",
    "\n",
    "print(f\"Predictions shape: {predictions.shape}\")  # (5, 100)\n",
    "print(f\"Ensemble prediction shape: {ensemble_pred.shape}\")  # (100,)\n",
    "\n",
    "# Visualize uncertainty\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Individual model predictions\n",
    "for i in range(ensemble_size):\n",
    "    ax1.scatter(test_x[:, 0], predictions[i], alpha=0.3, s=10, label=f'Model {i+1}')\n",
    "ax1.set_xlabel('Feature 1')\n",
    "ax1.set_ylabel('Prediction')\n",
    "ax1.set_title('Individual Model Predictions')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Ensemble prediction with uncertainty\n",
    "ax2.scatter(test_x[:, 0], ensemble_pred, c=ensemble_std, cmap='viridis', s=20)\n",
    "ax2.set_xlabel('Feature 1')\n",
    "ax2.set_ylabel('Ensemble Prediction')\n",
    "ax2.set_title('Ensemble Prediction (color = uncertainty)')\n",
    "plt.colorbar(ax2.collections[0], ax=ax2, label='Std Dev')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced: Monte Carlo Dropout with vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network with dropout\n",
    "class BayesianNet(bst.graph.Node):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = bst.nn.Linear(5, 10)\n",
    "        self.dropout1 = bst.nn.Dropout(0.3)\n",
    "        self.layer2 = bst.nn.Linear(10, 1)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = jnp.tanh(self.layer1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "model = BayesianNet()\n",
    "test_input = bst.random.randn(20, 5)\n",
    "\n",
    "# Multiple forward passes with dropout enabled (Monte Carlo sampling)\n",
    "n_samples = 50\n",
    "mc_predictions = []\n",
    "\n",
    "with bst.environ.context(fit=True):  # Keep dropout enabled\n",
    "    for _ in range(n_samples):\n",
    "        pred = model(test_input)\n",
    "        mc_predictions.append(pred)\n",
    "\n",
    "mc_predictions = jnp.array(mc_predictions)  # Shape: (50, 20, 1)\n",
    "\n",
    "# Compute mean and uncertainty\n",
    "mc_mean = jnp.mean(mc_predictions, axis=0)\n",
    "mc_std = jnp.std(mc_predictions, axis=0)\n",
    "\n",
    "print(f\"MC predictions shape: {mc_predictions.shape}\")\n",
    "print(f\"Mean prediction shape: {mc_mean.shape}\")\n",
    "print(f\"Uncertainty (std) range: [{mc_std.min():.3f}, {mc_std.max():.3f}]\")\n",
    "print(\"\\nMonte Carlo dropout provides Bayesian uncertainty estimates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Performance Tips and Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tip 1: Combine vmap with jit for best performance\n",
    "def slow_function(x):\n",
    "    return jnp.sum(jnp.sin(x) ** 2 + jnp.cos(x) ** 2)\n",
    "\n",
    "data = bst.random.randn(1000, 100)\n",
    "\n",
    "# Version 1: Just vmap\n",
    "vmap_only = bst.transform.vmap(slow_function)\n",
    "_ = vmap_only(data)  # Warmup\n",
    "start = time.time()\n",
    "_ = vmap_only(data)\n",
    "time_vmap = time.time() - start\n",
    "\n",
    "# Version 2: vmap + jit\n",
    "vmap_jit = jax.jit(bst.transform.vmap(slow_function))\n",
    "_ = vmap_jit(data)  # Warmup (compilation)\n",
    "start = time.time()\n",
    "_ = vmap_jit(data)\n",
    "time_vmap_jit = time.time() - start\n",
    "\n",
    "print(\"Performance comparison:\")\n",
    "print(f\"vmap only:     {time_vmap*1000:.2f} ms\")\n",
    "print(f\"vmap + jit:    {time_vmap_jit*1000:.2f} ms\")\n",
    "print(f\"Speedup:       {time_vmap/time_vmap_jit:.1f}x\")\n",
    "print(\"\\nAlways combine vmap with jit for production code!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tip 2: Choose the Right Batch Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch-first (standard): (batch, features)\n",
    "batch_first = bst.random.randn(1000, 128)\n",
    "\n",
    "# Feature-first: (features, batch)\n",
    "feature_first = bst.random.randn(128, 1000)\n",
    "\n",
    "def process_sample(x):\n",
    "    return jnp.mean(x ** 2)\n",
    "\n",
    "# For batch-first (standard)\n",
    "vmap_batch_first = jax.jit(bst.transform.vmap(process_sample, in_axes=0))\n",
    "\n",
    "# For feature-first\n",
    "vmap_feature_first = jax.jit(bst.transform.vmap(process_sample, in_axes=1))\n",
    "\n",
    "# Both work, but batch-first is more common\n",
    "_ = vmap_batch_first(batch_first)\n",
    "_ = vmap_feature_first(feature_first)\n",
    "\n",
    "print(\"Recommendation: Use batch-first layout (batch, ...) as the standard\")\n",
    "print(\"  - More intuitive\")\n",
    "print(\"  - Matches PyTorch/TensorFlow conventions\")\n",
    "print(\"  - Better for distributed training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we covered:\n",
    "\n",
    "1. **vmap Basics**: Automatic vectorization of functions\n",
    "2. **Axis Control**: Using `in_axes` and `out_axes` for flexible mapping\n",
    "3. **Nested vmap**: Handling multi-dimensional operations\n",
    "4. **Neural Networks**: Batched operations with BrainState modules\n",
    "5. **Per-Example Gradients**: Fine-grained gradient computation\n",
    "6. **StatefulMapping**: Maintaining state across mapped operations\n",
    "7. **pmap**: Multi-device parallelism\n",
    "8. **Ensemble Methods**: Efficient multi-model predictions\n",
    "9. **Monte Carlo Dropout**: Uncertainty estimation\n",
    "10. **Performance Tips**: Combining vmap with JIT\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

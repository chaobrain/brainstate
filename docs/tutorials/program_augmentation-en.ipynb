{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Program Augmentation\n",
    "\n",
    "\n",
    "The `BrainState` framework provides a powerful program functionality augmentation mechanism based on the [`pygraph` syntax](./pygraph-en.ipynb), allowing users to add additional features to the basic computational model, such as automatic differentiation, batching, and multi-device parallelism. This tutorial will detail how to utilize these enhancement features to optimize and extend your model. It is recommended to read the [`pygraph` syntax](./pygraph-en.ipynb) tutorial prior to this one.\n",
    "\n",
    "## 1. Automatic Differentiation\n",
    "\n",
    "Automatic differentiation is one of the most fundamental and important features in deep learning. The `BrainState` framework is built upon JAX’s automatic differentiation system, providing a [simple and intuitive API](../apis/augment.rst) for gradient computation.\n",
    "\n",
    "### 1.1 Automatic Differentiation Syntax\n",
    "\n",
    "The automatic differentiation interface provided by `brainstate` requires the user to specify the `State` collection for which gradients are needed. The basic syntax is as follows:\n",
    "\n",
    "```python\n",
    "gradients = brainstate.augment.grad(loss_fn, states)\n",
    "```\n",
    "\n",
    "Here, `loss_fn` represents the loss function, and `states` is the collection of parameters for which gradients are to be computed. The `grad` function returns a new function that accepts the same inputs as `loss_fn`, but the return value consists of the gradients of each parameter in `states`. The `grad` function is designed for scalar loss functions, but it can be replaced with other forms of differentiation functions for different types of loss functions. Currently, the supported automatic differentiation interfaces include:\n",
    "\n",
    "- `brainstate.augment.grad`: Automatic differentiation for scalar loss functions using reverse-mode automatic differentiation.\n",
    "- `brainstate.augment.vector_grad`: Automatic differentiation for vector loss functions using reverse-mode automatic differentiation.\n",
    "- `brainstate.augment.jacrev`: Jacobian matrix for scalar functions using reverse-mode automatic differentiation.\n",
    "- `brainstate.augment.jacfwd`: Jacobian matrix for scalar functions using forward-mode automatic differentiation.\n",
    "- `brainstate.augment.jacobian`: Jacobian matrix for scalar functions, equivalent to `brainstate.augment.jacrev`.\n",
    "- `brainstate.augment.hessian`: Hessian matrix for scalar functions using reverse-mode automatic differentiation.\n",
    "- For more detailed information, please refer to the [API documentation](../apis/augment.rst).\n",
    "\n",
    "The automatic differentiation interfaces provided by `brainstate` support returning the loss function value (`return_value=True`) and also support returning auxiliary data (`has_aux=True`).\n",
    "\n",
    "When `return_value=True`, the return value is a tuple where the first element is the gradient and the second element is the loss function value.\n",
    "\n",
    "```python\n",
    "gradients, loss = brainstate.augment.grad(loss_fn, states)\n",
    "```\n",
    "\n",
    "When `has_aux=True`, the return value is a tuple where the first element is the gradient and the second element is auxiliary data. In this case, `loss_fn` must return a tuple where the first element is the loss function value and the second element is the auxiliary data.\n",
    "\n",
    "```python\n",
    "def loss_fn(*args):\n",
    "    ...\n",
    "    return loss, aux\n",
    "\n",
    "gradients, aux = brainstate.augment.grad(loss_fn, states, has_aux=True)\n",
    "```\n",
    "\n",
    "When both `return_value=True` and `has_aux=True` are set to true, the return value is a tuple where the first element is the gradient, the second element is the loss function value, and the third element is the auxiliary data.\n",
    "\n",
    "```python\n",
    "def loss_fn(*args):\n",
    "    ...\n",
    "    return loss, aux\n",
    "\n",
    "gradients, loss, aux = brainstate.augment.grad(loss_fn, states, return_value=True, has_aux=True)\n",
    "```\n",
    "\n",
    "### 1.2 Basic Gradient Calculation\n",
    "\n",
    "The functions provided by `brainstate`, such as `grad` and `vector_grad`, support first-order gradient calculations. Below is a simple example:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e7d6ecf633fe575"
  },
  {
   "cell_type": "code",
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "import brainunit as u\n",
    "import brainstate as bst"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-20T08:09:34.111641Z",
     "start_time": "2025-01-20T08:09:33.112399Z"
    }
   },
   "id": "aaa076f8993f9a0b",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "# Create a simple linear layer model\n",
    "model = bst.nn.Linear(2, 3)\n",
    "\n",
    "# Prepare input data\n",
    "x = jnp.ones((1, 2))\n",
    "y = jnp.ones((1, 3))\n",
    "\n",
    "# Define the loss function\n",
    "def loss_fn(x, y):\n",
    "    return jnp.mean((y - model(x)) ** 2)\n",
    "\n",
    "# Retrieve model parameters\n",
    "weights = model.states()\n",
    "\n",
    "# Compute gradients\n",
    "grads = bst.augment.grad(loss_fn, weights)(x, y)\n",
    "\n",
    "# Print gradient information\n",
    "print(\"Gradients:\", grads)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-20T08:09:34.824836Z",
     "start_time": "2025-01-20T08:09:34.116875Z"
    }
   },
   "id": "64185a15f0412aaf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients: {('weight',): {'bias': Array([ 0.5462675, -1.7040147, -1.0706702], dtype=float32), 'weight': Array([[ 0.5462675, -1.7040147, -1.0706702],\n",
      "       [ 0.5462675, -1.7040147, -1.0706702]], dtype=float32)}}\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this example, a simple linear layer model is created using the `bst.nn.Linear` class, which takes two input features and produces three output features. Input data `x` and target output `y` are prepared as arrays of ones with shapes corresponding to the model's input and output dimensions.\n",
    "\n",
    "The loss function, defined as the mean squared error between the model's predictions and the target outputs, computes the average of the squared differences.\n",
    "\n",
    "The model parameters (weights) are retrieved using the `model.states()` method. Gradients of the loss function with respect to the model parameters are calculated using the automatic differentiation feature provided by `bst.augment.grad`. The computed gradients are then printed to the console."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0585e87f8b5d22f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3 Higher-Order Gradient Computation\n",
    "\n",
    "The `BrainState` framework supports the computation of higher-order derivatives, which can be very useful in certain optimization tasks:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c25f810ec7c47fc"
  },
  {
   "cell_type": "code",
   "source": [
    "# Compute the Hessian\n",
    "hessian = bst.augment.hessian(loss_fn, weights)(x, y)\n",
    "\n",
    "# Compute the Jacobian matrix\n",
    "jacobian = bst.augment.jacobian(loss_fn, weights)(x, y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-20T08:09:35.663818Z",
     "start_time": "2025-01-20T08:09:34.888966Z"
    }
   },
   "id": "2a78f47cc02fc1e5",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this example, the framework allows for the calculation of second-order derivatives by using the `bst.augment.hessian` function, which computes the Hessian matrix of the loss function with respect to the model parameters. Additionally, the Jacobian matrix can be calculated using the `bst.augment.jacobian` function, providing insight into how changes in model parameters affect the output of the loss function. Both of these higher-order derivative computations are essential for advanced optimization techniques and can enhance the model's performance in complex tasks."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49963314fbd318fe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4 Gradient Transformation and the Chain Rule\n",
    "\n",
    "You can combine multiple gradient computation operations:\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbaddde7cb2feb6c"
  },
  {
   "cell_type": "code",
   "source": [
    "# Combine multiple gradient operations\n",
    "def composite_grad(fn, params):\n",
    "    grad_fn = bst.augment.grad(fn, params)\n",
    "    return lambda *args: bst.augment.grad(grad_fn, params)(*args)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-20T08:09:35.676657Z",
     "start_time": "2025-01-20T08:09:35.671832Z"
    }
   },
   "id": "89f1f8533e5e63",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this example, a function `composite_grad` is defined to facilitate the combination of multiple gradient operations. The function first computes the gradient of a given function `fn` with respect to the specified parameters using `bst.augment.grad`. It then returns a new lambda function that, when called with arguments, applies the chain rule by computing the gradient of the previously obtained gradient function. This approach allows for the creation of more complex gradient computations, enabling users to apply higher-level operations efficiently while adhering to the principles of automatic differentiation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b1a9c596f78c93d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Batching Augmentation\n",
    "\n",
    "Batching is a key technique in deep learning that enhances computational efficiency. By processing multiple samples simultaneously, it improves hardware utilization and reduces computational overhead. The `brainstate` framework supports batching for [``pygraph`` models](./pygraph-en.ipynb), allowing users to implement batching through a simple API.\n",
    "\n",
    "Compared to `jax.vmap`, `brainstate.augment.vmap` introduces two additional parameters, ``in_states`` and ``out_states``, which specify which ``State`` objects in the model should undergo batching. Both ``in_states`` and ``out_states`` are dictionaries, where the keys represent the batching dimensions, and the values are the ``State`` objects to be batched (which can be any combination of ``State`` objects in a PyTree).\n",
    "\n",
    "Similar to `jax.vmap`, the `brainstate.augment.vmap` function also accepts the `in_axes` and `out_axes` parameters, which specify which dimensions of non-``State`` parameters should be batched. The usage of `in_axes` and `out_axes` is the same as in `jax.vmap`.\n",
    "\n",
    "Below are some simple examples:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "291d3cc1f776cc6f"
  },
  {
   "cell_type": "code",
   "source": [
    "batch_size = 4\n",
    "\n",
    "# 创建批处理数据和模型\n",
    "x_batch = jnp.ones((batch_size, 3)) * u.mA\n",
    "model = bst.nn.LIF(3)\n",
    "model.init_state(batch_size)\n",
    "\n",
    "\n",
    "# 对每一个批次计算损失\n",
    "@bst.augment.vmap(\n",
    "    in_states=model.states()  # 所有State都进行批处理\n",
    ")\n",
    "def eval(x):\n",
    "    with bst.environ.context(dt=0.1 * u.ms):\n",
    "        return model(x)\n",
    "\n",
    "eval(x_batch)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-20T08:09:36.269114Z",
     "start_time": "2025-01-20T08:09:35.685333Z"
    }
   },
   "id": "38ed91410c8742b7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": "``brainstate.augment.vmap`` automatically detects any ``State`` objects that do not have the correct batching dimensions set and issues a warning.",
   "metadata": {
    "collapsed": false
   },
   "id": "7c0e5a3b7e7b587b"
  },
  {
   "cell_type": "code",
   "source": [
    "class Foo(bst.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.a = bst.ParamState(jnp.arange(4))\n",
    "        self.b = bst.ShortTermState(jnp.arange(4))\n",
    "\n",
    "    def __call__(self):\n",
    "        self.b.value = self.a.value * self.b.value\n",
    "\n",
    "\n",
    "foo = Foo()\n",
    "r = bst.augment.vmap(foo, in_states=foo.a)()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-20T08:09:38.951019Z",
     "start_time": "2025-01-20T08:09:36.496680Z"
    }
   },
   "id": "2d12cfc3169d68dc",
   "outputs": [
    {
     "ename": "BatchAxisError",
     "evalue": "The value of State ShortTermState(\n  value=Traced<ShapedArray(int32[4])>with<BatchTrace> with\n    val = Array([[0, 0, 0, 0],\n         [0, 1, 2, 3],\n         [0, 2, 4, 6],\n         [0, 3, 6, 9]], dtype=int32)\n    batch_dim = 0\n) is batched, but it is not in the out_states.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mJaxStackTraceBeforeTransformation\u001B[0m         Traceback (most recent call last)",
      "File \u001B[1;32m~\\miniconda3\\envs\\brainpy-dev\\Lib\\runpy.py:198\u001B[0m, in \u001B[0;36m_run_module_as_main\u001B[1;34m()\u001B[0m\n\u001B[0;32m    197\u001B[0m     sys\u001B[38;5;241m.\u001B[39margv[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m=\u001B[39m mod_spec\u001B[38;5;241m.\u001B[39morigin\n\u001B[1;32m--> 198\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _run_code(code, main_globals, \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    199\u001B[0m                  \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m, mod_spec)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\brainpy-dev\\Lib\\runpy.py:88\u001B[0m, in \u001B[0;36m_run_code\u001B[1;34m()\u001B[0m\n\u001B[0;32m     81\u001B[0m run_globals\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m=\u001B[39m mod_name,\n\u001B[0;32m     82\u001B[0m                    \u001B[38;5;18m__file__\u001B[39m \u001B[38;5;241m=\u001B[39m fname,\n\u001B[0;32m     83\u001B[0m                    __cached__ \u001B[38;5;241m=\u001B[39m cached,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     86\u001B[0m                    __package__ \u001B[38;5;241m=\u001B[39m pkg_name,\n\u001B[0;32m     87\u001B[0m                    __spec__ \u001B[38;5;241m=\u001B[39m mod_spec)\n\u001B[1;32m---> 88\u001B[0m exec(code, run_globals)\n\u001B[0;32m     89\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m run_globals\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\brainpy-dev\\Lib\\site-packages\\ipykernel_launcher.py:18\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mipykernel\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m kernelapp \u001B[38;5;28;01mas\u001B[39;00m app\n\u001B[1;32m---> 18\u001B[0m app\u001B[38;5;241m.\u001B[39mlaunch_new_instance()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\brainpy-dev\\Lib\\site-packages\\traitlets\\config\\application.py:1075\u001B[0m, in \u001B[0;36mlaunch_instance\u001B[1;34m()\u001B[0m\n\u001B[0;32m   1074\u001B[0m app\u001B[38;5;241m.\u001B[39minitialize(argv)\n\u001B[1;32m-> 1075\u001B[0m app\u001B[38;5;241m.\u001B[39mstart()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\brainpy-dev\\Lib\\site-packages\\ipykernel\\kernelapp.py:739\u001B[0m, in \u001B[0;36mstart\u001B[1;34m()\u001B[0m\n\u001B[0;32m    738\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 739\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mio_loop\u001B[38;5;241m.\u001B[39mstart()\n\u001B[0;32m    740\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\brainpy-dev\\Lib\\site-packages\\tornado\\platform\\asyncio.py:195\u001B[0m, in \u001B[0;36mstart\u001B[1;34m()\u001B[0m\n\u001B[0;32m    194\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstart\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 195\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39masyncio_loop\u001B[38;5;241m.\u001B[39mrun_forever()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\brainpy-dev\\Lib\\asyncio\\base_events.py:607\u001B[0m, in \u001B[0;36mrun_forever\u001B[1;34m()\u001B[0m\n\u001B[0;32m    606\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m--> 607\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_once()\n\u001B[0;32m    608\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stopping:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\brainpy-dev\\Lib\\asyncio\\base_events.py:1922\u001B[0m, in \u001B[0;36m_run_once\u001B[1;34m()\u001B[0m\n\u001B[0;32m   1921\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1922\u001B[0m         handle\u001B[38;5;241m.\u001B[39m_run()\n\u001B[0;32m   1923\u001B[0m handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\brainpy-dev\\Lib\\asyncio\\events.py:80\u001B[0m, in \u001B[0;36m_run\u001B[1;34m()\u001B[0m\n\u001B[0;32m     79\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 80\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_context\u001B[38;5;241m.\u001B[39mrun(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_callback, \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_args)\n\u001B[0;32m     81\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mSystemExit\u001B[39;00m, \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m):\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\brainpy-dev\\Lib\\site-packages\\ipykernel\\kernelbase.py:545\u001B[0m, in \u001B[0;36mdispatch_queue\u001B[1;34m()\u001B[0m\n\u001B[0;32m    544\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 545\u001B[0m     \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_one()\n\u001B[0;32m    546\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\brainpy-dev\\Lib\\site-packages\\ipykernel\\kernelbase.py:534\u001B[0m, in \u001B[0;36mprocess_one\u001B[1;34m()\u001B[0m\n\u001B[0;32m    533\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m--> 534\u001B[0m \u001B[38;5;28;01mawait\u001B[39;00m dispatch(\u001B[38;5;241m*\u001B[39margs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\brainpy-dev\\Lib\\site-packages\\ipykernel\\kernelbase.py:437\u001B[0m, in \u001B[0;36mdispatch_shell\u001B[1;34m()\u001B[0m\n\u001B[0;32m    436\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39misawaitable(result):\n\u001B[1;32m--> 437\u001B[0m         \u001B[38;5;28;01mawait\u001B[39;00m result\n\u001B[0;32m    438\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\brainpy-dev\\Lib\\site-packages\\ipykernel\\ipkernel.py:362\u001B[0m, in \u001B[0;36mexecute_request\u001B[1;34m()\u001B[0m\n\u001B[0;32m    361\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_associate_new_top_level_threads_with(parent_header)\n\u001B[1;32m--> 362\u001B[0m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mexecute_request(stream, ident, parent)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\brainpy-dev\\Lib\\site-packages\\ipykernel\\kernelbase.py:778\u001B[0m, in \u001B[0;36mexecute_request\u001B[1;34m()\u001B[0m\n\u001B[0;32m    777\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39misawaitable(reply_content):\n\u001B[1;32m--> 778\u001B[0m     reply_content \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m reply_content\n\u001B[0;32m    780\u001B[0m \u001B[38;5;66;03m# Flush output before sending the reply.\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\brainpy-dev\\Lib\\site-packages\\ipykernel\\ipkernel.py:449\u001B[0m, in \u001B[0;36mdo_execute\u001B[1;34m()\u001B[0m\n\u001B[0;32m    448\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m accepts_params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcell_id\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[1;32m--> 449\u001B[0m     res \u001B[38;5;241m=\u001B[39m shell\u001B[38;5;241m.\u001B[39mrun_cell(\n\u001B[0;32m    450\u001B[0m         code,\n\u001B[0;32m    451\u001B[0m         store_history\u001B[38;5;241m=\u001B[39mstore_history,\n\u001B[0;32m    452\u001B[0m         silent\u001B[38;5;241m=\u001B[39msilent,\n\u001B[0;32m    453\u001B[0m         cell_id\u001B[38;5;241m=\u001B[39mcell_id,\n\u001B[0;32m    454\u001B[0m     )\n\u001B[0;32m    455\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\brainpy-dev\\Lib\\site-packages\\ipykernel\\zmqshell.py:549\u001B[0m, in \u001B[0;36mrun_cell\u001B[1;34m()\u001B[0m\n\u001B[0;32m    548\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_last_traceback \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 549\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrun_cell(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\brainpy-dev\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3075\u001B[0m, in \u001B[0;36mrun_cell\u001B[1;34m()\u001B[0m\n\u001B[0;32m   3074\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3075\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_cell(\n\u001B[0;32m   3076\u001B[0m         raw_cell, store_history, silent, shell_futures, cell_id\n\u001B[0;32m   3077\u001B[0m     )\n\u001B[0;32m   3078\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\brainpy-dev\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3130\u001B[0m, in \u001B[0;36m_run_cell\u001B[1;34m()\u001B[0m\n\u001B[0;32m   3129\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3130\u001B[0m     result \u001B[38;5;241m=\u001B[39m runner(coro)\n\u001B[0;32m   3131\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\brainpy-dev\\Lib\\site-packages\\IPython\\core\\async_helpers.py:128\u001B[0m, in \u001B[0;36m_pseudo_sync_runner\u001B[1;34m()\u001B[0m\n\u001B[0;32m    127\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 128\u001B[0m     coro\u001B[38;5;241m.\u001B[39msend(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    129\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\brainpy-dev\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3334\u001B[0m, in \u001B[0;36mrun_cell_async\u001B[1;34m()\u001B[0m\n\u001B[0;32m   3331\u001B[0m interactivity \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnone\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m silent \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mast_node_interactivity\n\u001B[1;32m-> 3334\u001B[0m has_raised \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrun_ast_nodes(code_ast\u001B[38;5;241m.\u001B[39mbody, cell_name,\n\u001B[0;32m   3335\u001B[0m        interactivity\u001B[38;5;241m=\u001B[39minteractivity, compiler\u001B[38;5;241m=\u001B[39mcompiler, result\u001B[38;5;241m=\u001B[39mresult)\n\u001B[0;32m   3337\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlast_execution_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m has_raised\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\brainpy-dev\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3517\u001B[0m, in \u001B[0;36mrun_ast_nodes\u001B[1;34m()\u001B[0m\n\u001B[0;32m   3516\u001B[0m     asy \u001B[38;5;241m=\u001B[39m compare(code)\n\u001B[1;32m-> 3517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrun_code(code, result, async_\u001B[38;5;241m=\u001B[39masy):\n\u001B[0;32m   3518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\brainpy-dev\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3577\u001B[0m, in \u001B[0;36mrun_code\u001B[1;34m()\u001B[0m\n\u001B[0;32m   3576\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 3577\u001B[0m         exec(code_obj, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muser_global_ns, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muser_ns)\n\u001B[0;32m   3578\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m   3579\u001B[0m     \u001B[38;5;66;03m# Reset our crash handler in place\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[6], line 11\u001B[0m\n\u001B[0;32m      8\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mb\u001B[38;5;241m.\u001B[39mvalue \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39ma\u001B[38;5;241m.\u001B[39mvalue \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mb\u001B[38;5;241m.\u001B[39mvalue\n\u001B[1;32m---> 11\u001B[0m foo \u001B[38;5;241m=\u001B[39m Foo()\n\u001B[0;32m     12\u001B[0m r \u001B[38;5;241m=\u001B[39m bst\u001B[38;5;241m.\u001B[39maugment\u001B[38;5;241m.\u001B[39mvmap(foo, in_states\u001B[38;5;241m=\u001B[39mfoo\u001B[38;5;241m.\u001B[39ma)()\n",
      "File \u001B[1;32mD:\\codes\\projects\\brainstate\\brainstate\\graph\\_graph_node.py:45\u001B[0m, in \u001B[0;36m__call__\u001B[1;34m()\u001B[0m\n\u001B[0;32m     44\u001B[0m node \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__new__\u001B[39m(\u001B[38;5;28mcls\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m---> 45\u001B[0m node\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m node\n",
      "Cell \u001B[1;32mIn[6], line 5\u001B[0m, in \u001B[0;36m__init__\u001B[1;34m()\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39ma \u001B[38;5;241m=\u001B[39m bst\u001B[38;5;241m.\u001B[39mParamState(jnp\u001B[38;5;241m.\u001B[39marange(\u001B[38;5;241m4\u001B[39m))\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mb \u001B[38;5;241m=\u001B[39m bst\u001B[38;5;241m.\u001B[39mShortTermState(jnp\u001B[38;5;241m.\u001B[39marange(\u001B[38;5;241m4\u001B[39m))\n",
      "File \u001B[1;32mD:\\codes\\projects\\brainstate\\brainstate\\_state.py:234\u001B[0m, in \u001B[0;36m__init__\u001B[1;34m()\u001B[0m\n\u001B[0;32m    231\u001B[0m \u001B[38;5;66;03m# update metadata\u001B[39;00m\n\u001B[0;32m    232\u001B[0m metadata\u001B[38;5;241m.\u001B[39mupdate(_value\u001B[38;5;241m=\u001B[39mvalue,\n\u001B[0;32m    233\u001B[0m                 _level\u001B[38;5;241m=\u001B[39m_get_trace_stack_level(),\n\u001B[1;32m--> 234\u001B[0m                 _source_info\u001B[38;5;241m=\u001B[39msource_info_util\u001B[38;5;241m.\u001B[39mcurrent(),\n\u001B[0;32m    235\u001B[0m                 _name\u001B[38;5;241m=\u001B[39mname,\n\u001B[0;32m    236\u001B[0m                 tag\u001B[38;5;241m=\u001B[39mtag,\n\u001B[0;32m    237\u001B[0m                 _been_writen\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m    239\u001B[0m \u001B[38;5;66;03m# avoid using self._setattr to avoid the check\u001B[39;00m\n",
      "\u001B[1;31mJaxStackTraceBeforeTransformation\u001B[0m: brainstate.augment._mapping.BatchAxisError: The value of State ShortTermState(\n  value=Traced<ShapedArray(int32[4])>with<BatchTrace> with\n    val = Array([[0, 0, 0, 0],\n         [0, 1, 2, 3],\n         [0, 2, 4, 6],\n         [0, 3, 6, 9]], dtype=int32)\n    batch_dim = 0\n) is batched, but it is not in the out_states.\n\nThe preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.\n\n--------------------",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mBatchAxisError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 12\u001B[0m\n\u001B[0;32m      8\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mb\u001B[38;5;241m.\u001B[39mvalue \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39ma\u001B[38;5;241m.\u001B[39mvalue \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mb\u001B[38;5;241m.\u001B[39mvalue\n\u001B[0;32m     11\u001B[0m foo \u001B[38;5;241m=\u001B[39m Foo()\n\u001B[1;32m---> 12\u001B[0m r \u001B[38;5;241m=\u001B[39m bst\u001B[38;5;241m.\u001B[39maugment\u001B[38;5;241m.\u001B[39mvmap(foo, in_states\u001B[38;5;241m=\u001B[39mfoo\u001B[38;5;241m.\u001B[39ma)()\n",
      "File \u001B[1;32mD:\\codes\\projects\\brainstate\\brainstate\\augment\\_mapping.py:147\u001B[0m, in \u001B[0;36m_vmap_transform.<locals>.vmapped_fn\u001B[1;34m(*args)\u001B[0m\n\u001B[0;32m    145\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out_axes_st) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    146\u001B[0m     out_axes_st \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m--> 147\u001B[0m out_state_vals, outs \u001B[38;5;241m=\u001B[39m restore_rngs(\n\u001B[0;32m    148\u001B[0m     jax\u001B[38;5;241m.\u001B[39mvmap(\n\u001B[0;32m    149\u001B[0m         new_fn,\n\u001B[0;32m    150\u001B[0m         in_axes\u001B[38;5;241m=\u001B[39m(in_axes_st, in_axes),\n\u001B[0;32m    151\u001B[0m         out_axes\u001B[38;5;241m=\u001B[39m(out_axes_st, out_axes),\n\u001B[0;32m    152\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mtransform_kwargs\n\u001B[0;32m    153\u001B[0m     ),\n\u001B[0;32m    154\u001B[0m     rngs\u001B[38;5;241m=\u001B[39mrngs\n\u001B[0;32m    155\u001B[0m )(in_state_vals, args)\n\u001B[0;32m    157\u001B[0m \u001B[38;5;66;03m# restore mapped state values\u001B[39;00m\n\u001B[0;32m    158\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, states \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(axis_to_out_states\u001B[38;5;241m.\u001B[39mvalues()):\n",
      "File \u001B[1;32mD:\\codes\\projects\\brainstate\\brainstate\\augment\\_random.py:64\u001B[0m, in \u001B[0;36m_rng_backup.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     62\u001B[0m rng_restorer\u001B[38;5;241m.\u001B[39mbackup()\n\u001B[0;32m     63\u001B[0m \u001B[38;5;66;03m# call the function\u001B[39;00m\n\u001B[1;32m---> 64\u001B[0m out \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     65\u001B[0m \u001B[38;5;66;03m# restore the random state\u001B[39;00m\n\u001B[0;32m     66\u001B[0m rng_restorer\u001B[38;5;241m.\u001B[39mrestore()\n",
      "    \u001B[1;31m[... skipping hidden 6 frame]\u001B[0m\n",
      "File \u001B[1;32mD:\\codes\\projects\\brainstate\\brainstate\\augment\\_mapping.py:123\u001B[0m, in \u001B[0;36m_vmap_transform.<locals>.new_fn\u001B[1;34m(in_states_, args)\u001B[0m\n\u001B[0;32m    121\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(state, RandomState) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mid\u001B[39m(state) \u001B[38;5;129;01min\u001B[39;00m rng_ids:\n\u001B[0;32m    122\u001B[0m             \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m--> 123\u001B[0m         state\u001B[38;5;241m.\u001B[39mraise_error_with_source_info(\n\u001B[0;32m    124\u001B[0m             BatchAxisError(\n\u001B[0;32m    125\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe value of State \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstate\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is batched, but it is not in the out_states.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    126\u001B[0m             )\n\u001B[0;32m    127\u001B[0m         )\n\u001B[0;32m    129\u001B[0m out_states_ \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    130\u001B[0m     [state\u001B[38;5;241m.\u001B[39mvalue \u001B[38;5;28;01mfor\u001B[39;00m state \u001B[38;5;129;01min\u001B[39;00m states]\n\u001B[0;32m    131\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m axis, states \u001B[38;5;129;01min\u001B[39;00m axis_to_out_states\u001B[38;5;241m.\u001B[39mitems()\n\u001B[0;32m    132\u001B[0m ]\n\u001B[0;32m    133\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out_states_, outs\n",
      "File \u001B[1;32mD:\\codes\\projects\\brainstate\\brainstate\\_state.py:351\u001B[0m, in \u001B[0;36mState.raise_error_with_source_info\u001B[1;34m(self, error)\u001B[0m\n\u001B[0;32m    349\u001B[0m name_stack \u001B[38;5;241m=\u001B[39m source_info_util\u001B[38;5;241m.\u001B[39mcurrent_name_stack() \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msource_info\u001B[38;5;241m.\u001B[39mname_stack\n\u001B[0;32m    350\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m source_info_util\u001B[38;5;241m.\u001B[39muser_context(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msource_info\u001B[38;5;241m.\u001B[39mtraceback, name_stack\u001B[38;5;241m=\u001B[39mname_stack):\n\u001B[1;32m--> 351\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n",
      "\u001B[1;31mBatchAxisError\u001B[0m: The value of State ShortTermState(\n  value=Traced<ShapedArray(int32[4])>with<BatchTrace> with\n    val = Array([[0, 0, 0, 0],\n         [0, 1, 2, 3],\n         [0, 2, 4, 6],\n         [0, 3, 6, 9]], dtype=int32)\n    batch_dim = 0\n) is batched, but it is not in the out_states."
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In the example above, we define a simple model `Foo`, where only `a` is batched, but `b` is not. `brainstate` will automatically detect this issue and raise an error. The correct approach is to batch `b` as well, for example, by setting it in `out_states`.",
   "id": "de23a7df426b8c67"
  },
  {
   "cell_type": "code",
   "source": [
    "foo = Foo()\n",
    "r = bst.augment.vmap(foo, in_states=foo.a, out_states=foo.b)()\n",
    "\n",
    "foo.b.value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-20T08:10:07.418691Z",
     "start_time": "2025-01-20T08:10:07.405013Z"
    }
   },
   "id": "69ba49a48fdfddb3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0, 0, 0, 0],\n",
       "       [0, 1, 2, 3],\n",
       "       [0, 2, 4, 6],\n",
       "       [0, 3, 6, 9]], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Multi-Device Parallel Computation\n",
    "\n",
    "In addition to batching, `brainstate` also supports multi-device parallel computation. We can use the `brainstate.augment.pmap` function to convert a model into one that supports parallel computation across multiple devices.\n",
    "\n",
    "The usage of the `brainstate.augment.pmap` augmentation function is fundamentally similar to that of the `brainstate.augment.vmap` function. However, the `pmap` function transforms the model to enable parallel computation across multiple devices, while the `vmap` function facilitates parallel computation across different threads on a single device.\n",
    "\n",
    "This capability is particularly valuable in leveraging the computational power of multiple devices, such as GPUs or TPUs, to accelerate training and inference processes in deep learning applications. By utilizing `pmap`, users can effectively distribute their workloads and achieve significant improvements in performance and efficiency."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec355b8d44d302d1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Combining Augmentation Transformations\n",
    "\n",
    "In practical applications, we often need to combine multiple transformations for program augmentation. The various program augmentation functions and compilation functions in `brainstate` can be used together seamlessly. Below is a simple example:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "484374f092cad37a"
  },
  {
   "cell_type": "code",
   "source": [
    "batch_size = 5\n",
    "xs = bst.random.rand(batch_size, 3)\n",
    "ys = bst.random.rand(batch_size, 4)\n",
    "\n",
    "net = bst.nn.Linear(3, 4)\n",
    "\n",
    "@bst.augment.vmap\n",
    "def batch_run(x):\n",
    "    return net(x)\n",
    "\n",
    "\n",
    "def loss_fn(x, y):\n",
    "    return jnp.mean((y - batch_run(x)) ** 2)\n",
    "\n",
    "\n",
    "weights = net.states(bst.ParamState)\n",
    "opt = bst.optim.Adam(1e-3)\n",
    "opt.register_trainable_weights(weights)\n",
    "\n",
    "\n",
    "@bst.compile.jit\n",
    "def batch_train(xs, ys):\n",
    "    grads, l = bst.augment.grad(loss_fn, weights, return_value=True)(xs, ys)\n",
    "    opt.update(grads)\n",
    "    return l\n",
    "\n",
    "\n",
    "l = batch_train(xs, ys)\n",
    "l"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-20T08:10:09.664906Z",
     "start_time": "2025-01-20T08:10:09.144478Z"
    }
   },
   "id": "b8ca72d641c86b9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1.2410938, dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this example, we define a loss function `loss_fn` that calculates the mean squared error between the model's predictions, obtained by invoking `batch_run(net, x)`, and the target values `y`. The model's trainable parameters (weights) are registered with the Adam optimizer.\n",
    "\n",
    "The `batch_train` function is compiled with Just-In-Time (JIT) compilation using `bst.compile.jit`. Within this function, we compute the gradients of the loss function with respect to the model parameters using `bst.augment.grad`, specifying that we want both gradients and the loss value returned.\n",
    "\n",
    "After calculating the gradients, we update the model parameters with the optimizer's `update` method. This approach illustrates how multiple functional enhancements, such as batching, gradient computation, and JIT compilation, can be effectively combined to streamline the training process. The result, `l`, contains the computed loss value, demonstrating the integration of these functionalities in a cohesive workflow."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bef89fbe485c06c9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Performance Optimization Recommendations\n",
    "\n",
    "When utilizing program augmentation transformations, the following points should be considered to achieve optimal performance:\n",
    "\n",
    "1. **Appropriate Batch Size**: Choose a suitable batch size based on device memory and computational capacity.\n",
    "2. **Gradient Accumulation**: When the batch size is limited, consider implementing gradient accumulation.\n",
    "3. **Cached Compilation**: Reuse compiled functions to reduce compilation overhead.\n",
    "4. **Memory Management**: Use `jax.device_get()` to release device memory in a timely manner.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8339501d9ac8610"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Debugging Techniques\n",
    "\n",
    "Debugging is an important topic when utilizing functional enhancements. The `brainstate` framework fully supports the debugging tools provided in JAX, such as the `print` function in `jax.debug`. Below is a simple example:\n",
    "\n",
    "```python\n",
    "# Debugging with jax.debug.print\n",
    "@bst.compile.jit\n",
    "def batch_train(xs, ys):\n",
    "    grads, l = bst.augment.grad(loss_fn, weights, return_value=True)(xs, ys)\n",
    "    jax.debug.print(\"Gradients: {g}\", g=grads)\n",
    "    opt.update(grads)\n",
    "    return l\n",
    "```\n",
    "\n",
    "For detailed usage, users can refer to the [JAX Debugging Documentation](https://jax.readthedocs.io/en/latest/debugging/index.html).\n",
    "\n",
    "\n",
    "In this example, the `batch_train` function utilizes `jax.debug.print` to output the computed gradients during the training process. This can be particularly useful for monitoring the training dynamics and diagnosing issues related to gradient computations. Leveraging debugging tools like this can enhance the development process and facilitate the identification of errors or unexpected behaviors in the model's training workflow."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85114a489d19fa4b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "The program augmentations based on `pygraph` are one of the core features of the `BrainState` framework. By effectively utilizing these augmentations, one can significantly improve the inference, training efficiency, and overall performance of models. This tutorial has covered the primary program augmentation features and their usage methods, providing a foundation for further exploration and application of these capabilities. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9fccc0bafeee258"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

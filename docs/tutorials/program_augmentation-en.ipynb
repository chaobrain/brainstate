{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Program Augmentation\n",
    "\n",
    "\n",
    "The `BrainState` framework provides a powerful program functionality augmentation mechanism based on the [`pygraph` syntax](./pygraph-en.ipynb), allowing users to add additional features to the basic computational model, such as automatic differentiation, batching, and multi-device parallelism. This tutorial will detail how to utilize these enhancement features to optimize and extend your model. It is recommended to read the [`pygraph` syntax](./pygraph-en.ipynb) tutorial prior to this one.\n",
    "\n",
    "## 1. Automatic Differentiation\n",
    "\n",
    "Automatic differentiation is one of the most fundamental and important features in deep learning. The `BrainState` framework is built upon JAXâ€™s automatic differentiation system, providing a [simple and intuitive API](../apis/augment.rst) for gradient computation.\n",
    "\n",
    "### 1.1 Automatic Differentiation Syntax\n",
    "\n",
    "The automatic differentiation interface provided by `brainstate` requires the user to specify the `State` collection for which gradients are needed. The basic syntax is as follows:\n",
    "\n",
    "```python\n",
    "gradients = brainstate.augment.grad(loss_fn, states)\n",
    "```\n",
    "\n",
    "Here, `loss_fn` represents the loss function, and `states` is the collection of parameters for which gradients are to be computed. The `grad` function returns a new function that accepts the same inputs as `loss_fn`, but the return value consists of the gradients of each parameter in `states`. The `grad` function is designed for scalar loss functions, but it can be replaced with other forms of differentiation functions for different types of loss functions. Currently, the supported automatic differentiation interfaces include:\n",
    "\n",
    "- `brainstate.augment.grad`: Automatic differentiation for scalar loss functions using reverse-mode automatic differentiation.\n",
    "- `brainstate.augment.vector_grad`: Automatic differentiation for vector loss functions using reverse-mode automatic differentiation.\n",
    "- `brainstate.augment.jacrev`: Jacobian matrix for scalar functions using reverse-mode automatic differentiation.\n",
    "- `brainstate.augment.jacfwd`: Jacobian matrix for scalar functions using forward-mode automatic differentiation.\n",
    "- `brainstate.augment.jacobian`: Jacobian matrix for scalar functions, equivalent to `brainstate.augment.jacrev`.\n",
    "- `brainstate.augment.hessian`: Hessian matrix for scalar functions using reverse-mode automatic differentiation.\n",
    "- For more detailed information, please refer to the [API documentation](../apis/augment.rst).\n",
    "\n",
    "The automatic differentiation interfaces provided by `brainstate` support returning the loss function value (`return_value=True`) and also support returning auxiliary data (`has_aux=True`).\n",
    "\n",
    "When `return_value=True`, the return value is a tuple where the first element is the gradient and the second element is the loss function value.\n",
    "\n",
    "```python\n",
    "gradients, loss = brainstate.augment.grad(loss_fn, states)\n",
    "```\n",
    "\n",
    "When `has_aux=True`, the return value is a tuple where the first element is the gradient and the second element is auxiliary data. In this case, `loss_fn` must return a tuple where the first element is the loss function value and the second element is the auxiliary data.\n",
    "\n",
    "```python\n",
    "def loss_fn(*args):\n",
    "    ...\n",
    "    return loss, aux\n",
    "\n",
    "gradients, aux = brainstate.augment.grad(loss_fn, states, has_aux=True)\n",
    "```\n",
    "\n",
    "When both `return_value=True` and `has_aux=True` are set to true, the return value is a tuple where the first element is the gradient, the second element is the loss function value, and the third element is the auxiliary data.\n",
    "\n",
    "```python\n",
    "def loss_fn(*args):\n",
    "    ...\n",
    "    return loss, aux\n",
    "\n",
    "gradients, loss, aux = brainstate.augment.grad(loss_fn, states, return_value=True, has_aux=True)\n",
    "```\n",
    "\n",
    "### 1.2 Basic Gradient Calculation\n",
    "\n",
    "The functions provided by `brainstate`, such as `grad` and `vector_grad`, support first-order gradient calculations. Below is a simple example:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e7d6ecf633fe575"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "import brainstate as bst"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-04T07:10:37.322915Z",
     "start_time": "2024-11-04T07:10:36.579993Z"
    }
   },
   "id": "aaa076f8993f9a0b",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients: {('weight',): {'bias': Array([-0.89415467, -1.5608642 , -1.6746773 ], dtype=float32), 'weight': Array([[-0.89415467, -1.5608642 , -1.6746773 ],\n",
      "       [-0.89415467, -1.5608642 , -1.6746773 ]], dtype=float32)}}\n"
     ]
    }
   ],
   "source": [
    "# Create a simple linear layer model\n",
    "model = bst.nn.Linear(2, 3)\n",
    "\n",
    "# Prepare input data\n",
    "x = jnp.ones((1, 2))\n",
    "y = jnp.ones((1, 3))\n",
    "\n",
    "# Define the loss function\n",
    "def loss_fn(x, y):\n",
    "    return jnp.mean((y - model(x)) ** 2)\n",
    "\n",
    "# Retrieve model parameters\n",
    "weights = model.states()\n",
    "\n",
    "# Compute gradients\n",
    "grads = bst.augment.grad(loss_fn, weights)(x, y)\n",
    "\n",
    "# Print gradient information\n",
    "print(\"Gradients:\", grads)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-04T07:10:37.953666Z",
     "start_time": "2024-11-04T07:10:37.323940Z"
    }
   },
   "id": "64185a15f0412aaf",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this example, a simple linear layer model is created using the `bst.nn.Linear` class, which takes two input features and produces three output features. Input data `x` and target output `y` are prepared as arrays of ones with shapes corresponding to the model's input and output dimensions.\n",
    "\n",
    "The loss function, defined as the mean squared error between the model's predictions and the target outputs, computes the average of the squared differences.\n",
    "\n",
    "The model parameters (weights) are retrieved using the `model.states()` method. Gradients of the loss function with respect to the model parameters are calculated using the automatic differentiation feature provided by `bst.augment.grad`. The computed gradients are then printed to the console."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0585e87f8b5d22f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3 Higher-Order Gradient Computation\n",
    "\n",
    "The `BrainState` framework supports the computation of higher-order derivatives, which can be very useful in certain optimization tasks:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c25f810ec7c47fc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Compute the Hessian\n",
    "hessian = bst.augment.hessian(loss_fn, weights)(x, y)\n",
    "\n",
    "# Compute the Jacobian matrix\n",
    "jacobian = bst.augment.jacobian(loss_fn, weights)(x, y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-04T07:10:38.677811Z",
     "start_time": "2024-11-04T07:10:37.954701Z"
    }
   },
   "id": "2a78f47cc02fc1e5",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this example, the framework allows for the calculation of second-order derivatives by using the `bst.augment.hessian` function, which computes the Hessian matrix of the loss function with respect to the model parameters. Additionally, the Jacobian matrix can be calculated using the `bst.augment.jacobian` function, providing insight into how changes in model parameters affect the output of the loss function. Both of these higher-order derivative computations are essential for advanced optimization techniques and can enhance the model's performance in complex tasks."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49963314fbd318fe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4 Gradient Transformation and the Chain Rule\n",
    "\n",
    "You can combine multiple gradient computation operations:\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbaddde7cb2feb6c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Combine multiple gradient operations\n",
    "def composite_grad(fn, params):\n",
    "    grad_fn = bst.augment.grad(fn, params)\n",
    "    return lambda *args: bst.augment.grad(grad_fn, params)(*args)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-04T07:10:38.682140Z",
     "start_time": "2024-11-04T07:10:38.678315Z"
    }
   },
   "id": "89f1f8533e5e63",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this example, a function `composite_grad` is defined to facilitate the combination of multiple gradient operations. The function first computes the gradient of a given function `fn` with respect to the specified parameters using `bst.augment.grad`. It then returns a new lambda function that, when called with arguments, applies the chain rule by computing the gradient of the previously obtained gradient function. This approach allows for the creation of more complex gradient computations, enabling users to apply higher-level operations efficiently while adhering to the principles of automatic differentiation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b1a9c596f78c93d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Batching Augmentation\n",
    "\n",
    "Batching is a key technique in deep learning that enhances computational efficiency. By processing multiple samples simultaneously, it improves hardware utilization and reduces computational overhead. The `brainstate` framework supports batching for [``pygraph`` models](./pygraph-en.ipynb), allowing users to implement batching through a simple API.\n",
    "\n",
    "### 2.1 Creating Batching Models\n",
    "\n",
    "We can transform a model to support batching using the `brainstate.augment.vmap` function. Below is a simple example:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "291d3cc1f776cc6f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Linear(\n  in_size=(2,),\n  out_size=(3,),\n  w_mask=None,\n  weight=ParamState(\n    value={'bias': Array([[0., 0., 0.],\n           [0., 0., 0.],\n           [0., 0., 0.],\n           [0., 0., 0.],\n           [0., 0., 0.]], dtype=float32), 'weight': Array([[[ 0.38410592, -1.3700708 ,  1.0667006 ],\n            [ 1.5859954 , -0.24795905, -1.3676361 ]],\n    \n           [[-1.3518977 , -0.8566778 , -1.5469979 ],\n            [ 0.87259007,  1.465411  ,  0.07184158]],\n    \n           [[ 0.8410974 , -1.5966035 ,  0.4221514 ],\n            [-0.1764095 , -1.3065816 ,  0.64682233]],\n    \n           [[-0.51042235,  1.0864646 ,  0.5021799 ],\n            [-0.337543  , -0.894522  ,  1.131219  ]],\n    \n           [[ 0.5159668 ,  0.62981915,  1.1093888 ],\n            [ 1.084107  , -0.3580393 ,  0.30819336]]], dtype=float32)}\n  )\n)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@bst.augment.vmap\n",
    "def create_linear(key):\n",
    "    bst.random.set_key(key)\n",
    "    return bst.nn.Linear(2, 3)\n",
    "\n",
    "batch_size = 5\n",
    "\n",
    "# Create a batched linear layer model\n",
    "linears = create_linear(bst.random.split_key(batch_size))\n",
    "\n",
    "linears"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-04T07:10:39.013190Z",
     "start_time": "2024-11-04T07:10:38.683146Z"
    }
   },
   "id": "38ed91410c8742b7",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this example, the `create_linear` function is defined to create linear layer models. By applying the `@bst.augment.vmap` decorator, this function is vectorized to enable batching. The `bst.random.split_key(batch_size)` function is used to generate a batch of random keys, each corresponding to a separate instance of the linear layer model. As a result, the `linears` variable contains a collection of linear layers that can process batches of input data, facilitating efficient training and inference in deep learning applications."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c0e5a3b7e7b587b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Using Batched Models\n",
    "\n",
    "We can perform batching on the model's state variables along a specified dimension. Below is a simple example:\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0e30ef059fd88d2"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{('weight',): {'bias': Array([[ 0.12934685, -0.34907067, -0.17345807],\n         [-0.19724102, -0.05216891, -0.33002084],\n         [-0.04470828, -0.5204247 ,  0.0091965 ],\n         [-0.2463954 , -0.10774099,  0.0844532 ],\n         [ 0.08000985, -0.09709602,  0.05567762]], dtype=float32),\n  'weight': Array([[[ 0.12934685, -0.34907067, -0.17345807],\n          [ 0.12934685, -0.34907067, -0.17345807]],\n  \n         [[-0.19724102, -0.05216891, -0.33002084],\n          [-0.19724102, -0.05216891, -0.33002084]],\n  \n         [[-0.04470828, -0.5204247 ,  0.0091965 ],\n          [-0.04470828, -0.5204247 ,  0.0091965 ]],\n  \n         [[-0.2463954 , -0.10774099,  0.0844532 ],\n          [-0.2463954 , -0.10774099,  0.0844532 ]],\n  \n         [[ 0.08000985, -0.09709602,  0.05567762],\n          [ 0.08000985, -0.09709602,  0.05567762]]], dtype=float32)}}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create batched data\n",
    "x_batch = jnp.ones((batch_size, 2))\n",
    "y_batch = jnp.ones((batch_size, 3))\n",
    "\n",
    "# Compute loss for each batch\n",
    "@bst.augment.vmap(in_axes=(0, 0))\n",
    "def eval(model, x):\n",
    "    return model(x)\n",
    "\n",
    "# Batched version of the loss function\n",
    "def batch_loss_fn(x_batch, y_batch):\n",
    "    predictions = eval(linears, x_batch)\n",
    "    return jnp.mean((y_batch - predictions) ** 2)\n",
    "\n",
    "# Compute batched gradients\n",
    "weights = linears.states(bst.ParamState)\n",
    "batch_grads = bst.augment.grad(batch_loss_fn, weights)(x_batch, y_batch)\n",
    "\n",
    "batch_grads"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-04T07:10:39.250378Z",
     "start_time": "2024-11-04T07:10:39.014250Z"
    }
   },
   "id": "2d12cfc3169d68dc",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this example, we first create batched input data `x_batch` and target output `y_batch`. The `eval` function, decorated with `@bst.augment.vmap`, computes the model's predictions for each input in the batch. The `batch_loss_fn` function calculates the mean squared error loss between the predictions and the actual outputs for the entire batch.\n",
    "\n",
    "To compute the gradients for the batched loss function, we retrieve the model parameters (weights) using `linears.states(bst.ParamState)` and then apply `bst.augment.grad` to the `batch_loss_fn`. This returns the gradients with respect to the model parameters, enabling efficient optimization in a batched context. This approach enhances computational performance and allows for the effective training of models on larger datasets."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47e643d4e9df80d4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Array([0, 1, 4, 9], dtype=int32)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Foo(bst.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.a = bst.ParamState(jnp.arange(4))\n",
    "        self.b = bst.ShortTermState(jnp.arange(4))\n",
    "\n",
    "    def __call__(self):\n",
    "        self.b.value = self.a.value * self.b.value\n",
    "\n",
    "\n",
    "@bst.augment.vmap\n",
    "def mul(foo):\n",
    "    foo()\n",
    "\n",
    "\n",
    "foo = Foo()\n",
    "mul(foo)\n",
    "\n",
    "foo.b.value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-04T07:10:39.288776Z",
     "start_time": "2024-11-04T07:10:39.251505Z"
    }
   },
   "id": "a471406426e37688",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the example above, we defined a simple model called `Foo`, which includes a `ParamState` and a `ShortTermState`. We utilized the `vmap` function to transform this model into one that supports batching. In the `mul` function, we invoked this batched model. In this scenario, the `vmap` function automatically recognizes that the values of `a` and `b` are batched, and it seamlessly retrieves these batched values for processing.\n",
    "\n",
    "This automatic handling of batched inputs simplifies the implementation of the model and allows users to focus on higher-level logic without worrying about the underlying mechanics of batching. By leveraging `vmap`, users can efficiently operate on collections of data, thus enhancing the model's scalability and performance."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ee5ee341f66ff7c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Specifying Batching for States\n",
    "\n",
    "During the modeling process, we often encounter situations where we need to batch certain `State` variables while leaving others unbatched. The `brainstate` framework provides the `brainstate.augment.StateAxes` class to specify which `State` variables require batching. `StateAxes` can be used to set the `in_axes` and `out_axes` parameters of `vmap`. Below is a simple example."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3423b6fe60e468b4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import brainunit as u\n",
    "\n",
    "class LIFNet(bst.nn.Module):\n",
    "    def __init__(self, nin, nout):\n",
    "        super().__init__()\n",
    "        self.i2r = bst.nn.Linear(nin, nout)\n",
    "        self.lif = bst.nn.LIF(nout)\n",
    "\n",
    "    def update(self, x):\n",
    "        r = self.i2r(x)\n",
    "        return self.lif(r * u.mA)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-04T07:10:39.293083Z",
     "start_time": "2024-11-04T07:10:39.289782Z"
    }
   },
   "id": "69ba49a48fdfddb3",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this example, we define a simple LIF neuron model that includes a linear layer and an LIF neuron.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4324e84d2f4b97d3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "n_in = 2\n",
    "n_out = 3\n",
    "batch_size = 5\n",
    "\n",
    "net = LIFNet(n_in, n_out)\n",
    "\n",
    "@bst.augment.vmap(out_axes=bst.augment.StateAxes({'new': 0, ...: None}))\n",
    "def init_net(key):\n",
    "    bst.random.set_key(key)\n",
    "\n",
    "    # Initialize a batch of model state variables\n",
    "    with bst.catch_new_states('new'):\n",
    "        bst.nn.init_all_states(net)\n",
    "\n",
    "    # Return a batch of the model\n",
    "    return net\n",
    "\n",
    "net = init_net(bst.random.split_key(batch_size))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-04T07:10:39.317626Z",
     "start_time": "2024-11-04T07:10:39.294209Z"
    }
   },
   "id": "7660beb979409be7",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "ArrayImpl([[0., 0., 0.],\n           [0., 0., 0.],\n           [0., 0., 0.],\n           [0., 0., 0.],\n           [0., 0., 0.]], dtype=float32) * mvolt"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.lif.V.value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-04T07:10:39.398643Z",
     "start_time": "2024-11-04T07:10:39.318313Z"
    }
   },
   "id": "dd7376202bb17268",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this example, we use the `StateAxes` class to specify which `State` variables need batching. We first catch all newly initialized `State` variables using `brainstate.catch_new_states`, assigning them a common tag (`tag='new'`). In the `vmap` function, we utilize `StateAxes` to indicate that all `State` variables with `tag='new'` should be batched. This allows us to perform batching operations on the newly initialized `State` variables."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f00185b1ce20ee2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "@bst.augment.vmap(in_axes=(bst.augment.StateAxes({'new': 0, ...: None}), 0))\n",
    "def batch_run(model, x):\n",
    "    with bst.environ.context(dt=0.1 * u.ms):\n",
    "        o = model.update(x)\n",
    "    return o\n",
    "\n",
    "xs = bst.random.rand(batch_size, 2) < 0.5\n",
    "\n",
    "r = batch_run(net, xs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-04T07:10:39.900989Z",
     "start_time": "2024-11-04T07:10:39.398643Z"
    }
   },
   "id": "a3c89a9a33d37ff6",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "ArrayImpl([[0.0107702 , 0.03013004, 0.05423805],\n           [0.0107702 , 0.03013004, 0.05423805],\n           [0.0107702 , 0.03013004, 0.05423805],\n           [0.01397169, 0.0071168 , 0.04299236],\n           [0.01397169, 0.0071168 , 0.04299236]], dtype=float32) * mvolt"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.lif.V.value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-04T07:10:39.911542Z",
     "start_time": "2024-11-04T07:10:39.902392Z"
    }
   },
   "id": "3d9c0a17825372cd",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this example, we use the `vmap` function to invoke the previously batched model. In the `batch_run` function, we again specify with `StateAxes` that all `State` variables with `tag='new'` require batching, while other `State` instances do not need batching.\n",
    "\n",
    "Through the above examples, we see that even though our model definition is single-batch, we can flexibly apply the `vmap` function to achieve batching during model invocation. This allows us to avoid concerns about batching in the model definition, enabling us to decide on batching at the point of model execution."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a9b8694d909b682"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Multi-Device Parallel Computation\n",
    "\n",
    "In addition to batching, `brainstate` also supports multi-device parallel computation. We can use the `brainstate.augment.pmap` function to convert a model into one that supports parallel computation across multiple devices.\n",
    "\n",
    "The usage of the `brainstate.augment.pmap` augmentation function is fundamentally similar to that of the `brainstate.augment.vmap` function. However, the `pmap` function transforms the model to enable parallel computation across multiple devices, while the `vmap` function facilitates parallel computation across different threads on a single device.\n",
    "\n",
    "This capability is particularly valuable in leveraging the computational power of multiple devices, such as GPUs or TPUs, to accelerate training and inference processes in deep learning applications. By utilizing `pmap`, users can effectively distribute their workloads and achieve significant improvements in performance and efficiency."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec355b8d44d302d1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Combining Augmentation Transformations\n",
    "\n",
    "In practical applications, we often need to combine multiple transformations for program augmentation. The various program augmentation functions and compilation functions in `brainstate` can be used together seamlessly. Below is a simple example:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "484374f092cad37a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Array(0.32209805, dtype=float32)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys = bst.random.rand(batch_size, 3)\n",
    "\n",
    "def loss_fn(x, y):\n",
    "    return jnp.mean((y - batch_run(net, x)) ** 2)\n",
    "\n",
    "\n",
    "weights = net.states(bst.ParamState)\n",
    "opt = bst.optim.Adam(1e-3)\n",
    "opt.register_trainable_weights(weights)\n",
    "\n",
    "\n",
    "@bst.compile.jit\n",
    "def batch_train(xs, ys):\n",
    "    grads, l = bst.augment.grad(loss_fn, weights, return_value=True)(xs, ys)\n",
    "    opt.update(grads)\n",
    "    return l\n",
    "    \n",
    "    \n",
    "l = batch_train(xs, ys)\n",
    "l"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-04T07:10:40.127216Z",
     "start_time": "2024-11-04T07:10:39.911607Z"
    }
   },
   "id": "b8ca72d641c86b9",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this example, we define a loss function `loss_fn` that calculates the mean squared error between the model's predictions, obtained by invoking `batch_run(net, x)`, and the target values `y`. The model's trainable parameters (weights) are registered with the Adam optimizer.\n",
    "\n",
    "The `batch_train` function is compiled with Just-In-Time (JIT) compilation using `bst.compile.jit`. Within this function, we compute the gradients of the loss function with respect to the model parameters using `bst.augment.grad`, specifying that we want both gradients and the loss value returned.\n",
    "\n",
    "After calculating the gradients, we update the model parameters with the optimizer's `update` method. This approach illustrates how multiple functional enhancements, such as batching, gradient computation, and JIT compilation, can be effectively combined to streamline the training process. The result, `l`, contains the computed loss value, demonstrating the integration of these functionalities in a cohesive workflow."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bef89fbe485c06c9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Performance Optimization Recommendations\n",
    "\n",
    "When utilizing program augmentation transformations, the following points should be considered to achieve optimal performance:\n",
    "\n",
    "1. **Appropriate Batch Size**: Choose a suitable batch size based on device memory and computational capacity.\n",
    "2. **Gradient Accumulation**: When the batch size is limited, consider implementing gradient accumulation.\n",
    "3. **Cached Compilation**: Reuse compiled functions to reduce compilation overhead.\n",
    "4. **Memory Management**: Use `jax.device_get()` to release device memory in a timely manner.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8339501d9ac8610"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Debugging Techniques\n",
    "\n",
    "Debugging is an important topic when utilizing functional enhancements. The `brainstate` framework fully supports the debugging tools provided in JAX, such as the `print` function in `jax.debug`. Below is a simple example:\n",
    "\n",
    "```python\n",
    "# Debugging with jax.debug.print\n",
    "@bst.compile.jit\n",
    "def batch_train(xs, ys):\n",
    "    grads, l = bst.augment.grad(loss_fn, weights, return_value=True)(xs, ys)\n",
    "    jax.debug.print(\"Gradients: {g}\", g=grads)\n",
    "    opt.update(grads)\n",
    "    return l\n",
    "```\n",
    "\n",
    "For detailed usage, users can refer to the [JAX Debugging Documentation](https://jax.readthedocs.io/en/latest/debugging/index.html).\n",
    "\n",
    "\n",
    "In this example, the `batch_train` function utilizes `jax.debug.print` to output the computed gradients during the training process. This can be particularly useful for monitoring the training dynamics and diagnosing issues related to gradient computations. Leveraging debugging tools like this can enhance the development process and facilitate the identification of errors or unexpected behaviors in the model's training workflow."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85114a489d19fa4b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "The program augmentations based on `pygraph` are one of the core features of the `BrainState` framework. By effectively utilizing these augmentations, one can significantly improve the inference, training efficiency, and overall performance of models. This tutorial has covered the primary program augmentation features and their usage methods, providing a foundation for further exploration and application of these capabilities. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9fccc0bafeee258"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

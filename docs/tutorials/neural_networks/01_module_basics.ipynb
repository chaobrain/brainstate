{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": "# Module System Basics\n\nThe module system is the foundation for building neural networks in BrainState. It provides a clean, object-oriented interface for organizing stateful computations.\n\nIn this tutorial, you will learn:\n\n- 🏗️ The `Module` base class and its role\n- 🔨 How to create custom modules\n- 🧩 Module composition and nesting\n- 🎯 Parameter management and initialization\n- 📦 Working with module hierarchies\n\n## Why Modules?\n\nModules (via `brainstate.nn.Module`) provide:\n\n✅ **Automatic state management** - States are tracked automatically  \n✅ **Clean abstractions** - Encapsulate related computations  \n✅ **Reusability** - Build once, use everywhere  \n✅ **Composability** - Combine simple modules into complex systems"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import brainstate\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "node_intro",
   "metadata": {},
   "source": "## 1. The Module Base Class\n\n`brainstate.nn.Module` is the base class for all modules in BrainState. It provides:\n\n- Automatic registration of child modules\n- State collection and management\n- Pretty printing and inspection\n- Integration with JAX transformations\n\n### Creating Your First Module\n\nThe simplest module inherits from `Module` and implements `update()`:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first_module",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModule(brainstate.nn.Module):\n",
    "    \"\"\"A minimal module that adds a constant.\"\"\"\n",
    "    \n",
    "    def __init__(self, constant=1.0):\n",
    "        super().__init__()  # Always call parent __init__\n",
    "        self.constant = constant\n",
    "    \n",
    "    def update(self, x):\n",
    "        return x + self.constant\n",
    "\n",
    "# Create and use the module\n",
    "module = SimpleModule(constant=5.0)\n",
    "result = module(jnp.array([1.0, 2.0, 3.0]))\n",
    "\n",
    "print(\"Input:\", jnp.array([1.0, 2.0, 3.0]))\n",
    "print(\"Output:\", result)\n",
    "print(\"\\nModule:\")\n",
    "print(module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stateful_module",
   "metadata": {},
   "source": [
    "### Adding States to Modules\n",
    "\n",
    "Modules become powerful when they contain states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stateful_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Counter(brainstate.nn.Module):\n",
    "    \"\"\"A module that counts how many times it's called.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Create a state to track the count\n",
    "        self.count = brainstate.ShortTermState(jnp.array(0))\n",
    "    \n",
    "    def update(self, x):\n",
    "        # Increment counter\n",
    "        self.count.value = self.count.value + 1\n",
    "        # Return input with count\n",
    "        return x * self.count.value\n",
    "\n",
    "# Test the counter\n",
    "counter = Counter()\n",
    "print(\"Initial count:\", counter.count.value)\n",
    "\n",
    "for i in range(5):\n",
    "    result = counter(jnp.array(10.0))\n",
    "    print(f\"Call {i+1}: count={counter.count.value}, result={result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear_module",
   "metadata": {},
   "source": [
    "## 2. Creating Custom Modules\n",
    "\n",
    "Let's build a complete linear layer from scratch to understand module design:\n",
    "\n",
    "### Example: Custom Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom_linear",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(brainstate.nn.Module):\n",
    "    \"\"\"A linear transformation: y = W @ x + b\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, use_bias=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.use_bias = use_bias\n",
    "        \n",
    "        # Initialize weight with Xavier/Glorot initialization\n",
    "        std = jnp.sqrt(2.0 / (in_features + out_features))\n",
    "        self.weight = brainstate.ParamState(\n",
    "            brainstate.random.randn(in_features, out_features) * std\n",
    "        )\n",
    "        \n",
    "        # Initialize bias to zero\n",
    "        if use_bias:\n",
    "            self.bias = brainstate.ParamState(jnp.zeros(out_features))\n",
    "    \n",
    "    def update(self, x):\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (..., in_features)\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of shape (..., out_features)\n",
    "        \"\"\"\n",
    "        out = x @ self.weight.value\n",
    "        if self.use_bias:\n",
    "            out = out + self.bias.value\n",
    "        return out\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Linear(in_features={self.in_features}, out_features={self.out_features}, use_bias={self.use_bias})\"\n",
    "\n",
    "# Create and test the linear layer\n",
    "brainstate.random.seed(42)\n",
    "linear = Linear(in_features=5, out_features=3)\n",
    "\n",
    "# Forward pass\n",
    "x = jnp.ones(5)\n",
    "y = linear(x)\n",
    "\n",
    "print(\"Module:\")\n",
    "print(linear)\n",
    "print(f\"\\nWeight shape: {linear.weight.value.shape}\")\n",
    "print(f\"Bias shape: {linear.bias.value.shape}\")\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print(f\"Output: {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activation_module",
   "metadata": {},
   "source": [
    "### Example: Custom Activation Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom_activation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyReLU(brainstate.nn.Module):\n",
    "    \"\"\"Leaky ReLU activation: y = max(alpha * x, x)\"\"\"\n",
    "    \n",
    "    def __init__(self, negative_slope=0.01):\n",
    "        super().__init__()\n",
    "        self.negative_slope = negative_slope\n",
    "    \n",
    "    def update(self, x):\n",
    "        return jnp.where(x > 0, x, self.negative_slope * x)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"LeakyReLU(negative_slope={self.negative_slope})\"\n",
    "\n",
    "# Test the activation\n",
    "activation = LeakyReLU(negative_slope=0.1)\n",
    "x = jnp.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "y = activation(x)\n",
    "\n",
    "print(\"Activation:\", activation)\n",
    "print(f\"Input:  {x}\")\n",
    "print(f\"Output: {y}\")\n",
    "\n",
    "# Visualize\n",
    "x_plot = jnp.linspace(-3, 3, 100)\n",
    "y_plot = activation(x_plot)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x_plot, y_plot, linewidth=2, label='LeakyReLU(0.1)')\n",
    "plt.axhline(0, color='gray', linestyle='--', alpha=0.3)\n",
    "plt.axvline(0, color='gray', linestyle='--', alpha=0.3)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.title('Leaky ReLU Activation Function')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composition",
   "metadata": {},
   "source": [
    "## 3. Module Composition and Nesting\n",
    "\n",
    "The real power of modules comes from composing them into larger networks.\n",
    "\n",
    "### Sequential Composition\n",
    "\n",
    "Build a network by stacking layers sequentially:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sequential",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(brainstate.nn.Module):\n",
    "    \"\"\"Multi-layer perceptron with customizable architecture.\"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes, activation='relu'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = []\n",
    "        \n",
    "        # Create layers\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            # Add linear layer\n",
    "            layer = Linear(layer_sizes[i], layer_sizes[i+1])\n",
    "            setattr(self, f'layer_{i}', layer)  # Register as attribute\n",
    "            self.layers.append(layer)\n",
    "            \n",
    "            # Add activation (except for last layer)\n",
    "            if i < len(layer_sizes) - 2:\n",
    "                if activation == 'relu':\n",
    "                    act = LeakyReLU(negative_slope=0.0)  # Standard ReLU\n",
    "                else:\n",
    "                    act = LeakyReLU(negative_slope=0.01)\n",
    "                setattr(self, f'activation_{i}', act)\n",
    "                self.layers.append(act)\n",
    "    \n",
    "    def update(self, x):\n",
    "        \"\"\"Forward pass through all layers.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# Create a 3-layer MLP\n",
    "brainstate.random.seed(0)\n",
    "mlp = MLP(layer_sizes=[10, 64, 32, 5])\n",
    "\n",
    "# Forward pass\n",
    "x = brainstate.random.randn(10)\n",
    "y = mlp(x)\n",
    "\n",
    "print(\"MLP Architecture:\")\n",
    "print(mlp)\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print(f\"Output: {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residual",
   "metadata": {},
   "source": [
    "### Residual Connections\n",
    "\n",
    "Implement skip connections for deeper networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residual_block",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(brainstate.nn.Module):\n",
    "    \"\"\"Residual block: y = F(x) + x\"\"\"\n",
    "    \n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Two linear layers with activation in between\n",
    "        self.linear1 = Linear(dim, dim)\n",
    "        self.activation = LeakyReLU(0.0)\n",
    "        self.linear2 = Linear(dim, dim)\n",
    "    \n",
    "    def update(self, x):\n",
    "        # Compute residual\n",
    "        residual = x\n",
    "        \n",
    "        # Forward through layers\n",
    "        out = self.linear1(x)\n",
    "        out = self.activation(out)\n",
    "        out = self.linear2(out)\n",
    "        \n",
    "        # Add residual\n",
    "        return out + residual\n",
    "\n",
    "class ResNet(brainstate.nn.Module):\n",
    "    \"\"\"Simple ResNet with multiple residual blocks.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_blocks=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_proj = Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.blocks = []\n",
    "        for i in range(n_blocks):\n",
    "            block = ResidualBlock(hidden_dim)\n",
    "            setattr(self, f'block_{i}', block)\n",
    "            self.blocks.append(block)\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_proj = Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def update(self, x):\n",
    "        # Project to hidden dimension\n",
    "        x = self.input_proj(x)\n",
    "        \n",
    "        # Pass through residual blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Project to output\n",
    "        x = self.output_proj(x)\n",
    "        return x\n",
    "\n",
    "# Create ResNet\n",
    "brainstate.random.seed(0)\n",
    "resnet = ResNet(input_dim=10, hidden_dim=32, output_dim=5, n_blocks=3)\n",
    "\n",
    "# Forward pass\n",
    "x = brainstate.random.randn(10)\n",
    "y = resnet(x)\n",
    "\n",
    "print(\"ResNet:\")\n",
    "print(resnet)\n",
    "print(f\"\\nOutput shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "param_management",
   "metadata": {},
   "source": [
    "## 4. Parameter Management\n",
    "\n",
    "### Counting Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "count_params",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(module):\n",
    "    \"\"\"Count total number of parameters in a module.\"\"\"\n",
    "    total = 0\n",
    "    \n",
    "    # Use StateTraceStack to find all states\n",
    "    with brainstate.StateTraceStack() as stack:\n",
    "        # Do a dummy forward pass to trigger state access\n",
    "        try:\n",
    "            if hasattr(module, 'in_features'):\n",
    "                dummy_input = jnp.zeros(module.in_features)\n",
    "            elif hasattr(module, 'input_proj'):\n",
    "                dummy_input = jnp.zeros(module.input_proj.in_features)\n",
    "            else:\n",
    "                dummy_input = jnp.zeros(10)\n",
    "            module(dummy_input)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Count parameters from read states\n",
    "        for state in stack.get_read_states():\n",
    "            if isinstance(state, brainstate.ParamState):\n",
    "                if hasattr(state.value, 'size'):\n",
    "                    total += state.value.size\n",
    "    \n",
    "    return total\n",
    "\n",
    "# Count parameters in different modules\n",
    "print(\"Parameter Counts:\")\n",
    "print(f\"Linear(5→3): {count_parameters(linear):,} parameters\")\n",
    "print(f\"MLP: {count_parameters(mlp):,} parameters\")\n",
    "print(f\"ResNet: {count_parameters(resnet):,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initialization",
   "metadata": {},
   "source": [
    "### Custom Initialization\n",
    "\n",
    "You can customize how parameters are initialized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom_init",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearWithCustomInit(brainstate.nn.Module):\n",
    "    \"\"\"Linear layer with customizable initialization.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, init_method='xavier'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Initialize based on method\n",
    "        if init_method == 'xavier':\n",
    "            # Xavier/Glorot initialization\n",
    "            std = jnp.sqrt(2.0 / (in_features + out_features))\n",
    "            w = brainstate.random.randn(in_features, out_features) * std\n",
    "        elif init_method == 'he':\n",
    "            # He initialization (for ReLU)\n",
    "            std = jnp.sqrt(2.0 / in_features)\n",
    "            w = brainstate.random.randn(in_features, out_features) * std\n",
    "        elif init_method == 'uniform':\n",
    "            # Uniform initialization\n",
    "            bound = jnp.sqrt(6.0 / (in_features + out_features))\n",
    "            w = brainstate.random.uniform(-bound, bound, (in_features, out_features))\n",
    "        elif init_method == 'zeros':\n",
    "            w = jnp.zeros((in_features, out_features))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown init method: {init_method}\")\n",
    "        \n",
    "        self.weight = brainstate.ParamState(w)\n",
    "        self.bias = brainstate.ParamState(jnp.zeros(out_features))\n",
    "    \n",
    "    def update(self, x):\n",
    "        return x @ self.weight.value + self.bias.value\n",
    "\n",
    "# Compare different initializations\n",
    "brainstate.random.seed(42)\n",
    "methods = ['xavier', 'he', 'uniform', 'zeros']\n",
    "layers = {}\n",
    "\n",
    "print(\"Weight Statistics for Different Initializations:\\n\")\n",
    "for method in methods:\n",
    "    brainstate.random.seed(42)  # Same seed for fair comparison\n",
    "    layer = LinearWithCustomInit(100, 100, init_method=method)\n",
    "    layers[method] = layer\n",
    "    \n",
    "    w = layer.weight.value\n",
    "    print(f\"{method:8s}: mean={jnp.mean(w):7.4f}, std={jnp.std(w):7.4f}, \"\n",
    "          f\"min={jnp.min(w):7.4f}, max={jnp.max(w):7.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freeze_params",
   "metadata": {},
   "source": [
    "### Freezing Parameters\n",
    "\n",
    "Sometimes you want to freeze certain parameters during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freeze_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FreezableLinear(brainstate.nn.Module):\n",
    "    \"\"\"Linear layer with freezable parameters.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        \n",
    "        std = jnp.sqrt(2.0 / (in_features + out_features))\n",
    "        self.weight = brainstate.ParamState(brainstate.random.randn(in_features, out_features) * std)\n",
    "        self.bias = brainstate.ParamState(jnp.zeros(out_features))\n",
    "        \n",
    "        # Freeze flags (not states, just regular attributes)\n",
    "        self.weight_frozen = False\n",
    "        self.bias_frozen = False\n",
    "    \n",
    "    def freeze_weight(self):\n",
    "        \"\"\"Freeze the weight parameter.\"\"\"\n",
    "        self.weight_frozen = True\n",
    "    \n",
    "    def freeze_bias(self):\n",
    "        \"\"\"Freeze the bias parameter.\"\"\"\n",
    "        self.bias_frozen = True\n",
    "    \n",
    "    def unfreeze_weight(self):\n",
    "        \"\"\"Unfreeze the weight parameter.\"\"\"\n",
    "        self.weight_frozen = False\n",
    "    \n",
    "    def unfreeze_bias(self):\n",
    "        \"\"\"Unfreeze the bias parameter.\"\"\"\n",
    "        self.bias_frozen = False\n",
    "    \n",
    "    def update(self, x):\n",
    "        return x @ self.weight.value + self.bias.value\n",
    "\n",
    "# Example usage\n",
    "brainstate.random.seed(0)\n",
    "layer = FreezableLinear(5, 3)\n",
    "\n",
    "print(\"Initial state:\")\n",
    "print(f\"Weight frozen: {layer.weight_frozen}\")\n",
    "print(f\"Bias frozen: {layer.bias_frozen}\")\n",
    "\n",
    "# Freeze weight\n",
    "layer.freeze_weight()\n",
    "print(\"\\nAfter freezing weight:\")\n",
    "print(f\"Weight frozen: {layer.weight_frozen}\")\n",
    "print(f\"Bias frozen: {layer.bias_frozen}\")\n",
    "\n",
    "# In training, you would check these flags before updating\n",
    "print(\"\\n💡 In training loops, check freeze flags before applying gradients:\")\n",
    "print(\"   if not layer.weight_frozen:\")\n",
    "print(\"       layer.weight.value -= lr * grad_weight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical_example",
   "metadata": {},
   "source": [
    "## 5. Practical Example: Building a Classifier\n",
    "\n",
    "Let's put everything together to build a complete classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classifier",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(brainstate.nn.Module):\n",
    "    \"\"\"A complete classifier with feature extraction and classification head.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dims, num_classes, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Feature extractor (backbone)\n",
    "        self.backbone = MLP([input_dim] + hidden_dims, activation='relu')\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = Linear(hidden_dims[-1], num_classes)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.training = brainstate.ShortTermState(True)\n",
    "    \n",
    "    def dropout(self, x):\n",
    "        \"\"\"Apply dropout if in training mode.\"\"\"\n",
    "        if not self.training.value:\n",
    "            return x\n",
    "        \n",
    "        keep_prob = 1.0 - self.dropout_rate\n",
    "        mask = brainstate.random.bernoulli(keep_prob, x.shape)\n",
    "        return x * mask / keep_prob\n",
    "    \n",
    "    def update(self, x):\n",
    "        # Extract features\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        # Apply dropout\n",
    "        features = self.dropout(features)\n",
    "        \n",
    "        # Classify\n",
    "        logits = self.classifier(features)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def train_mode(self):\n",
    "        \"\"\"Set to training mode.\"\"\"\n",
    "        self.training.value = True\n",
    "    \n",
    "    def eval_mode(self):\n",
    "        \"\"\"Set to evaluation mode.\"\"\"\n",
    "        self.training.value = False\n",
    "\n",
    "# Create classifier\n",
    "brainstate.random.seed(42)\n",
    "classifier = Classifier(\n",
    "    input_dim=784,      # MNIST-like input\n",
    "    hidden_dims=[256, 128],\n",
    "    num_classes=10,\n",
    "    dropout_rate=0.5\n",
    ")\n",
    "\n",
    "print(\"Classifier:\")\n",
    "print(classifier)\n",
    "\n",
    "# Test in training mode\n",
    "classifier.train_mode()\n",
    "x = brainstate.random.randn(784)\n",
    "logits_train = classifier(x)\n",
    "\n",
    "print(f\"\\n[Training Mode]\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Logits shape: {logits_train.shape}\")\n",
    "print(f\"Logits: {logits_train}\")\n",
    "\n",
    "# Test in eval mode\n",
    "classifier.eval_mode()\n",
    "logits_eval = classifier(x)\n",
    "\n",
    "print(f\"\\n[Eval Mode]\")\n",
    "print(f\"Logits: {logits_eval}\")\n",
    "print(f\"\\nNote: Outputs differ due to dropout in training mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": "## Summary\n\nIn this tutorial, you learned:\n\n✅ **Module base class** - Foundation for all modules  \n✅ **Custom modules** - Creating layers from scratch  \n✅ **Composition** - Building complex networks from simple modules  \n✅ **Residual connections** - Skip connections for deeper networks  \n✅ **Parameter management** - Initialization, counting, and freezing  \n✅ **Complete example** - Full classifier with feature extraction  \n\n### Key Principles\n\n1. 🏗️ **Always inherit from `brainstate.nn.Module`**\n2. 🔧 **Call `super().__init__()` in your constructor**\n3. 📦 **Use `ParamState` for trainable parameters**\n4. 🎯 **Register child modules as attributes**\n5. 🔄 **Implement `update()` for forward computation**\n\n### Best Practices\n\n- ✨ Use descriptive names for layers and states\n- 📝 Document the input/output shapes in docstrings\n- 🎨 Implement `__repr__` for better debugging\n- 🔍 Use proper initialization for different activation functions\n- 🧪 Test modules independently before composition\n\n### Next Steps\n\nContinue with:\n- **Basic Layers** - Pre-built layers (Linear, Conv, Pooling)\n- **Activations & Normalization** - Activation functions and normalization layers\n- **Recurrent Networks** - RNN, LSTM, GRU for sequences\n- **Training** - Gradient computation and optimization"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

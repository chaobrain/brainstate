{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Activation Functions and Normalization\n",
    "\n",
    "Activation functions and normalization layers are critical components that enable deep neural networks to learn complex patterns and train stably.\n",
    "\n",
    "In this tutorial, you will learn:\n",
    "\n",
    "- üéØ **Activation Functions** - ReLU, GELU, Sigmoid, Tanh and variants\n",
    "- üìä **Batch Normalization** - Stabilizing training with BatchNorm\n",
    "- üìê **Layer Normalization** - Alternative normalization for sequences\n",
    "- üî≤ **Group Normalization** - For small batch sizes\n",
    "- ‚öñÔ∏è **When to use each** - Practical guidelines\n",
    "\n",
    "## Why Are These Important?\n",
    "\n",
    "**Activation Functions** introduce non-linearity, allowing networks to learn complex patterns  \n",
    "**Normalization Layers** stabilize training, accelerate convergence, and act as regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import brainstate\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activation_intro",
   "metadata": {},
   "source": [
    "## 1. Activation Functions\n",
    "\n",
    "Activation functions determine the output of a neuron given its input.\n",
    "\n",
    "### ReLU Family\n",
    "\n",
    "#### Standard ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU: max(0, x)\n",
    "relu = brainstate.nn.ReLU()\n",
    "\n",
    "x = jnp.linspace(-3, 3, 100)\n",
    "y = relu(x)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, y, linewidth=2, label='ReLU')\n",
    "plt.axhline(0, color='gray', linestyle='--', alpha=0.3)\n",
    "plt.axvline(0, color='gray', linestyle='--', alpha=0.3)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.title('ReLU Activation', fontweight='bold')\n",
    "plt.legend()\n",
    "\n",
    "# Test with data\n",
    "test_input = jnp.array([-2, -1, 0, 1, 2])\n",
    "test_output = relu(test_input)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.stem(test_input, test_output, basefmt=' ')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.title('ReLU on Sample Points', fontweight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Input:  {test_input}\")\n",
    "print(f\"Output: {test_output}\")\n",
    "print(\"\\n‚úÖ Advantages: Simple, fast, no gradient vanishing for positive values\")\n",
    "print(\"‚ö†Ô∏è  Issue: 'Dying ReLU' problem when neurons always output 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leaky_relu",
   "metadata": {},
   "source": [
    "#### Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leaky_relu_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeakyReLU: max(alpha * x, x)\n",
    "leaky_relu = brainstate.nn.LeakyReLU(negative_slope=0.1)\n",
    "\n",
    "y_leaky = leaky_relu(x)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x, y, linewidth=2, label='ReLU', alpha=0.5)\n",
    "plt.plot(x, y_leaky, linewidth=2, label='Leaky ReLU (Œ±=0.1)')\n",
    "plt.axhline(0, color='gray', linestyle='--', alpha=0.3)\n",
    "plt.axvline(0, color='gray', linestyle='--', alpha=0.3)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.title('ReLU vs Leaky ReLU', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Advantage: Allows small gradient for negative values\")\n",
    "print(\"   Helps prevent dying ReLU problem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern_activations",
   "metadata": {},
   "source": [
    "### Modern Activation Functions\n",
    "\n",
    "#### GELU (Gaussian Error Linear Unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gelu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GELU: Smooth, probabilistic activation\n",
    "gelu = brainstate.nn.GELU()\n",
    "\n",
    "y_gelu = gelu(x)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x, y, linewidth=2, label='ReLU', alpha=0.5)\n",
    "plt.plot(x, y_gelu, linewidth=2, label='GELU')\n",
    "plt.axhline(0, color='gray', linestyle='--', alpha=0.3)\n",
    "plt.axvline(0, color='gray', linestyle='--', alpha=0.3)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.title('ReLU vs GELU', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Used in Transformers (BERT, GPT)\")\n",
    "print(\"‚úÖ Smooth, differentiable everywhere\")\n",
    "print(\"‚úÖ Stochastic regularization properties\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sigmoid_tanh",
   "metadata": {},
   "source": [
    "### Classic Activations\n",
    "\n",
    "#### Sigmoid and Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sigmoid_tanh_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid: 1 / (1 + exp(-x))\n",
    "sigmoid = brainstate.nn.Sigmoid()\n",
    "\n",
    "# Tanh: (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n",
    "tanh = brainstate.nn.Tanh()\n",
    "\n",
    "y_sigmoid = sigmoid(x)\n",
    "y_tanh = tanh(x)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, y_sigmoid, linewidth=2, label='Sigmoid', color='blue')\n",
    "plt.axhline(0.5, color='gray', linestyle='--', alpha=0.3)\n",
    "plt.axvline(0, color='gray', linestyle='--', alpha=0.3)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.title('Sigmoid: œÉ(x) = 1/(1+e‚ÅªÀ£)', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.ylim([-0.1, 1.1])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, y_tanh, linewidth=2, label='Tanh', color='green')\n",
    "plt.axhline(0, color='gray', linestyle='--', alpha=0.3)\n",
    "plt.axvline(0, color='gray', linestyle='--', alpha=0.3)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.title('Tanh: tanh(x)', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.ylim([-1.1, 1.1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Sigmoid:\")\n",
    "print(\"  ‚úÖ Output range: (0, 1) - good for probabilities\")\n",
    "print(\"  ‚ö†Ô∏è  Vanishing gradients for large |x|\")\n",
    "print(\"\\nTanh:\")\n",
    "print(\"  ‚úÖ Output range: (-1, 1) - zero-centered\")\n",
    "print(\"  ‚ö†Ô∏è  Also suffers from vanishing gradients\")\n",
    "print(\"  üìù Often used in RNN/LSTM gates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activation_comparison",
   "metadata": {},
   "source": [
    "### Comparing All Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "all_activations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare multiple activations\n",
    "activations = {\n",
    "    'ReLU': brainstate.nn.ReLU(),\n",
    "    'LeakyReLU': brainstate.nn.LeakyReLU(0.1),\n",
    "    'GELU': brainstate.nn.GELU(),\n",
    "    'ELU': brainstate.nn.ELU(),\n",
    "    'Sigmoid': brainstate.nn.Sigmoid(),\n",
    "    'Tanh': brainstate.nn.Tanh(),\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, activation) in enumerate(activations.items()):\n",
    "    y_act = activation(x)\n",
    "    axes[idx].plot(x, y_act, linewidth=2.5, color=f'C{idx}')\n",
    "    axes[idx].axhline(0, color='gray', linestyle='--', alpha=0.3)\n",
    "    axes[idx].axvline(0, color='gray', linestyle='--', alpha=0.3)\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "    axes[idx].set_xlabel('Input', fontsize=10)\n",
    "    axes[idx].set_ylabel('Output', fontsize=10)\n",
    "    axes[idx].set_title(name, fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Activation Function Guide:\\n\")\n",
    "print(\"ReLU:        Default choice, fast, works well in most cases\")\n",
    "print(\"LeakyReLU:   When dying ReLU is a problem\")\n",
    "print(\"GELU:        Transformers, NLP models\")\n",
    "print(\"ELU:         Smooth variant of ReLU with negative values\")\n",
    "print(\"Sigmoid:     Output layer for binary classification\")\n",
    "print(\"Tanh:        RNN/LSTM gates, when zero-centered output needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "softmax",
   "metadata": {},
   "source": [
    "### Softmax - For Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "softmax_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax: converts logits to probabilities\n",
    "softmax = brainstate.nn.Softmax()\n",
    "\n",
    "# Example logits from a classifier\n",
    "logits = jnp.array([2.0, 1.0, 0.5, 3.0, 0.1])\n",
    "probs = softmax(logits)\n",
    "\n",
    "print(\"Logits:       \", logits)\n",
    "print(\"Probabilities:\", probs)\n",
    "print(f\"Sum of probs:  {jnp.sum(probs):.6f} (should be 1.0)\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "classes = ['Cat', 'Dog', 'Bird', 'Fish', 'Horse']\n",
    "\n",
    "axes[0].bar(classes, logits, color='steelblue', alpha=0.7)\n",
    "axes[0].set_ylabel('Logit Value')\n",
    "axes[0].set_title('Raw Logits', fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "axes[1].bar(classes, probs, color='coral', alpha=0.7)\n",
    "axes[1].set_ylabel('Probability')\n",
    "axes[1].set_title('After Softmax', fontweight='bold')\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add values on bars\n",
    "for i, v in enumerate(probs):\n",
    "    axes[1].text(i, v + 0.02, f'{v:.3f}', ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Softmax converts logits to valid probability distribution\")\n",
    "print(\"   Use for multi-class classification output layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normalization_intro",
   "metadata": {},
   "source": [
    "## 2. Normalization Layers\n",
    "\n",
    "Normalization stabilizes training by controlling the distribution of activations.\n",
    "\n",
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batchnorm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BatchNorm: Normalizes across batch dimension\n",
    "brainstate.random.seed(42)\n",
    "batch_norm = brainstate.nn.BatchNorm1d(num_features=10)\n",
    "\n",
    "print(\"BatchNorm1d:\")\n",
    "print(batch_norm)\n",
    "\n",
    "# Create batch of data with varying statistics\n",
    "batch_size = 32\n",
    "features = 10\n",
    "x = brainstate.random.randn(batch_size, features) * 5 + 10  # mean‚âà10, std‚âà5\n",
    "\n",
    "print(f\"\\nBefore BatchNorm:\")\n",
    "print(f\"  Mean: {jnp.mean(x, axis=0)[:3]}...\")\n",
    "print(f\"  Std:  {jnp.std(x, axis=0)[:3]}...\")\n",
    "\n",
    "# Apply batch norm\n",
    "y = batch_norm(x)\n",
    "\n",
    "print(f\"\\nAfter BatchNorm:\")\n",
    "print(f\"  Mean: {jnp.mean(y, axis=0)[:3]}...\")\n",
    "print(f\"  Std:  {jnp.std(y, axis=0)[:3]}...\")\n",
    "\n",
    "print(\"\\n‚úÖ Normalizes to ~mean=0, ~std=1 across batch\")\n",
    "print(\"‚úÖ Learns scale (Œ≥) and shift (Œ≤) parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "batchnorm_visual",
   "metadata": {},
   "source": [
    "### Visualizing BatchNorm Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batchnorm_visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with different distributions\n",
    "brainstate.random.seed(0)\n",
    "x1 = brainstate.random.randn(1000) * 10 + 50  # High variance, high mean\n",
    "x2 = brainstate.random.randn(1000) * 2 - 5     # Low variance, negative mean\n",
    "\n",
    "# Create batch norm (treating as batch dimension)\n",
    "bn = brainstate.nn.BatchNorm1d(num_features=1)\n",
    "\n",
    "# Reshape to (batch, features)\n",
    "x1_batch = x1[:, None]\n",
    "x2_batch = x2[:, None]\n",
    "\n",
    "# Apply normalization\n",
    "y1 = bn(x1_batch).flatten()\n",
    "y2 = bn(x2_batch).flatten()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Distribution 1 - before\n",
    "axes[0, 0].hist(np.array(x1), bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[0, 0].set_title('Distribution 1 - Before BN', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Value')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].axvline(jnp.mean(x1), color='red', linestyle='--', label=f'Œº={jnp.mean(x1):.1f}')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Distribution 1 - after\n",
    "axes[0, 1].hist(np.array(y1), bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[0, 1].set_title('Distribution 1 - After BN', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Value')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].axvline(jnp.mean(y1), color='red', linestyle='--', label=f'Œº={jnp.mean(y1):.2f}')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Distribution 2 - before\n",
    "axes[1, 0].hist(np.array(x2), bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[1, 0].set_title('Distribution 2 - Before BN', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Value')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].axvline(jnp.mean(x2), color='red', linestyle='--', label=f'Œº={jnp.mean(x2):.1f}')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Distribution 2 - after\n",
    "axes[1, 1].hist(np.array(y2), bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[1, 1].set_title('Distribution 2 - After BN', fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Value')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].axvline(jnp.mean(y2), color='red', linestyle='--', label=f'Œº={jnp.mean(y2):.2f}')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"BatchNorm standardizes different distributions to similar scale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "layernorm",
   "metadata": {},
   "source": [
    "### Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "layernorm_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LayerNorm: Normalizes across features (not batch)\n",
    "layer_norm = brainstate.nn.LayerNorm(normalized_shape=(10,))\n",
    "\n",
    "print(\"LayerNorm:\")\n",
    "print(layer_norm)\n",
    "\n",
    "# Single sample\n",
    "x_single = brainstate.random.randn(10) * 5 + 10\n",
    "\n",
    "print(f\"\\nBefore LayerNorm (single sample):\")\n",
    "print(f\"  Values: {x_single}\")\n",
    "print(f\"  Mean: {jnp.mean(x_single):.3f}\")\n",
    "print(f\"  Std:  {jnp.std(x_single):.3f}\")\n",
    "\n",
    "y_single = layer_norm(x_single)\n",
    "\n",
    "print(f\"\\nAfter LayerNorm:\")\n",
    "print(f\"  Values: {y_single}\")\n",
    "print(f\"  Mean: {jnp.mean(y_single):.3f}\")\n",
    "print(f\"  Std:  {jnp.std(y_single):.3f}\")\n",
    "\n",
    "print(\"\\n‚úÖ LayerNorm works on single samples\")\n",
    "print(\"‚úÖ Popular in Transformers and RNNs\")\n",
    "print(\"‚úÖ Independent of batch size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "groupnorm",
   "metadata": {},
   "source": [
    "### Group Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "groupnorm_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GroupNorm: Divides channels into groups and normalizes within groups\n",
    "group_norm = brainstate.nn.GroupNorm(num_groups=4, num_channels=16)\n",
    "\n",
    "print(\"GroupNorm:\")\n",
    "print(group_norm)\n",
    "\n",
    "# 2D feature map: (batch, height, width, channels)\n",
    "x_img = brainstate.random.randn(2, 8, 8, 16) * 3 + 2\n",
    "y_img = group_norm(x_img)\n",
    "\n",
    "print(f\"\\nInput shape: {x_img.shape}\")\n",
    "print(f\"Output shape: {y_img.shape}\")\n",
    "print(f\"Number of groups: {group_norm.num_groups}\")\n",
    "print(f\"Channels per group: {16 // 4}\")\n",
    "\n",
    "print(\"\\n‚úÖ Works well with small batch sizes\")\n",
    "print(\"‚úÖ Alternative to BatchNorm for small batches\")\n",
    "print(\"‚úÖ Used in computer vision models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norm_comparison",
   "metadata": {},
   "source": [
    "### Comparing Normalization Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norm_comparison_table",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Method': ['BatchNorm', 'LayerNorm', 'GroupNorm', 'InstanceNorm'],\n",
    "    'Normalizes Over': ['Batch + Spatial', 'Features', 'Groups', 'Spatial per channel'],\n",
    "    'Best For': ['CNNs, large batches', 'RNNs, Transformers', 'Small batches', 'Style transfer'],\n",
    "    'Batch Dependent': ['Yes', 'No', 'No', 'No'],\n",
    "    'Typical Use': ['Vision', 'NLP', 'Vision (small batch)', 'GANs, Style'],\n",
    "})\n",
    "\n",
    "print(\"\\nüìä Normalization Method Comparison:\\n\")\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nüéØ Quick Guide:\")\n",
    "print(\"  ‚Ä¢ Large batch + CNN ‚Üí BatchNorm\")\n",
    "print(\"  ‚Ä¢ Small batch + CNN ‚Üí GroupNorm\")\n",
    "print(\"  ‚Ä¢ Sequences/RNN/Transformer ‚Üí LayerNorm\")\n",
    "print(\"  ‚Ä¢ Single image inference ‚Üí LayerNorm or GroupNorm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical_example",
   "metadata": {},
   "source": [
    "## 3. Putting It All Together\n",
    "\n",
    "Building a complete network with activations and normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete_network",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModernCNN(brainstate.graph.Node):\n",
    "    \"\"\"CNN with modern activations and normalization.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Block 1: Conv + BatchNorm + GELU\n",
    "        self.conv1 = brainstate.nn.Conv2d(3, 64, kernel_size=(3, 3), padding='SAME')\n",
    "        self.bn1 = brainstate.nn.BatchNorm2d(64)\n",
    "        self.act1 = brainstate.nn.GELU()\n",
    "        self.pool1 = brainstate.nn.MaxPool2d(kernel_size=(2, 2), strides=(2, 2))\n",
    "        \n",
    "        # Block 2\n",
    "        self.conv2 = brainstate.nn.Conv2d(64, 128, kernel_size=(3, 3), padding='SAME')\n",
    "        self.bn2 = brainstate.nn.BatchNorm2d(128)\n",
    "        self.act2 = brainstate.nn.GELU()\n",
    "        self.pool2 = brainstate.nn.MaxPool2d(kernel_size=(2, 2), strides=(2, 2))\n",
    "        \n",
    "        # Classifier\n",
    "        self.flatten = brainstate.nn.Flatten()\n",
    "        self.fc1 = brainstate.nn.Linear(128 * 8 * 8, 256)\n",
    "        self.ln = brainstate.nn.LayerNorm((256,))\n",
    "        self.act3 = brainstate.nn.GELU()\n",
    "        self.dropout = brainstate.nn.Dropout(p=0.5)\n",
    "        self.fc2 = brainstate.nn.Linear(256, num_classes)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Block 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Block 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Classifier\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.ln(x)\n",
    "        x = self.act3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create and test\n",
    "brainstate.random.seed(0)\n",
    "model = ModernCNN(num_classes=10)\n",
    "\n",
    "# Forward pass\n",
    "x = brainstate.random.randn(4, 32, 32, 3)  # 4 images\n",
    "logits = model(x)\n",
    "\n",
    "print(\"Modern CNN with GELU + BatchNorm + LayerNorm:\")\n",
    "print(model)\n",
    "print(f\"\\nInput: {x.shape}\")\n",
    "print(f\"Output: {logits.shape}\")\n",
    "print(f\"\\nLogits: {logits[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned:\n",
    "\n",
    "‚úÖ **Activation Functions**\n",
    "  - ReLU family (ReLU, Leaky ReLU, ELU)\n",
    "  - Modern activations (GELU, SiLU)\n",
    "  - Classic activations (Sigmoid, Tanh)\n",
    "  - Softmax for classification\n",
    "\n",
    "‚úÖ **Normalization Layers**\n",
    "  - BatchNorm for large-batch training\n",
    "  - LayerNorm for sequences and transformers\n",
    "  - GroupNorm for small-batch scenarios\n",
    "\n",
    "‚úÖ **Practical Guidelines**\n",
    "  - When to use each activation\n",
    "  - When to use each normalization\n",
    "  - How to combine them effectively\n",
    "\n",
    "### Quick Reference Card\n",
    "\n",
    "| Task | Activation | Normalization |\n",
    "|------|-----------|---------------|\n",
    "| **CNN (large batch)** | ReLU/GELU | BatchNorm |\n",
    "| **CNN (small batch)** | ReLU/GELU | GroupNorm |\n",
    "| **Transformer/NLP** | GELU | LayerNorm |\n",
    "| **RNN/LSTM** | Tanh (gates) | LayerNorm |\n",
    "| **Binary output** | Sigmoid | - |\n",
    "| **Multi-class output** | Softmax | - |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. üéØ **Use ReLU/GELU** as default activations\n",
    "2. üìä **Add normalization** after conv/linear layers\n",
    "3. ‚ö° **Order**: Conv/Linear ‚Üí Norm ‚Üí Activation\n",
    "4. üîç **Experiment** with activation functions\n",
    "5. üìù **Use appropriate normalization** for your batch size\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Continue with:\n",
    "- **Recurrent Networks** - Handle sequential data\n",
    "- **Training** - Optimize with gradient descent\n",
    "- **Advanced Architectures** - ResNets, Transformers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

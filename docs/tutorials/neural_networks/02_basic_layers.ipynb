{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Basic Neural Network Layers\n",
    "\n",
    "BrainState provides a comprehensive set of pre-built layers for building neural networks. This tutorial covers the essential building blocks.\n",
    "\n",
    "You will learn about:\n",
    "\n",
    "- üìè **Linear layers** - Fully connected transformations\n",
    "- üî≤ **Convolutional layers** - Spatial feature extraction (1D, 2D, 3D)\n",
    "- üèä **Pooling layers** - Downsampling operations\n",
    "- üíß **Dropout layers** - Regularization techniques\n",
    "- üîß **Utility layers** - Flatten, reshape, and more\n",
    "\n",
    "These layers are optimized, well-tested, and ready to use in your models!"
   ]
  },
  {
   "cell_type": "code",
   "id": "imports",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T10:41:25.199552Z",
     "start_time": "2025-10-10T10:41:23.288994Z"
    }
   },
   "source": [
    "import brainstate\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "linear_layers",
   "metadata": {},
   "source": [
    "## 1. Linear (Fully Connected) Layers\n",
    "\n",
    "Linear layers perform the transformation: **y = Wx + b**\n",
    "\n",
    "### Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "id": "linear_basic",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T10:42:34.462759Z",
     "start_time": "2025-10-10T10:42:34.317605Z"
    }
   },
   "source": [
    "# Create a linear layer\n",
    "brainstate.random.seed(42)\n",
    "linear = brainstate.nn.Linear(10, 5)\n",
    "\n",
    "print(\"Linear Layer:\")\n",
    "print(linear)\n",
    "print(f\"\\nWeight shape: {linear.weight.value['weight'].shape}\")\n",
    "print(f\"Bias shape: {linear.weight.value['bias'].shape}\")\n",
    "\n",
    "# Forward pass\n",
    "x = brainstate.random.randn(10)\n",
    "y = linear(x)\n",
    "\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print(f\"Output: {y}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Layer:\n",
      "Linear(\n",
      "  in_size=(10,),\n",
      "  out_size=(5,),\n",
      "  w_mask=None,\n",
      "  weight=ParamState(\n",
      "    value={\n",
      "      'bias': ShapedArray(float32[5]),\n",
      "      'weight': ShapedArray(float32[10,5])\n",
      "    }\n",
      "  )\n",
      ")\n",
      "\n",
      "Weight shape: (10, 5)\n",
      "Bias shape: (5,)\n",
      "\n",
      "Input shape: (10,)\n",
      "Output shape: (5,)\n",
      "Output: [ 0.24681929  1.2860886  -1.6367221   0.29457197 -0.9486235 ]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "linear_batch",
   "metadata": {},
   "source": [
    "### Batch Processing\n",
    "\n",
    "Linear layers automatically handle batched inputs:"
   ]
  },
  {
   "cell_type": "code",
   "id": "linear_batched",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T10:44:16.854306Z",
     "start_time": "2025-10-10T10:44:16.477764Z"
    }
   },
   "source": [
    "# Batched input: (batch_size, features)\n",
    "x_batch = brainstate.random.randn(32, 10)  # 32 samples, 10 features each\n",
    "y_batch = linear(x_batch)\n",
    "\n",
    "print(f\"Batched input shape: {x_batch.shape}\")\n",
    "print(f\"Batched output shape: {y_batch.shape}\")\n",
    "\n",
    "# Works with arbitrary batch dimensions\n",
    "x_multi = brainstate.random.randn(8, 4, 10)  # (batch1, batch2, features)\n",
    "y_multi = linear(x_multi)\n",
    "\n",
    "print(f\"\\nMulti-batch input: {x_multi.shape}\")\n",
    "print(f\"Multi-batch output: {y_multi.shape}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batched input shape: (32, 10)\n",
      "Batched output shape: (32, 5)\n",
      "\n",
      "Multi-batch input: (8, 4, 10)\n",
      "Multi-batch output: (8, 4, 5)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "linear_variants",
   "metadata": {},
   "source": [
    "### Linear Layer Variants\n",
    "\n",
    "BrainState provides specialized linear layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear_sparse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparseLinear: For sparse connectivity\n",
    "sparse_linear = brainstate.nn.SparseLinear(\n",
    "    num_in=100,\n",
    "    num_out=50,\n",
    "    prob=0.1,  # 10% connectivity\n",
    "    weight=0.05\n",
    ")\n",
    "\n",
    "print(\"Sparse Linear Layer:\")\n",
    "print(sparse_linear)\n",
    "\n",
    "x = brainstate.random.randn(100)\n",
    "y = sparse_linear(x)\n",
    "print(f\"\\nInput: {x.shape} ‚Üí Output: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conv_layers",
   "metadata": {},
   "source": [
    "## 2. Convolutional Layers\n",
    "\n",
    "Convolutional layers extract spatial features using learnable filters.\n",
    "\n",
    "### Conv1d - 1D Convolution\n",
    "\n",
    "Used for sequential data (time series, audio, text):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conv1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv1d: (batch, length, in_channels) ‚Üí (batch, length, out_channels)\n",
    "brainstate.random.seed(0)\n",
    "conv1d = brainstate.nn.Conv1d(\n",
    "    in_channels=3,\n",
    "    out_channels=16,\n",
    "    kernel_size=3,\n",
    "    padding='SAME'  # Keep spatial dimensions\n",
    ")\n",
    "\n",
    "print(\"Conv1d Layer:\")\n",
    "print(conv1d)\n",
    "\n",
    "# Input: (batch=4, length=100, channels=3)\n",
    "x = brainstate.random.randn(4, 100, 3)\n",
    "y = conv1d(x)\n",
    "\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print(f\"Kernel shape: {conv1d.w.value.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conv2d",
   "metadata": {},
   "source": [
    "### Conv2d - 2D Convolution\n",
    "\n",
    "The workhorse for image processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conv2d_basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv2d: (batch, height, width, in_channels) ‚Üí (batch, height, width, out_channels)\n",
    "conv2d = brainstate.nn.Conv2d(\n",
    "    in_channels=3,      # RGB input\n",
    "    out_channels=32,    # 32 feature maps\n",
    "    kernel_size=(3, 3), # 3x3 kernel\n",
    "    strides=(1, 1),     # Stride of 1\n",
    "    padding='SAME'\n",
    ")\n",
    "\n",
    "print(\"Conv2d Layer:\")\n",
    "print(conv2d)\n",
    "\n",
    "# Input: (batch=8, height=28, width=28, channels=3)\n",
    "x_image = brainstate.random.randn(8, 28, 28, 3)\n",
    "y_image = conv2d(x_image)\n",
    "\n",
    "print(f\"\\nInput shape: {x_image.shape}\")\n",
    "print(f\"Output shape: {y_image.shape}\")\n",
    "print(f\"Kernel shape: {conv2d.w.value.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conv_features",
   "metadata": {},
   "source": [
    "### Visualizing Convolutional Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize_conv",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple image with a pattern\n",
    "def create_test_image():\n",
    "    img = jnp.zeros((28, 28, 1))\n",
    "    # Add vertical edge\n",
    "    img = img.at[5:23, 10:13, 0].set(1.0)\n",
    "    # Add horizontal edge\n",
    "    img = img.at[10:13, 5:23, 0].set(1.0)\n",
    "    return img\n",
    "\n",
    "# Create and apply conv\n",
    "brainstate.random.seed(42)\n",
    "edge_conv = brainstate.nn.Conv2d(\n",
    "    in_channels=1,\n",
    "    out_channels=4,\n",
    "    kernel_size=(3, 3),\n",
    "    padding='SAME'\n",
    ")\n",
    "\n",
    "test_img = create_test_image()\n",
    "features = edge_conv(test_img[None, ...])[0]  # Add/remove batch dim\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(test_img[:, :, 0], cmap='gray')\n",
    "axes[0].set_title('Input Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Feature maps\n",
    "for i in range(4):\n",
    "    axes[i+1].imshow(np.array(features[:, :, i]), cmap='viridis')\n",
    "    axes[i+1].set_title(f'Feature Map {i}')\n",
    "    axes[i+1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conv_params",
   "metadata": {},
   "source": [
    "### Convolution Parameters\n",
    "\n",
    "Understanding stride and padding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conv_stride",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different stride values\n",
    "x = brainstate.random.randn(1, 28, 28, 3)\n",
    "\n",
    "configs = [\n",
    "    {'strides': (1, 1), 'padding': 'SAME', 'name': 'Stride 1, SAME'},\n",
    "    {'strides': (2, 2), 'padding': 'SAME', 'name': 'Stride 2, SAME'},\n",
    "    {'strides': (1, 1), 'padding': 'VALID', 'name': 'Stride 1, VALID'},\n",
    "]\n",
    "\n",
    "print(f\"Input shape: {x.shape}\\n\")\n",
    "\n",
    "for config in configs:\n",
    "    brainstate.random.seed(0)\n",
    "    conv = brainstate.nn.Conv2d(\n",
    "        in_channels=3,\n",
    "        out_channels=16,\n",
    "        kernel_size=(3, 3),\n",
    "        strides=config['strides'],\n",
    "        padding=config['padding']\n",
    "    )\n",
    "    y = conv(x)\n",
    "    print(f\"{config['name']:20s}: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conv3d",
   "metadata": {},
   "source": [
    "### Conv3d - 3D Convolution\n",
    "\n",
    "For video or volumetric data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conv3d_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv3d: (batch, depth, height, width, channels)\n",
    "conv3d = brainstate.nn.Conv3d(\n",
    "    in_channels=3,\n",
    "    out_channels=16,\n",
    "    kernel_size=(3, 3, 3),\n",
    "    padding='SAME'\n",
    ")\n",
    "\n",
    "# Video input: (batch=2, frames=16, height=64, width=64, channels=3)\n",
    "x_video = brainstate.random.randn(2, 16, 64, 64, 3)\n",
    "y_video = conv3d(x_video)\n",
    "\n",
    "print(f\"Video input shape: {x_video.shape}\")\n",
    "print(f\"Video output shape: {y_video.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pooling",
   "metadata": {},
   "source": [
    "## 3. Pooling Layers\n",
    "\n",
    "Pooling layers downsample feature maps, reducing spatial dimensions.\n",
    "\n",
    "### Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maxpool",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MaxPool2d: Takes maximum value in each window\n",
    "maxpool = brainstate.nn.MaxPool2d(\n",
    "    kernel_size=(2, 2),\n",
    "    strides=(2, 2)\n",
    ")\n",
    "\n",
    "print(\"MaxPool2d Layer:\")\n",
    "print(maxpool)\n",
    "\n",
    "# Input: (batch=4, height=28, width=28, channels=16)\n",
    "x = brainstate.random.randn(4, 28, 28, 16)\n",
    "y = maxpool(x)\n",
    "\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print(f\"Spatial reduction: {x.shape[1]}x{x.shape[2]} ‚Üí {y.shape[1]}x{y.shape[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "avgpool",
   "metadata": {},
   "source": [
    "### Average Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "avgpool_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AvgPool2d: Takes average value in each window\n",
    "avgpool = brainstate.nn.AvgPool2d(\n",
    "    kernel_size=(2, 2),\n",
    "    strides=(2, 2)\n",
    ")\n",
    "\n",
    "y_avg = avgpool(x)\n",
    "\n",
    "print(\"AvgPool2d Layer:\")\n",
    "print(avgpool)\n",
    "print(f\"\\nOutput shape: {y_avg.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive_pool",
   "metadata": {},
   "source": [
    "### Adaptive Pooling\n",
    "\n",
    "Pools to a fixed output size regardless of input size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive_pool_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaptiveAvgPool2d: Always outputs specified size\n",
    "adaptive_pool = brainstate.nn.AdaptiveAvgPool2d(output_size=(7, 7))\n",
    "\n",
    "# Works with any input size\n",
    "inputs = [\n",
    "    brainstate.random.randn(1, 28, 28, 16),\n",
    "    brainstate.random.randn(1, 56, 56, 16),\n",
    "    brainstate.random.randn(1, 224, 224, 16),\n",
    "]\n",
    "\n",
    "print(\"AdaptiveAvgPool2d (output: 7x7)\\n\")\n",
    "for i, x in enumerate(inputs):\n",
    "    y = adaptive_pool(x)\n",
    "    print(f\"Input {x.shape[1:3]:>8} ‚Üí Output {y.shape[1:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pool_comparison",
   "metadata": {},
   "source": [
    "### Comparing Pooling Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pool_visual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple test pattern\n",
    "x_test = jnp.array([\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 8],\n",
    "    [9, 10, 11, 12],\n",
    "    [13, 14, 15, 16]\n",
    "], dtype=jnp.float32)\n",
    "\n",
    "x_test = x_test[None, :, :, None]  # Add batch and channel dims\n",
    "\n",
    "# Apply different pooling\n",
    "maxpool_2x2 = brainstate.nn.MaxPool2d(kernel_size=(2, 2), strides=(2, 2))\n",
    "avgpool_2x2 = brainstate.nn.AvgPool2d(kernel_size=(2, 2), strides=(2, 2))\n",
    "\n",
    "y_max = maxpool_2x2(x_test)\n",
    "y_avg = avgpool_2x2(x_test)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "# Original\n",
    "im1 = axes[0].imshow(x_test[0, :, :, 0], cmap='viridis', interpolation='nearest')\n",
    "axes[0].set_title('Original (4x4)', fontsize=12, fontweight='bold')\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        axes[0].text(j, i, f'{x_test[0,i,j,0]:.0f}', \n",
    "                    ha='center', va='center', color='white', fontsize=10)\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Max pooled\n",
    "im2 = axes[1].imshow(y_max[0, :, :, 0], cmap='viridis', interpolation='nearest')\n",
    "axes[1].set_title('Max Pooled (2x2)', fontsize=12, fontweight='bold')\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axes[1].text(j, i, f'{y_max[0,i,j,0]:.0f}', \n",
    "                    ha='center', va='center', color='white', fontsize=10)\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "# Avg pooled\n",
    "im3 = axes[2].imshow(y_avg[0, :, :, 0], cmap='viridis', interpolation='nearest')\n",
    "axes[2].set_title('Avg Pooled (2x2)', fontsize=12, fontweight='bold')\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axes[2].text(j, i, f'{y_avg[0,i,j,0]:.1f}', \n",
    "                    ha='center', va='center', color='white', fontsize=10)\n",
    "plt.colorbar(im3, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"MaxPool takes the maximum value from each 2x2 window\")\n",
    "print(\"AvgPool takes the average value from each 2x2 window\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dropout",
   "metadata": {},
   "source": [
    "## 4. Dropout and Regularization\n",
    "\n",
    "Dropout randomly sets activations to zero during training for regularization.\n",
    "\n",
    "### Standard Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dropout_basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout: Randomly zero out elements\n",
    "dropout = brainstate.nn.Dropout(p=0.5)  # Drop 50% of activations\n",
    "\n",
    "print(\"Dropout Layer:\")\n",
    "print(dropout)\n",
    "\n",
    "# Create test input\n",
    "brainstate.random.seed(42)\n",
    "x = jnp.ones(10)\n",
    "\n",
    "# Apply dropout multiple times (different masks)\n",
    "print(\"\\nOriginal:\", x)\n",
    "print(\"\\nDropout outputs (training mode):\")\n",
    "for i in range(3):\n",
    "    y = dropout(x)\n",
    "    print(f\"  {i+1}: {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dropout_modes",
   "metadata": {},
   "source": [
    "### Training vs Evaluation Mode\n",
    "\n",
    "Dropout behaves differently during training and evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dropout_modes_example",
   "metadata": {},
   "outputs": [],
   "source": "class NetworkWithDropout(brainstate.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super().__init__()\n        self.linear1 = brainstate.nn.Linear(input_dim, hidden_dim)\n        self.dropout = brainstate.nn.Dropout(p=0.5)\n        self.linear2 = brainstate.nn.Linear(hidden_dim, output_dim)\n    \n    def update(self, x):\n        x = self.linear1(x)\n        x = jnp.maximum(0, x)  # ReLU\n        x = self.dropout(x)\n        x = self.linear2(x)\n        return x\n\n# Create network\nbrainstate.random.seed(0)\nnet = NetworkWithDropout(10, 20, 5)\n\n# Test input\nx = brainstate.random.randn(10)\n\n# Compare outputs\ny1 = net(x)\ny2 = net(x)\n\nprint(\"With dropout (training):\")\nprint(f\"  Output 1: {y1}\")\nprint(f\"  Output 2: {y2}\")\nprint(f\"  Outputs differ: {not jnp.allclose(y1, y2)}\")\n\nprint(\"\\nüí° Note: In training mode, outputs vary due to random dropout masks\")\nprint(\"   In eval mode, dropout is disabled for consistent predictions\")"
  },
  {
   "cell_type": "markdown",
   "id": "utility_layers",
   "metadata": {},
   "source": [
    "## 5. Utility Layers\n",
    "\n",
    "### Flatten Layer\n",
    "\n",
    "Flattens multi-dimensional inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flatten",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten: Reshape to 1D\n",
    "flatten = brainstate.nn.Flatten()\n",
    "\n",
    "# Example: After convolution, flatten before fully connected\n",
    "x_conv = brainstate.random.randn(4, 7, 7, 64)  # (batch, H, W, C)\n",
    "x_flat = flatten(x_conv)\n",
    "\n",
    "print(f\"Before flatten: {x_conv.shape}\")\n",
    "print(f\"After flatten: {x_flat.shape}\")\n",
    "print(f\"\\nFlattened: {7 * 7 * 64} = {x_flat.shape[1]} features per sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete_cnn",
   "metadata": {},
   "source": [
    "## 6. Building a Complete CNN\n",
    "\n",
    "Let's combine everything into a complete convolutional neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete_cnn_example",
   "metadata": {},
   "outputs": [],
   "source": "class SimpleCNN(brainstate.nn.Module):\n    \"\"\"Simple CNN for image classification.\"\"\"\n    \n    def __init__(self, num_classes=10):\n        super().__init__()\n        \n        # Conv block 1\n        self.conv1 = brainstate.nn.Conv2d(3, 32, kernel_size=(3, 3), padding='SAME')\n        self.pool1 = brainstate.nn.MaxPool2d(kernel_size=(2, 2), strides=(2, 2))\n        \n        # Conv block 2\n        self.conv2 = brainstate.nn.Conv2d(32, 64, kernel_size=(3, 3), padding='SAME')\n        self.pool2 = brainstate.nn.MaxPool2d(kernel_size=(2, 2), strides=(2, 2))\n        \n        # Conv block 3\n        self.conv3 = brainstate.nn.Conv2d(64, 128, kernel_size=(3, 3), padding='SAME')\n        self.pool3 = brainstate.nn.MaxPool2d(kernel_size=(2, 2), strides=(2, 2))\n        \n        # Flatten and classify\n        self.flatten = brainstate.nn.Flatten()\n        self.fc1 = brainstate.nn.Linear(128 * 4 * 4, 256)  # Assuming 32x32 input\n        self.dropout = brainstate.nn.Dropout(p=0.5)\n        self.fc2 = brainstate.nn.Linear(256, num_classes)\n    \n    def update(self, x):\n        # Conv block 1\n        x = self.conv1(x)\n        x = jnp.maximum(0, x)  # ReLU\n        x = self.pool1(x)\n        \n        # Conv block 2\n        x = self.conv2(x)\n        x = jnp.maximum(0, x)\n        x = self.pool2(x)\n        \n        # Conv block 3\n        x = self.conv3(x)\n        x = jnp.maximum(0, x)\n        x = self.pool3(x)\n        \n        # Classifier\n        x = self.flatten(x)\n        x = self.fc1(x)\n        x = jnp.maximum(0, x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        \n        return x\n\n# Create CNN\nbrainstate.random.seed(42)\ncnn = SimpleCNN(num_classes=10)\n\nprint(\"Simple CNN Architecture:\")\nprint(cnn)\n\n# Test with batch of images\nbatch_size = 8\nimages = brainstate.random.randn(batch_size, 32, 32, 3)  # CIFAR-10 size\nlogits = cnn(images)\n\nprint(f\"\\nInput shape: {images.shape}\")\nprint(f\"Output shape: {logits.shape}\")\nprint(f\"\\nLogits for first image: {logits[0]}\")"
  },
  {
   "cell_type": "markdown",
   "id": "feature_viz",
   "metadata": {},
   "source": [
    "### Visualizing CNN Feature Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature_visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get intermediate features\n",
    "def get_conv_features(model, x):\n",
    "    \"\"\"Extract features from each conv layer.\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Conv 1\n",
    "    x = model.conv1(x)\n",
    "    x = jnp.maximum(0, x)\n",
    "    features.append(x)\n",
    "    x = model.pool1(x)\n",
    "    \n",
    "    # Conv 2\n",
    "    x = model.conv2(x)\n",
    "    x = jnp.maximum(0, x)\n",
    "    features.append(x)\n",
    "    x = model.pool2(x)\n",
    "    \n",
    "    # Conv 3\n",
    "    x = model.conv3(x)\n",
    "    x = jnp.maximum(0, x)\n",
    "    features.append(x)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract features\n",
    "single_image = images[0:1]  # Take first image\n",
    "features = get_conv_features(cnn, single_image)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow((np.array(single_image[0]) + 1) / 2)  # Normalize to [0,1]\n",
    "axes[0].set_title('Input Image\\n(32√ó32√ó3)', fontsize=10, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Feature maps\n",
    "layer_names = ['Conv1\\n(32√ó32√ó32)', 'Conv2\\n(16√ó16√ó64)', 'Conv3\\n(8√ó8√ó128)']\n",
    "for i, (feat, name) in enumerate(zip(features, layer_names)):\n",
    "    # Show first feature map from each layer\n",
    "    feat_map = np.array(feat[0, :, :, 0])\n",
    "    axes[i+1].imshow(feat_map, cmap='viridis')\n",
    "    axes[i+1].set_title(name, fontsize=10, fontweight='bold')\n",
    "    axes[i+1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Feature map shapes:\")\n",
    "for i, feat in enumerate(features):\n",
    "    print(f\"  Layer {i+1}: {feat.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned about:\n",
    "\n",
    "‚úÖ **Linear layers** - Fully connected transformations  \n",
    "‚úÖ **Convolutional layers** - 1D, 2D, 3D spatial feature extraction  \n",
    "‚úÖ **Pooling layers** - Max, average, and adaptive pooling  \n",
    "‚úÖ **Dropout** - Regularization through random masking  \n",
    "‚úÖ **Utility layers** - Flatten and reshape operations  \n",
    "‚úÖ **Complete CNN** - Building end-to-end architectures  \n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "| Layer Type | Use Case | Key Parameters |\n",
    "|------------|----------|----------------|\n",
    "| **Linear** | Fully connected | `in_features`, `out_features` |\n",
    "| **Conv2d** | Image features | `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding` |\n",
    "| **MaxPool2d** | Downsampling | `kernel_size`, `strides` |\n",
    "| **Dropout** | Regularization | `p` (drop probability) |\n",
    "| **Flatten** | Shape transformation | None |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. üéØ **Use SAME padding** to preserve spatial dimensions\n",
    "2. üìê **Double check shapes** - especially after conv/pool operations\n",
    "3. üíß **Add dropout** after dense layers for regularization\n",
    "4. üèä **Pool after activation** - standard practice in CNNs\n",
    "5. üîç **Visualize features** - helps debug and understand the network\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Continue with:\n",
    "- **Activations & Normalization** - Improve training stability\n",
    "- **Recurrent Networks** - Handle sequential data\n",
    "- **Training** - Put it all together with optimization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

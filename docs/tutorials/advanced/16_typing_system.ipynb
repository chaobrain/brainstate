{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 16: Type System and Annotations\n",
    "\n",
    "In this tutorial, we'll explore BrainState's advanced type system, which provides powerful type checking and validation capabilities for neural network development.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "- Use the typing module for type annotations\n",
    "- Work with OneOfTypes for union-like types\n",
    "- Use JointTypes for composite type requirements\n",
    "- Apply type annotations to neural network code\n",
    "- Validate inputs using type constraints\n",
    "- Design type-safe APIs\n",
    "- Follow typing best practices\n",
    "\n",
    "## Why Types Matter\n",
    "\n",
    "Strong typing provides:\n",
    "- **Early error detection**: Catch bugs before runtime\n",
    "- **Better documentation**: Types document expected inputs/outputs\n",
    "- **IDE support**: Better autocomplete and hints\n",
    "- **Refactoring safety**: Types help prevent breaking changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brainstate as bst\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Union, Optional, Tuple, List, Dict, Callable\n",
    "\n",
    "# Set random seed\n",
    "bst.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Type Annotations\n",
    "\n",
    "Start with standard Python typing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic typed layer\n",
    "class TypedLinear(bst.graph.Node):\n",
    "    \"\"\"Linear layer with type annotations.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features: int, out_features: int, use_bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.in_features: int = in_features\n",
    "        self.out_features: int = out_features\n",
    "        \n",
    "        # Parameter with type annotation\n",
    "        self.weight: bst.ParamState = bst.ParamState(\n",
    "            bst.random.randn(in_features, out_features) * 0.1\n",
    "        )\n",
    "        \n",
    "        if use_bias:\n",
    "            self.bias: Optional[bst.ParamState] = bst.ParamState(\n",
    "                jnp.zeros(out_features)\n",
    "            )\n",
    "        else:\n",
    "            self.bias: Optional[bst.ParamState] = None\n",
    "    \n",
    "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Input array of shape (..., in_features)\n",
    "            \n",
    "        Returns:\n",
    "            Output array of shape (..., out_features)\n",
    "        \"\"\"\n",
    "        output: jnp.ndarray = x @ self.weight.value\n",
    "        \n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias.value\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Create typed layer\n",
    "layer = TypedLinear(in_features=10, out_features=5)\n",
    "x = bst.random.randn(3, 10)\n",
    "output = layer(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nType annotations help IDEs understand the code!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Union and Optional\n",
    "from typing import Union, Optional\n",
    "\n",
    "class FlexibleActivation(bst.graph.Node):\n",
    "    \"\"\"Activation that accepts function or string.\"\"\"\n",
    "    \n",
    "    def __init__(self, activation: Union[str, Callable[[jnp.ndarray], jnp.ndarray]]):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store activation\n",
    "        if isinstance(activation, str):\n",
    "            # String activation name\n",
    "            activations = {\n",
    "                'relu': jax.nn.relu,\n",
    "                'tanh': jnp.tanh,\n",
    "                'sigmoid': jax.nn.sigmoid,\n",
    "            }\n",
    "            self.activation: Callable = activations[activation]\n",
    "        else:\n",
    "            # Custom function\n",
    "            self.activation: Callable = activation\n",
    "    \n",
    "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        return self.activation(x)\n",
    "\n",
    "# Test with string\n",
    "act1 = FlexibleActivation('relu')\n",
    "x = jnp.array([-1.0, 0.0, 1.0])\n",
    "print(f\"ReLU activation: {act1(x)}\")\n",
    "\n",
    "# Test with custom function\n",
    "def custom_activation(x):\n",
    "    return jnp.maximum(0, x) ** 2\n",
    "\n",
    "act2 = FlexibleActivation(custom_activation)\n",
    "print(f\"Custom activation: {act2(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. OneOfTypes: Union-like Types\n",
    "\n",
    "BrainState's `OneOfTypes` provides advanced union type functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brainstate.mixin import OneOfTypes\n",
    "\n",
    "# OneOfTypes example\n",
    "# Define a type that can be int, float, or array\n",
    "NumericType = OneOfTypes[int, float, jnp.ndarray]\n",
    "\n",
    "class ScalableLayer(bst.graph.Node):\n",
    "    \"\"\"Layer with flexible scaling.\"\"\"\n",
    "    \n",
    "    def __init__(self, features: int, scale: Union[int, float, jnp.ndarray]):\n",
    "        super().__init__()\n",
    "        self.features = features\n",
    "        self.weight = bst.ParamState(bst.random.randn(features, features) * 0.1)\n",
    "        \n",
    "        # Scale can be scalar or array\n",
    "        if isinstance(scale, (int, float)):\n",
    "            self.scale = jnp.array(scale)\n",
    "        else:\n",
    "            self.scale = scale\n",
    "    \n",
    "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        return (x @ self.weight.value) * self.scale\n",
    "\n",
    "# Test with different scale types\n",
    "x = bst.random.randn(2, 5)\n",
    "\n",
    "# Scalar scale\n",
    "layer1 = ScalableLayer(5, scale=2.0)\n",
    "out1 = layer1(x)\n",
    "print(f\"Scalar scale (2.0): mean output = {jnp.mean(out1):.3f}\")\n",
    "\n",
    "# Array scale (per-feature)\n",
    "layer2 = ScalableLayer(5, scale=jnp.array([0.5, 1.0, 1.5, 2.0, 2.5]))\n",
    "out2 = layer2(x)\n",
    "print(f\"Array scale: output shape = {out2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. JointTypes: Composite Type Requirements\n",
    "\n",
    "`JointTypes` allows specifying that a value must satisfy multiple type constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brainstate.mixin import JointTypes\n",
    "\n",
    "# JointTypes usage (conceptual example)\n",
    "# In practice, JointTypes is used internally by BrainState\n",
    "\n",
    "class ValidatedLayer(bst.graph.Node):\n",
    "    \"\"\"Layer with input validation.\"\"\"\n",
    "    \n",
    "    def __init__(self, features: int, min_val: float = -1.0, max_val: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.features = features\n",
    "        self.min_val = min_val\n",
    "        self.max_val = max_val\n",
    "        self.weight = bst.ParamState(bst.random.randn(features, features) * 0.1)\n",
    "    \n",
    "    def validate_input(self, x: jnp.ndarray) -> bool:\n",
    "        \"\"\"Validate input constraints.\"\"\"\n",
    "        # Check type\n",
    "        if not isinstance(x, jnp.ndarray):\n",
    "            return False\n",
    "        \n",
    "        # Check shape\n",
    "        if x.shape[-1] != self.features:\n",
    "            return False\n",
    "        \n",
    "        # Check value range\n",
    "        if jnp.any(x < self.min_val) or jnp.any(x > self.max_val):\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        if not self.validate_input(x):\n",
    "            raise ValueError(\n",
    "                f\"Input must be ndarray with shape (..., {self.features}) \"\n",
    "                f\"and values in [{self.min_val}, {self.max_val}]\"\n",
    "            )\n",
    "        \n",
    "        return x @ self.weight.value\n",
    "\n",
    "# Test validation\n",
    "layer = ValidatedLayer(features=5, min_val=-2.0, max_val=2.0)\n",
    "\n",
    "# Valid input\n",
    "x_valid = bst.random.randn(3, 5) * 0.5  # Values in range\n",
    "try:\n",
    "    out = layer(x_valid)\n",
    "    print(\"Valid input: Success!\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Invalid input (out of range)\n",
    "x_invalid = bst.random.randn(3, 5) * 10  # Values likely out of range\n",
    "try:\n",
    "    out = layer(x_invalid)\n",
    "    print(\"Invalid input: Success!\")\n",
    "except ValueError as e:\n",
    "    print(f\"Expected error: Input validation failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Type Annotations for Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive type annotations\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "\n",
    "class WellTypedMLP(bst.graph.Node):\n",
    "    \"\"\"MLP with comprehensive type annotations.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        layer_sizes: List[int],\n",
    "        activation: str = 'relu',\n",
    "        dropout_rate: Optional[float] = None,\n",
    "        use_batchnorm: bool = False\n",
    "    ):\n",
    "        \"\"\"Initialize MLP.\n",
    "        \n",
    "        Args:\n",
    "            layer_sizes: List of layer dimensions [input, hidden1, ..., output]\n",
    "            activation: Activation function name\n",
    "            dropout_rate: Dropout rate (None to disable)\n",
    "            use_batchnorm: Whether to use batch normalization\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_sizes: List[int] = layer_sizes\n",
    "        self.activation_name: str = activation\n",
    "        self.dropout_rate: Optional[float] = dropout_rate\n",
    "        self.use_batchnorm: bool = use_batchnorm\n",
    "        \n",
    "        # Build layers\n",
    "        self.layers: List[bst.nn.Linear] = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            layer = bst.nn.Linear(layer_sizes[i], layer_sizes[i + 1])\n",
    "            self.layers.append(layer)\n",
    "            setattr(self, f'layer_{i}', layer)\n",
    "        \n",
    "        # Optional batch norm\n",
    "        if use_batchnorm:\n",
    "            self.batchnorms: Optional[List[bst.nn.BatchNorm1d]] = [\n",
    "                bst.nn.BatchNorm1d(size) for size in layer_sizes[1:-1]\n",
    "            ]\n",
    "            for i, bn in enumerate(self.batchnorms):\n",
    "                setattr(self, f'bn_{i}', bn)\n",
    "        else:\n",
    "            self.batchnorms: Optional[List] = None\n",
    "        \n",
    "        # Optional dropout\n",
    "        if dropout_rate is not None:\n",
    "            self.dropout: Optional[bst.nn.Dropout] = bst.nn.Dropout(dropout_rate)\n",
    "        else:\n",
    "            self.dropout: Optional[bst.nn.Dropout] = None\n",
    "        \n",
    "        # Activation function\n",
    "        activations: Dict[str, Callable] = {\n",
    "            'relu': jax.nn.relu,\n",
    "            'tanh': jnp.tanh,\n",
    "            'sigmoid': jax.nn.sigmoid,\n",
    "        }\n",
    "        self.activation: Callable[[jnp.ndarray], jnp.ndarray] = activations[activation]\n",
    "    \n",
    "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Input array of shape (batch, input_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Output array of shape (batch, output_dim)\n",
    "        \"\"\"\n",
    "        # Process through layers\n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            x = layer(x)\n",
    "            \n",
    "            # Optional batch norm\n",
    "            if self.batchnorms is not None:\n",
    "                x = self.batchnorms[i](x)\n",
    "            \n",
    "            # Activation\n",
    "            x = self.activation(x)\n",
    "            \n",
    "            # Optional dropout\n",
    "            if self.dropout is not None:\n",
    "                x = self.dropout(x)\n",
    "        \n",
    "        # Final layer (no activation)\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "    \n",
    "    def get_config(self) -> Dict[str, any]:\n",
    "        \"\"\"Get configuration dictionary.\n",
    "        \n",
    "        Returns:\n",
    "            Configuration dict\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'layer_sizes': self.layer_sizes,\n",
    "            'activation': self.activation_name,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "            'use_batchnorm': self.use_batchnorm,\n",
    "        }\n",
    "\n",
    "# Create typed MLP\n",
    "model = WellTypedMLP(\n",
    "    layer_sizes=[10, 20, 15, 5],\n",
    "    activation='relu',\n",
    "    dropout_rate=0.3,\n",
    "    use_batchnorm=True\n",
    ")\n",
    "\n",
    "# Test\n",
    "x = bst.random.randn(8, 10)\n",
    "output = model(x)\n",
    "\n",
    "print(f\"Model configuration:\")\n",
    "for key, value in model.get_config().items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Type Hints for State Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typed state management\n",
    "from typing import TypeVar, Generic\n",
    "\n",
    "class TypedStatefulLayer(bst.graph.Node):\n",
    "    \"\"\"Layer with explicitly typed states.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Parameters with type hints\n",
    "        self.W_input: bst.ParamState = bst.ParamState(\n",
    "            bst.random.randn(input_dim, hidden_dim) * 0.1\n",
    "        )\n",
    "        self.W_hidden: bst.ParamState = bst.ParamState(\n",
    "            bst.random.randn(hidden_dim, hidden_dim) * 0.1\n",
    "        )\n",
    "        self.bias: bst.ParamState = bst.ParamState(\n",
    "            jnp.zeros(hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Hidden state\n",
    "        self.hidden: bst.ShortTermState = bst.ShortTermState(\n",
    "            jnp.zeros(hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Metrics\n",
    "        self.call_count: bst.ShortTermState = bst.ShortTermState(\n",
    "            jnp.array(0, dtype=jnp.int32)\n",
    "        )\n",
    "    \n",
    "    def reset_state(self) -> None:\n",
    "        \"\"\"Reset hidden state.\"\"\"\n",
    "        self.hidden.value = jnp.zeros_like(self.hidden.value)\n",
    "        self.call_count.value = jnp.array(0, dtype=jnp.int32)\n",
    "    \n",
    "    def get_parameters(self) -> Dict[str, jnp.ndarray]:\n",
    "        \"\"\"Get parameter dictionary.\n",
    "        \n",
    "        Returns:\n",
    "            Dict mapping parameter names to values\n",
    "        \"\"\"\n",
    "        params: Dict[str, jnp.ndarray] = {}\n",
    "        for name, state in self.states(bst.ParamState).items():\n",
    "            params[name] = state.value\n",
    "        return params\n",
    "    \n",
    "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"Forward pass with state update.\n",
    "        \n",
    "        Args:\n",
    "            x: Input of shape (batch, input_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Output of shape (batch, hidden_dim)\n",
    "        \"\"\"\n",
    "        # Update count\n",
    "        self.call_count.value += 1\n",
    "        \n",
    "        # Compute new hidden state\n",
    "        h_new: jnp.ndarray = jnp.tanh(\n",
    "            x @ self.W_input.value +\n",
    "            self.hidden.value @ self.W_hidden.value +\n",
    "            self.bias.value\n",
    "        )\n",
    "        \n",
    "        # Update state\n",
    "        self.hidden.value = h_new\n",
    "        \n",
    "        return h_new\n",
    "\n",
    "# Test typed stateful layer\n",
    "layer = TypedStatefulLayer(input_dim=5, hidden_dim=10)\n",
    "\n",
    "# Process sequence\n",
    "sequence = [bst.random.randn(1, 5) for _ in range(5)]\n",
    "\n",
    "for i, x in enumerate(sequence):\n",
    "    output = layer(x)\n",
    "    if i == 0:\n",
    "        print(f\"Step {i}: output shape = {output.shape}\")\n",
    "\n",
    "print(f\"\\nProcessed {layer.call_count.value} steps\")\n",
    "print(f\"Final hidden state norm: {jnp.linalg.norm(layer.hidden.value):.3f}\")\n",
    "\n",
    "# Get parameters\n",
    "params = layer.get_parameters()\n",
    "print(f\"\\nParameters: {list(params.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Type-Safe API Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type-safe initialization\n",
    "from typing import Protocol, runtime_checkable\n",
    "\n",
    "@runtime_checkable\n",
    "class Initializer(Protocol):\n",
    "    \"\"\"Protocol for weight initializers.\"\"\"\n",
    "    \n",
    "    def __call__(self, shape: Tuple[int, ...], dtype: jnp.dtype = jnp.float32) -> jnp.ndarray:\n",
    "        \"\"\"Generate initial weights.\"\"\"\n",
    "        ...\n",
    "\n",
    "class XavierInitializer:\n",
    "    \"\"\"Xavier/Glorot initialization.\"\"\"\n",
    "    \n",
    "    def __call__(self, shape: Tuple[int, ...], dtype: jnp.dtype = jnp.float32) -> jnp.ndarray:\n",
    "        if len(shape) < 2:\n",
    "            return jnp.zeros(shape, dtype=dtype)\n",
    "        \n",
    "        fan_in, fan_out = shape[0], shape[-1]\n",
    "        limit = jnp.sqrt(6.0 / (fan_in + fan_out))\n",
    "        return bst.random.uniform(-limit, limit, shape).astype(dtype)\n",
    "\n",
    "class HeInitializer:\n",
    "    \"\"\"He initialization for ReLU networks.\"\"\"\n",
    "    \n",
    "    def __call__(self, shape: Tuple[int, ...], dtype: jnp.dtype = jnp.float32) -> jnp.ndarray:\n",
    "        if len(shape) < 2:\n",
    "            return jnp.zeros(shape, dtype=dtype)\n",
    "        \n",
    "        fan_in = shape[0]\n",
    "        std = jnp.sqrt(2.0 / fan_in)\n",
    "        return (bst.random.randn(*shape) * std).astype(dtype)\n",
    "\n",
    "class ConfigurableLayer(bst.graph.Node):\n",
    "    \"\"\"Layer with type-safe initialization.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        initializer: Initializer = XavierInitializer()\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Verify initializer conforms to protocol\n",
    "        if not isinstance(initializer, Initializer):\n",
    "            raise TypeError(f\"Initializer must implement Initializer protocol\")\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.weight: bst.ParamState = bst.ParamState(\n",
    "            initializer((in_features, out_features))\n",
    "        )\n",
    "        self.bias: bst.ParamState = bst.ParamState(\n",
    "            jnp.zeros(out_features)\n",
    "        )\n",
    "    \n",
    "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        return x @ self.weight.value + self.bias.value\n",
    "\n",
    "# Test different initializers\n",
    "print(\"Initializer Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "x = bst.random.randn(10, 20)\n",
    "\n",
    "# Xavier initialization\n",
    "layer_xavier = ConfigurableLayer(20, 10, initializer=XavierInitializer())\n",
    "out_xavier = layer_xavier(x)\n",
    "print(f\"Xavier init - weight std: {jnp.std(layer_xavier.weight.value):.4f}\")\n",
    "\n",
    "# He initialization\n",
    "layer_he = ConfigurableLayer(20, 10, initializer=HeInitializer())\n",
    "out_he = layer_he(x)\n",
    "print(f\"He init - weight std: {jnp.std(layer_he.weight.value):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Type Checking and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime type checking\n",
    "def check_shape(x: jnp.ndarray, expected_shape: Tuple[int, ...], name: str = \"input\"):\n",
    "    \"\"\"Validate array shape.\n",
    "    \n",
    "    Args:\n",
    "        x: Array to check\n",
    "        expected_shape: Expected shape (use -1 for any size)\n",
    "        name: Name for error messages\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If shape doesn't match\n",
    "    \"\"\"\n",
    "    if len(x.shape) != len(expected_shape):\n",
    "        raise ValueError(\n",
    "            f\"{name} has {len(x.shape)} dimensions, expected {len(expected_shape)}\"\n",
    "        )\n",
    "    \n",
    "    for i, (actual, expected) in enumerate(zip(x.shape, expected_shape)):\n",
    "        if expected != -1 and actual != expected:\n",
    "            raise ValueError(\n",
    "                f\"{name} dimension {i} is {actual}, expected {expected}\"\n",
    "            )\n",
    "\n",
    "class ValidatedConv(bst.graph.Node):\n",
    "    \"\"\"Convolution with strict input validation.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        self.kernel = bst.ParamState(\n",
    "            bst.random.randn(out_channels, in_channels, kernel_size, kernel_size) * 0.1\n",
    "        )\n",
    "    \n",
    "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"Apply convolution.\n",
    "        \n",
    "        Args:\n",
    "            x: Input of shape (batch, height, width, in_channels)\n",
    "            \n",
    "        Returns:\n",
    "            Output of shape (batch, height, width, out_channels)\n",
    "        \"\"\"\n",
    "        # Validate input shape\n",
    "        check_shape(x, (-1, -1, -1, self.in_channels), \"input\")\n",
    "        \n",
    "        # Apply convolution (simplified)\n",
    "        # In real implementation, would use jax.lax.conv\n",
    "        batch, h, w, _ = x.shape\n",
    "        \n",
    "        # Placeholder: just return correct shape\n",
    "        output = jnp.zeros((batch, h, w, self.out_channels))\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test validation\n",
    "conv = ValidatedConv(in_channels=3, out_channels=16, kernel_size=3)\n",
    "\n",
    "# Valid input\n",
    "x_valid = bst.random.randn(2, 32, 32, 3)\n",
    "try:\n",
    "    out = conv(x_valid)\n",
    "    print(f\"Valid input: output shape = {out.shape}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Invalid input (wrong channels)\n",
    "x_invalid = bst.random.randn(2, 32, 32, 5)\n",
    "try:\n",
    "    out = conv(x_invalid)\n",
    "    print(f\"Invalid input: output shape = {out.shape}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Expected error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best Practices Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best practices example\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union, Callable\n",
    "\n",
    "class BestPracticeModel(bst.graph.Node):\n",
    "    \"\"\"Model demonstrating typing best practices.\n",
    "    \n",
    "    This class shows:\n",
    "    - Complete type annotations\n",
    "    - Docstrings with type information\n",
    "    - Runtime validation\n",
    "    - Clear return types\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dims: List[int],\n",
    "        output_dim: int,\n",
    "        activation: Union[str, Callable[[jnp.ndarray], jnp.ndarray]] = 'relu',\n",
    "        dropout_rate: Optional[float] = None\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize model.\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Input feature dimension\n",
    "            hidden_dims: List of hidden layer dimensions\n",
    "            output_dim: Output dimension\n",
    "            activation: Activation function (name or callable)\n",
    "            dropout_rate: Dropout rate (None to disable)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store config\n",
    "        self.input_dim: int = input_dim\n",
    "        self.hidden_dims: List[int] = hidden_dims\n",
    "        self.output_dim: int = output_dim\n",
    "        \n",
    "        # Build architecture\n",
    "        all_dims: List[int] = [input_dim] + hidden_dims + [output_dim]\n",
    "        self.layers: List[bst.nn.Linear] = []\n",
    "        \n",
    "        for i, (in_d, out_d) in enumerate(zip(all_dims[:-1], all_dims[1:])):\n",
    "            layer: bst.nn.Linear = bst.nn.Linear(in_d, out_d)\n",
    "            self.layers.append(layer)\n",
    "            setattr(self, f'layer_{i}', layer)\n",
    "    \n",
    "    def forward(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, input_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, output_dim)\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If input shape is invalid\n",
    "        \"\"\"\n",
    "        # Validate\n",
    "        if x.shape[-1] != self.input_dim:\n",
    "            raise ValueError(\n",
    "                f\"Expected input dim {self.input_dim}, got {x.shape[-1]}\"\n",
    "            )\n",
    "        \n",
    "        # Process\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get model summary.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with model information\n",
    "        \"\"\"\n",
    "        n_params: int = sum(\n",
    "            p.value.size for p in self.states(bst.ParamState).values()\n",
    "        )\n",
    "        \n",
    "        summary: Dict[str, Any] = {\n",
    "            'input_dim': self.input_dim,\n",
    "            'hidden_dims': self.hidden_dims,\n",
    "            'output_dim': self.output_dim,\n",
    "            'n_layers': len(self.layers),\n",
    "            'n_parameters': n_params,\n",
    "        }\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Create and test\n",
    "model: BestPracticeModel = BestPracticeModel(\n",
    "    input_dim=10,\n",
    "    hidden_dims=[20, 15],\n",
    "    output_dim=5\n",
    ")\n",
    "\n",
    "summary: Dict[str, Any] = model.summary()\n",
    "print(\"Model Summary:\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in summary.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we covered:\n",
    "\n",
    "1. **Basic Type Annotations**: Using standard Python typing\n",
    "2. **Generic Types**: Union, Optional, Tuple, List, Dict\n",
    "3. **OneOfTypes**: BrainState's union-like types\n",
    "4. **JointTypes**: Composite type requirements\n",
    "5. **Neural Network Types**: Comprehensive typing for models\n",
    "6. **State Management Types**: Typed state handling\n",
    "7. **Type-Safe APIs**: Protocols and runtime checking\n",
    "8. **Validation**: Runtime type and shape validation\n",
    "9. **Best Practices**: Complete typing guidelines\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **Type annotations improve code quality** and catch bugs early\n",
    "- Use **Optional** for nullable values\n",
    "- Use **Union** for multiple type options\n",
    "- **Document types** in docstrings\n",
    "- **Validate inputs** at runtime when needed\n",
    "- **Protocol** enables structural typing\n",
    "- Types make **refactoring safer**\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. Always annotate function signatures\n",
    "2. Use Optional for nullable parameters\n",
    "3. Document types in docstrings\n",
    "4. Validate critical inputs at runtime\n",
    "5. Use Protocol for interface requirements\n",
    "6. Keep type annotations simple and readable\n",
    "7. Use typing for self-documentation\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next tutorial, we'll explore:\n",
    "- **Utility Functions**: filter, struct, PrettyObject, DictManager\n",
    "- Helper functions for common tasks\n",
    "- Pretty printing and visualization\n",
    "- Dictionary and structure management"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

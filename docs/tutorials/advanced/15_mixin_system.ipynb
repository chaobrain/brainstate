{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 15: Mixin System and Computation Modes\n",
    "\n",
    "In this tutorial, we'll explore BrainState's mixin system, which provides powerful mechanisms for controlling computation behavior through modes.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "- Understand the Mixin base class and its purpose\n",
    "- Use Mode for computation behavior control\n",
    "- Work with JointMode for combining modes\n",
    "- Apply Batching mode for batch operations\n",
    "- Use Training mode for train/eval switching\n",
    "- Create custom mixins for specialized behavior\n",
    "- Design mode-aware neural network components\n",
    "\n",
    "## What are Mixins?\n",
    "\n",
    "Mixins are a design pattern that allows classes to share behavior without traditional inheritance. In BrainState, mixins control:\n",
    "- **Computation modes**: Training vs evaluation, batching behavior\n",
    "- **Dynamic behavior**: Change functionality based on context\n",
    "- **Composability**: Combine multiple behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brainstate as bst\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed\n",
    "bst.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Mode System\n",
    "\n",
    "`Mode` is the foundation for controlling computation behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding Mode basics\n",
    "from brainstate.mixin import Mode\n",
    "\n",
    "# Mode controls behavior through flags\n",
    "print(\"Mode System Basics:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check current mode\n",
    "print(f\"Current fit mode: {bst.environ.get('fit', default=True)}\")\n",
    "\n",
    "# Change mode temporarily\n",
    "with bst.environ.context(fit=False):\n",
    "    print(f\"Inside context - fit mode: {bst.environ.get('fit')}\")\n",
    "\n",
    "print(f\"After context - fit mode: {bst.environ.get('fit', default=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode-Aware Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mode-aware dropout\n",
    "class ModeAwareDropout(bst.graph.Node):\n",
    "    \"\"\"Dropout that respects training mode.\"\"\"\n",
    "    \n",
    "    def __init__(self, rate=0.5):\n",
    "        super().__init__()\n",
    "        self.rate = rate\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Check if we're in training mode\n",
    "        is_training = bst.environ.get('fit', default=True)\n",
    "        \n",
    "        if is_training:\n",
    "            # Apply dropout during training\n",
    "            keep_prob = 1 - self.rate\n",
    "            mask = bst.random.bernoulli(keep_prob, x.shape)\n",
    "            return jnp.where(mask, x / keep_prob, 0)\n",
    "        else:\n",
    "            # No dropout during evaluation\n",
    "            return x\n",
    "\n",
    "# Test mode-aware dropout\n",
    "dropout = ModeAwareDropout(rate=0.5)\n",
    "x = jnp.ones((5, 10))\n",
    "\n",
    "print(\"Training mode (dropout active):\")\n",
    "with bst.environ.context(fit=True):\n",
    "    out_train = dropout(x)\n",
    "    print(f\"  Non-zero elements: {jnp.sum(out_train != 0)} / {out_train.size}\")\n",
    "    print(f\"  Mean: {jnp.mean(out_train):.3f} (should be ~1.0)\")\n",
    "\n",
    "print(\"\\nEvaluation mode (dropout disabled):\")\n",
    "with bst.environ.context(fit=False):\n",
    "    out_eval = dropout(x)\n",
    "    print(f\"  Non-zero elements: {jnp.sum(out_eval != 0)} / {out_eval.size}\")\n",
    "    print(f\"  Mean: {jnp.mean(out_eval):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training Mode\n",
    "\n",
    "The `Training` mixin provides train/eval mode switching functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training mode in practice\n",
    "class TrainingAwareNetwork(bst.graph.Node):\n",
    "    \"\"\"Network that behaves differently in train vs eval mode.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        self.linear1 = bst.nn.Linear(input_dim, hidden_dim)\n",
    "        self.dropout = bst.nn.Dropout(dropout_rate)\n",
    "        self.batchnorm = bst.nn.BatchNorm1d(hidden_dim)\n",
    "        self.linear2 = bst.nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.batchnorm(x)  # BatchNorm behaves differently in train/eval\n",
    "        x = jax.nn.relu(x)\n",
    "        x = self.dropout(x)    # Dropout only active in training\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "# Create network\n",
    "net = TrainingAwareNetwork(input_dim=10, hidden_dim=20, output_dim=5)\n",
    "x = bst.random.randn(32, 10)\n",
    "\n",
    "# Training mode\n",
    "print(\"Training Mode:\")\n",
    "with bst.environ.context(fit=True):\n",
    "    out_train1 = net(x)\n",
    "    out_train2 = net(x)\n",
    "    # Outputs differ due to dropout randomness\n",
    "    diff_train = jnp.mean(jnp.abs(out_train1 - out_train2))\n",
    "    print(f\"  Output variance (dropout): {diff_train:.4f}\")\n",
    "\n",
    "# Evaluation mode\n",
    "print(\"\\nEvaluation Mode:\")\n",
    "with bst.environ.context(fit=False):\n",
    "    out_eval1 = net(x)\n",
    "    out_eval2 = net(x)\n",
    "    # Outputs identical (deterministic)\n",
    "    diff_eval = jnp.mean(jnp.abs(out_eval1 - out_eval2))\n",
    "    print(f\"  Output variance (no dropout): {diff_eval:.4f}\")\n",
    "    print(f\"  Outputs are deterministic: {jnp.allclose(out_eval1, out_eval2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Context Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with mode switching\n",
    "def train_epoch(model, train_data, learning_rate=0.01):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    x_train, y_train = train_data\n",
    "    \n",
    "    # Ensure training mode\n",
    "    with bst.environ.context(fit=True):\n",
    "        def loss_fn():\n",
    "            pred = model(x_train)\n",
    "            return jnp.mean((pred - y_train) ** 2)\n",
    "        \n",
    "        loss, grads = bst.augment.grad(\n",
    "            loss_fn,\n",
    "            model.states(bst.ParamState),\n",
    "            return_value=True\n",
    "        )()\n",
    "        \n",
    "        # Update parameters\n",
    "        for name, grad in grads.items():\n",
    "            model.states()[name].value -= learning_rate * grad\n",
    "        \n",
    "        return float(loss)\n",
    "\n",
    "def evaluate(model, test_data):\n",
    "    \"\"\"Evaluate model.\"\"\"\n",
    "    x_test, y_test = test_data\n",
    "    \n",
    "    # Ensure evaluation mode\n",
    "    with bst.environ.context(fit=False):\n",
    "        pred = model(x_test)\n",
    "        loss = jnp.mean((pred - y_test) ** 2)\n",
    "        return float(loss)\n",
    "\n",
    "# Generate data\n",
    "x_train = bst.random.randn(100, 10)\n",
    "y_train = bst.random.randn(100, 5)\n",
    "x_test = bst.random.randn(20, 10)\n",
    "y_test = bst.random.randn(20, 5)\n",
    "\n",
    "# Train\n",
    "model = TrainingAwareNetwork(10, 20, 5, dropout_rate=0.2)\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(50):\n",
    "    train_loss = train_epoch(model, (x_train, y_train))\n",
    "    test_loss = evaluate(model, (x_test, y_test))\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Test Loss = {test_loss:.4f}\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(train_losses, label='Train Loss', alpha=0.7)\n",
    "plt.plot(test_losses, label='Test Loss', alpha=0.7)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training with Mode Switching')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Batching Mode\n",
    "\n",
    "The `Batching` mixin controls how layers handle batched inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batching mode example\n",
    "class BatchAwareLayer(bst.graph.Node):\n",
    "    \"\"\"Layer that adapts to batching mode.\"\"\"\n",
    "    \n",
    "    def __init__(self, features):\n",
    "        super().__init__()\n",
    "        self.features = features\n",
    "        self.weight = bst.ParamState(bst.random.randn(features, features) * 0.1)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Check batching mode\n",
    "        batch_mode = bst.environ.get('batch', default=True)\n",
    "        \n",
    "        if batch_mode:\n",
    "            # Batched computation: (batch, features) @ (features, features)\n",
    "            return x @ self.weight.value\n",
    "        else:\n",
    "            # Single sample: (features,) @ (features, features)\n",
    "            return x @ self.weight.value\n",
    "\n",
    "layer = BatchAwareLayer(features=5)\n",
    "\n",
    "# Test with batch\n",
    "x_batch = bst.random.randn(10, 5)\n",
    "print(\"Batched input:\")\n",
    "with bst.environ.context(batch=True):\n",
    "    out_batch = layer(x_batch)\n",
    "    print(f\"  Input shape: {x_batch.shape}\")\n",
    "    print(f\"  Output shape: {out_batch.shape}\")\n",
    "\n",
    "# Test with single sample\n",
    "x_single = bst.random.randn(5)\n",
    "print(\"\\nSingle sample:\")\n",
    "with bst.environ.context(batch=False):\n",
    "    out_single = layer(x_single)\n",
    "    print(f\"  Input shape: {x_single.shape}\")\n",
    "    print(f\"  Output shape: {out_single.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. JointMode: Combining Multiple Modes\n",
    "\n",
    "`JointMode` allows combining multiple behavioral modes simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple modes simultaneously\n",
    "class MultiModalLayer(bst.graph.Node):\n",
    "    \"\"\"Layer that responds to multiple mode flags.\"\"\"\n",
    "    \n",
    "    def __init__(self, features, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        self.features = features\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.weight = bst.ParamState(bst.random.randn(features, features) * 0.1)\n",
    "        self.call_count = bst.ShortTermState(jnp.array(0))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Get mode flags\n",
    "        is_training = bst.environ.get('fit', default=True)\n",
    "        is_batched = bst.environ.get('batch', default=True)\n",
    "        verbose = bst.environ.get('verbose', default=False)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  Mode: training={is_training}, batched={is_batched}\")\n",
    "        \n",
    "        # Increment counter\n",
    "        self.call_count.value += 1\n",
    "        \n",
    "        # Apply transformation\n",
    "        output = x @ self.weight.value\n",
    "        \n",
    "        # Apply dropout if training\n",
    "        if is_training:\n",
    "            keep_prob = 1 - self.dropout_rate\n",
    "            mask = bst.random.bernoulli(keep_prob, output.shape)\n",
    "            output = jnp.where(mask, output / keep_prob, 0)\n",
    "        \n",
    "        return output\n",
    "\n",
    "layer = MultiModalLayer(features=8)\n",
    "x = bst.random.randn(5, 8)\n",
    "\n",
    "# Test different mode combinations\n",
    "print(\"Mode Combinations:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Training + Batched\n",
    "print(\"\\n1. Training + Batched:\")\n",
    "with bst.environ.context(fit=True, batch=True, verbose=True):\n",
    "    out = layer(x)\n",
    "    print(f\"   Output shape: {out.shape}\")\n",
    "\n",
    "# Evaluation + Batched\n",
    "print(\"\\n2. Evaluation + Batched:\")\n",
    "with bst.environ.context(fit=False, batch=True, verbose=True):\n",
    "    out = layer(x)\n",
    "    print(f\"   Output shape: {out.shape}\")\n",
    "\n",
    "# Evaluation + Single\n",
    "print(\"\\n3. Evaluation + Single sample:\")\n",
    "with bst.environ.context(fit=False, batch=False, verbose=True):\n",
    "    out = layer(x[0])\n",
    "    print(f\"   Output shape: {out.shape}\")\n",
    "\n",
    "print(f\"\\nTotal calls to layer: {layer.call_count.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Custom Mixins\n",
    "\n",
    "Create custom mixins for specialized behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom mixin for debugging\n",
    "class DebugMode:\n",
    "    \"\"\"Mixin for debug-aware components.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def is_debug():\n",
    "        return bst.environ.get('debug', default=False)\n",
    "    \n",
    "    @staticmethod\n",
    "    def debug_print(msg):\n",
    "        if DebugMode.is_debug():\n",
    "            print(f\"[DEBUG] {msg}\")\n",
    "\n",
    "class DebugLayer(bst.graph.Node, DebugMode):\n",
    "    \"\"\"Layer with debug capabilities.\"\"\"\n",
    "    \n",
    "    def __init__(self, features):\n",
    "        super().__init__()\n",
    "        self.features = features\n",
    "        self.weight = bst.ParamState(bst.random.randn(features, features) * 0.1)\n",
    "        self.debug_print(f\"Initialized DebugLayer with {features} features\")\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.debug_print(f\"Input shape: {x.shape}\")\n",
    "        self.debug_print(f\"Weight stats: mean={jnp.mean(self.weight.value):.3f}, \"\n",
    "                        f\"std={jnp.std(self.weight.value):.3f}\")\n",
    "        \n",
    "        output = x @ self.weight.value\n",
    "        \n",
    "        self.debug_print(f\"Output shape: {output.shape}\")\n",
    "        self.debug_print(f\"Output stats: mean={jnp.mean(output):.3f}, \"\n",
    "                        f\"std={jnp.std(output):.3f}\")\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test without debug\n",
    "print(\"Normal mode (no debug output):\")\n",
    "layer = DebugLayer(features=5)\n",
    "x = bst.random.randn(3, 5)\n",
    "out = layer(x)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Debug mode enabled:\")\n",
    "with bst.environ.context(debug=True):\n",
    "    layer2 = DebugLayer(features=5)\n",
    "    out = layer2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Precision Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision mode mixin\n",
    "class PrecisionMode:\n",
    "    \"\"\"Control computation precision.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_dtype():\n",
    "        precision = bst.environ.get('precision', default='float32')\n",
    "        if precision == 'float16':\n",
    "            return jnp.float16\n",
    "        elif precision == 'float64':\n",
    "            return jnp.float64\n",
    "        else:\n",
    "            return jnp.float32\n",
    "\n",
    "class PrecisionAwareLayer(bst.graph.Node, PrecisionMode):\n",
    "    \"\"\"Layer that adapts to precision mode.\"\"\"\n",
    "    \n",
    "    def __init__(self, features):\n",
    "        super().__init__()\n",
    "        dtype = self.get_dtype()\n",
    "        self.weight = bst.ParamState(\n",
    "            bst.random.randn(features, features).astype(dtype) * 0.1\n",
    "        )\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        dtype = self.get_dtype()\n",
    "        x = x.astype(dtype)\n",
    "        return x @ self.weight.value\n",
    "\n",
    "# Test different precisions\n",
    "x = bst.random.randn(3, 4)\n",
    "\n",
    "print(\"Precision Modes:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for precision in ['float16', 'float32', 'float64']:\n",
    "    with bst.environ.context(precision=precision):\n",
    "        layer = PrecisionAwareLayer(features=4)\n",
    "        out = layer(x)\n",
    "        print(f\"{precision}: output dtype = {out.dtype}, \"\n",
    "              f\"memory = {out.nbytes} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Practical Example: Multi-Mode Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete example with multiple modes\n",
    "class ProductionNetwork(bst.graph.Node):\n",
    "    \"\"\"Production-ready network with multiple modes.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.layer1 = bst.nn.Linear(input_dim, hidden_dim)\n",
    "        self.bn1 = bst.nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout1 = bst.nn.Dropout(0.3)\n",
    "        \n",
    "        self.layer2 = bst.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bn2 = bst.nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout2 = bst.nn.Dropout(0.3)\n",
    "        \n",
    "        self.layer3 = bst.nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        # Statistics\n",
    "        self.forward_count = bst.ShortTermState(jnp.array(0))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Track calls\n",
    "        self.forward_count.value += 1\n",
    "        \n",
    "        # Debug info\n",
    "        if bst.environ.get('debug', default=False):\n",
    "            is_training = bst.environ.get('fit', default=True)\n",
    "            mode_str = \"TRAIN\" if is_training else \"EVAL\"\n",
    "            print(f\"[{mode_str}] Forward pass #{self.forward_count.value}\")\n",
    "        \n",
    "        # Layer 1\n",
    "        x = self.layer1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Layer 2\n",
    "        x = self.layer2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Output\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def reset_stats(self):\n",
    "        \"\"\"Reset statistics.\"\"\"\n",
    "        self.forward_count.value = jnp.array(0)\n",
    "\n",
    "# Create network\n",
    "net = ProductionNetwork(input_dim=20, hidden_dim=50, output_dim=10)\n",
    "x_train = bst.random.randn(32, 20)\n",
    "x_test = bst.random.randn(8, 20)\n",
    "\n",
    "print(\"Production Network Demo:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Training mode\n",
    "print(\"\\n1. Training (with debug):\")\n",
    "with bst.environ.context(fit=True, debug=True):\n",
    "    out_train = net(x_train)\n",
    "\n",
    "# Evaluation mode\n",
    "print(\"\\n2. Evaluation (with debug):\")\n",
    "with bst.environ.context(fit=False, debug=True):\n",
    "    out_test = net(x_test)\n",
    "\n",
    "# Silent evaluation\n",
    "print(\"\\n3. Silent evaluation:\")\n",
    "with bst.environ.context(fit=False, debug=False):\n",
    "    for _ in range(5):\n",
    "        _ = net(x_test)\n",
    "    print(f\"   Completed 5 forward passes silently\")\n",
    "\n",
    "print(f\"\\nTotal forward passes: {net.forward_count.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Mode Inheritance and Composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composing multiple custom modes\n",
    "class ProfilingMode:\n",
    "    \"\"\"Profiling mode for performance analysis.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def should_profile():\n",
    "        return bst.environ.get('profile', default=False)\n",
    "\n",
    "class LoggingMode:\n",
    "    \"\"\"Logging mode for tracking operations.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def should_log():\n",
    "        return bst.environ.get('log', default=False)\n",
    "\n",
    "class AdvancedLayer(bst.graph.Node, ProfilingMode, LoggingMode):\n",
    "    \"\"\"Layer with profiling and logging.\"\"\"\n",
    "    \n",
    "    def __init__(self, features):\n",
    "        super().__init__()\n",
    "        self.features = features\n",
    "        self.weight = bst.ParamState(bst.random.randn(features, features) * 0.1)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        import time\n",
    "        \n",
    "        # Logging\n",
    "        if self.should_log():\n",
    "            print(f\"[LOG] Processing input of shape {x.shape}\")\n",
    "        \n",
    "        # Profiling\n",
    "        if self.should_profile():\n",
    "            start = time.time()\n",
    "        \n",
    "        # Computation\n",
    "        output = x @ self.weight.value\n",
    "        \n",
    "        # Profiling\n",
    "        if self.should_profile():\n",
    "            elapsed = (time.time() - start) * 1000\n",
    "            print(f\"[PROFILE] Computation took {elapsed:.3f} ms\")\n",
    "        \n",
    "        # Logging\n",
    "        if self.should_log():\n",
    "            print(f\"[LOG] Output shape: {output.shape}\")\n",
    "        \n",
    "        return output\n",
    "\n",
    "layer = AdvancedLayer(features=100)\n",
    "x = bst.random.randn(50, 100)\n",
    "\n",
    "print(\"Normal mode:\")\n",
    "_ = layer(x)\n",
    "print(\"  (no output)\")\n",
    "\n",
    "print(\"\\nWith logging:\")\n",
    "with bst.environ.context(log=True):\n",
    "    _ = layer(x)\n",
    "\n",
    "print(\"\\nWith profiling:\")\n",
    "with bst.environ.context(profile=True):\n",
    "    _ = layer(x)\n",
    "\n",
    "print(\"\\nWith both:\")\n",
    "with bst.environ.context(log=True, profile=True):\n",
    "    _ = layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best Practices and Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best practice: Mode-aware configuration\n",
    "class ConfigurableModel(bst.graph.Node):\n",
    "    \"\"\"Model with mode-based configuration.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Build layers based on config\n",
    "        self.layers = []\n",
    "        for i, (in_dim, out_dim) in enumerate(zip(\n",
    "            config['layer_dims'][:-1],\n",
    "            config['layer_dims'][1:]\n",
    "        )):\n",
    "            layer = bst.nn.Linear(in_dim, out_dim)\n",
    "            self.layers.append(layer)\n",
    "            setattr(self, f'layer_{i}', layer)\n",
    "        \n",
    "        # Optional components based on config\n",
    "        if config.get('use_dropout', False):\n",
    "            self.dropout = bst.nn.Dropout(config['dropout_rate'])\n",
    "        else:\n",
    "            self.dropout = None\n",
    "        \n",
    "        if config.get('use_batchnorm', False):\n",
    "            self.batchnorms = [\n",
    "                bst.nn.BatchNorm1d(dim) \n",
    "                for dim in config['layer_dims'][1:-1]\n",
    "            ]\n",
    "            for i, bn in enumerate(self.batchnorms):\n",
    "                setattr(self, f'bn_{i}', bn)\n",
    "        else:\n",
    "            self.batchnorms = None\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            x = layer(x)\n",
    "            \n",
    "            # Optional batch norm\n",
    "            if self.batchnorms is not None:\n",
    "                x = self.batchnorms[i](x)\n",
    "            \n",
    "            x = jax.nn.relu(x)\n",
    "            \n",
    "            # Optional dropout\n",
    "            if self.dropout is not None:\n",
    "                x = self.dropout(x)\n",
    "        \n",
    "        # Final layer\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "\n",
    "# Different configurations\n",
    "configs = {\n",
    "    'simple': {\n",
    "        'layer_dims': [10, 20, 5],\n",
    "        'use_dropout': False,\n",
    "        'use_batchnorm': False\n",
    "    },\n",
    "    'regularized': {\n",
    "        'layer_dims': [10, 20, 15, 5],\n",
    "        'use_dropout': True,\n",
    "        'dropout_rate': 0.3,\n",
    "        'use_batchnorm': True\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Model Configurations:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, config in configs.items():\n",
    "    model = ConfigurableModel(config)\n",
    "    n_params = sum(p.value.size for p in model.states(bst.ParamState).values())\n",
    "    print(f\"\\n{name.capitalize()} model:\")\n",
    "    print(f\"  Layers: {config['layer_dims']}\")\n",
    "    print(f\"  Dropout: {config.get('use_dropout', False)}\")\n",
    "    print(f\"  BatchNorm: {config.get('use_batchnorm', False)}\")\n",
    "    print(f\"  Parameters: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we covered:\n",
    "\n",
    "1. **Mode System**: Foundation for controlling computation behavior\n",
    "2. **Training Mode**: Train/eval switching for dropout and batch norm\n",
    "3. **Batching Mode**: Handling batched vs single inputs\n",
    "4. **JointMode**: Combining multiple modes simultaneously\n",
    "5. **Custom Mixins**: Creating specialized behavior (debug, precision, profiling)\n",
    "6. **Production Patterns**: Multi-mode networks\n",
    "7. **Mode Composition**: Combining multiple mixins\n",
    "8. **Best Practices**: Configuration-based models\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **Modes control behavior** without changing code structure\n",
    "- Use **context managers** to temporarily change modes\n",
    "- **Training mode** affects dropout, batch normalization\n",
    "- **Custom mixins** enable specialized functionality\n",
    "- Modes are **composable** - combine multiple behaviors\n",
    "- **environ.context** is the primary interface for mode control\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. Always use context managers for temporary mode changes\n",
    "2. Check mode flags explicitly when behavior differs\n",
    "3. Document which modes your components respond to\n",
    "4. Create custom mixins for reusable behavior patterns\n",
    "5. Use meaningful mode names (fit, debug, profile, etc.)\n",
    "6. Design components to work correctly in all modes\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next tutorial, we'll explore:\n",
    "- **Type System**: OneOfTypes, JointTypes, type annotations\n",
    "- Type checking and validation\n",
    "- Best practices for type hints in neural networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Brainstate Tutorial\n\nThis tutorial demonstrates how to build stateful neural systems with [brainstate](https://github.com/chaobrain/brainstate). The walkthrough adapts several scripts from the `examples/` directory and shows how to compose modules, stage transformations with JAX, and handle checkpoints in practice."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Getting Ready\n\nInstall the core packages before running the notebook. On a CPU-only machine you can start with:\n\n```bash\npip install brainstate braintools jax[cpu] matplotlib orbax-checkpoint\n```"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport brainstate"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Functional API: Working with States and Graphs\n\nThis section mirrors `examples/001_functional_api.py`. We build an `MLP` node that tracks internal call counts, split its states into parameter and auxiliary collections, and train it with `jax.jit`-compiled functions."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "rng = np.random.default_rng(0)\nX = np.linspace(0.0, 1.0, 100)[:, None]\nY = 0.8 * X ** 2 + 0.1 + rng.normal(0.0, 0.1, size=X.shape)\n\n\ndef dataset(batch_size: int):\n    while True:\n        idx = rng.choice(len(X), size=batch_size, replace=True)\n        yield X[idx], Y[idx]\n\n\nclass Linear(brainstate.nn.Module):\n    def __init__(self, din: int, dout: int):\n        super().__init__()\n        self.w = brainstate.ParamState(brainstate.random.rand(din, dout))\n        self.b = brainstate.ParamState(jnp.zeros((dout,)))\n\n    def __call__(self, x):\n        return x @ self.w.value + self.b.value\n\n\nclass CallCount(brainstate.State):\n    pass\n\n\nclass RegressionMLP(brainstate.graph.Node):\n    def __init__(self, din: int, dhidden: int, dout: int):\n        self.count = CallCount(jnp.array(0))\n        self.linear1 = Linear(din, dhidden)\n        self.linear2 = Linear(dhidden, dout)\n\n    def __call__(self, x):\n        self.count.value += 1\n        x = self.linear1(x)\n        x = jax.nn.relu(x)\n        x = self.linear2(x)\n        return x"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "We separate the graph definition from its states using `treefy_split`, define training/evaluation steps, and then merge the updated states back into a full model for logging."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "graphdef, params, counts = brainstate.graph.treefy_split(\n    RegressionMLP(din=1, dhidden=32, dout=1), brainstate.ParamState, CallCount\n)\n\n\n@jax.jit\ndef train_step(param_states, count_states, batch):\n    x, y = batch\n\n    def loss_fn(pstates):\n        model = brainstate.graph.treefy_merge(graphdef, pstates, count_states)\n        y_pred = model(x)\n        new_counts = brainstate.graph.treefy_states(model, CallCount)\n        loss = jnp.mean((y - y_pred) ** 2)\n        return loss, new_counts\n\n    grads, count_states = jax.grad(loss_fn, has_aux=True)(param_states)\n    param_states = jax.tree.map(lambda w, g: w - 0.1 * g, param_states, grads)\n    return param_states, count_states\n\n\n@jax.jit\ndef eval_step(param_states, count_states, batch):\n    x, y = batch\n    model = brainstate.graph.treefy_merge(graphdef, param_states, count_states)\n    y_pred = model(x)\n    loss = jnp.mean((y - y_pred) ** 2)\n    return {'loss': loss}\n\n\nfor step, batch in zip(range(1000), dataset(32)):\n    params, counts = train_step(params, counts, batch)\n    if step % 200 == 0:\n        logs = eval_step(params, counts, (X, Y))\n        print(f\"step: {step}, loss: {logs['loss']:.4f}\")\n\nmodel = brainstate.graph.treefy_merge(graphdef, params, counts)\nprint(f\"call count: {model.count.value}\")\n\ny_pred = model(X)\nplt.scatter(X, Y, color='steelblue', label='data')\nplt.plot(X, y_pred, color='black', label='fit')\nplt.legend()\nplt.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Lifted Transforms and Optimizers\n\nHere we rework the regression example following `examples/002_lifted_transforms.py`. The model stays entirely inside a `brainstate.nn.Module`, and `brainstate.transform` decorators lift JAX transformations so that the state collections are updated automatically. We pair the model with `braintools.optim.SGD` to handle parameter updates."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import braintools\n\nclass LiftedMLP(brainstate.nn.Module):\n    def __init__(self, din: int, dhidden: int, dout: int):\n        super().__init__()\n        self.count = CallCount(jnp.array(0))\n        self.linear1 = Linear(din, dhidden)\n        self.linear2 = Linear(dhidden, dout)\n\n    def __call__(self, x):\n        self.count.value += 1\n        x = self.linear1(x)\n        x = jax.nn.relu(x)\n        x = self.linear2(x)\n        return x\n\nmodel2 = LiftedMLP(1, 32, 1)\noptimizer = braintools.optim.SGD(1e-3)\noptimizer.register_trainable_weights(model2.states(brainstate.ParamState))\n\n\n@brainstate.transform.jit\ndef train_step_lifted(batch):\n    x, y = batch\n\n    def loss_fn():\n        preds = model2(x)\n        return jnp.mean((y - preds) ** 2)\n\n    grads = brainstate.transform.grad(loss_fn, optimizer.param_states.to_pytree())()\n    optimizer.update(grads)\n\n\n@brainstate.transform.jit\ndef eval_step_lifted(batch):\n    x, y = batch\n    preds = model2(x)\n    loss = jnp.mean((y - preds) ** 2)\n    return {'loss': loss}\n\nfor step, batch in zip(range(1000), dataset(32)):\n    train_step_lifted(batch)\n    if step % 200 == 0:\n        logs = eval_step_lifted((X, Y))\n        print(f\"[lifted] step: {step}, loss: {logs['loss']:.4f}\")\n\nprint(f\"lifted call count: {model2.count.value}\")\nplt.scatter(X, Y, color='steelblue', label='data')\nplt.plot(X, model2(X), color='darkorange', label='lifted fit')\nplt.legend()\nplt.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Saving and Loading Checkpoints\n\nCheckpointing follows `examples/005_save_load_checkpoints.py`. We snapshot the full state tree with `orbax.checkpoint`, restore it into an abstractly initialised model, and resume inference without writing custom serialization logic."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import os\nfrom tempfile import TemporaryDirectory\n\nimport orbax.checkpoint as orbax\n\n\nclass CheckpointMLP(brainstate.nn.Module):\n    def __init__(self, din: int, dmid: int, dout: int):\n        super().__init__()\n        self.dense1 = brainstate.nn.Linear(din, dmid)\n        self.dense2 = brainstate.nn.Linear(dmid, dout)\n\n    def __call__(self, x):\n        x = self.dense1(x)\n        x = jax.nn.relu(x)\n        x = self.dense2(x)\n        return x\n\n\ndef create_model(seed: int):\n    brainstate.random.seed(seed)\n    return CheckpointMLP(10, 20, 30)\n\n\ndef create_and_save(seed: int, path: str):\n    model = create_model(seed)\n    state_tree = brainstate.graph.treefy_states(model)\n    checkpointer = orbax.PyTreeCheckpointer()\n    checkpointer.save(os.path.join(path, 'state'), state_tree)\n\n\ndef load_model(path: str) -> CheckpointMLP:\n    model = brainstate.transform.abstract_init(lambda: create_model(0))\n    state_tree = brainstate.graph.treefy_states(model)\n    checkpointer = orbax.PyTreeCheckpointer()\n    state_tree = checkpointer.restore(os.path.join(path, 'state'), item=state_tree)\n    brainstate.graph.update_states(model, state_tree)\n    return model\n\n\nwith TemporaryDirectory() as tmpdir:\n    create_and_save(42, tmpdir)\n    restored = load_model(tmpdir)\n    y = restored(jnp.ones((1, 10)))\n    print(restored)\n    print(y)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Where to Go Next\n\nExplore the rest of the `examples/` directory for domain-specific workflows:\n\n- `examples/100_hh_neuron_model.py` shows how to simulate detailed Hodgkin-Huxley neurons.\n- `examples/203_brainscale_for_snns.py` demonstrates brain-inspired training loops for spiking neural networks.\n- `examples/300_integrator_rnn.py` introduces rate-based recurrent models with integrator dynamics.\n\nThe [Brainstate documentation](https://brainstate.readthedocs.io/) expands on graph manipulation, transforms, and interoperability across the BrainX ecosystem."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 26: Debugging Tips and Techniques\n",
    "\n",
    "In this tutorial, we'll explore debugging strategies for BrainState and JAX code.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "- Understand common JAX/BrainState errors\n",
    "- Use JAX debugging tools effectively\n",
    "- Debug shape mismatches and type errors\n",
    "- Handle tracer errors\n",
    "- Identify and fix NaN/Inf issues\n",
    "- Debug JIT compilation problems\n",
    "- Use visualization for debugging\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Debugging functional code with transformations (JIT, vmap, grad) requires different approaches than traditional imperative debugging.\n",
    "\n",
    "Common challenges:\n",
    "- **Abstract tracers**: Values during JIT compilation\n",
    "- **Shape errors**: Dimension mismatches\n",
    "- **Type errors**: Incompatible dtypes\n",
    "- **NaN/Inf propagation**: Numerical instability\n",
    "- **State management**: Tracking state updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brainstate as bst\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Any\n",
    "\n",
    "bst.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Common Errors and Solutions\n",
    "\n",
    "### 1.1 Shape Mismatch Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Error 1: Shape Mismatch\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ❌ WRONG: Shape mismatch\n",
    "print(\"\\n❌ Common mistake:\")\n",
    "print(\"\"\"\n",
    "x = jnp.array([[1, 2, 3]])  # Shape: (1, 3)\n",
    "y = jnp.array([1, 2])       # Shape: (2,)\n",
    "result = x + y  # ERROR: Incompatible shapes!\n",
    "\"\"\")\n",
    "\n",
    "# ✓ SOLUTION 1: Check shapes before operations\n",
    "print(\"✓ Solution 1: Check shapes\")\n",
    "x = jnp.array([[1, 2, 3]])\n",
    "y = jnp.array([1, 2])\n",
    "\n",
    "print(f\"x.shape = {x.shape}\")\n",
    "print(f\"y.shape = {y.shape}\")\n",
    "print(\"Shapes incompatible for addition!\")\n",
    "\n",
    "# ✓ SOLUTION 2: Reshape to match\n",
    "print(\"\\n✓ Solution 2: Reshape\")\n",
    "y_reshaped = jnp.array([1, 2, 3])  # Match x's second dimension\n",
    "result = x + y_reshaped\n",
    "print(f\"x.shape = {x.shape}\")\n",
    "print(f\"y_reshaped.shape = {y_reshaped.shape}\")\n",
    "print(f\"result.shape = {result.shape}\")\n",
    "print(f\"result = {result}\")\n",
    "\n",
    "# ✓ SOLUTION 3: Use broadcasting correctly\n",
    "print(\"\\n✓ Solution 3: Broadcasting\")\n",
    "x = jnp.array([[1, 2, 3], [4, 5, 6]])  # (2, 3)\n",
    "y = jnp.array([10, 20, 30])            # (3,)\n",
    "result = x + y  # Broadcasting: (2, 3) + (3,) -> (2, 3)\n",
    "print(f\"x.shape = {x.shape}\")\n",
    "print(f\"y.shape = {y.shape}\")\n",
    "print(f\"result.shape = {result.shape}\")\n",
    "print(f\"result = \\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Tracer Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nError 2: Tracer Errors\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ❌ WRONG: Using traced values in Python control flow\n",
    "print(\"\\n❌ Common mistake:\")\n",
    "print(\"\"\"\n",
    "@jax.jit\n",
    "def wrong_function(x):\n",
    "    if x > 0:  # ERROR: Can't use traced value in if!\n",
    "        return x * 2\n",
    "    else:\n",
    "        return x * 3\n",
    "\"\"\")\n",
    "\n",
    "# ✓ SOLUTION 1: Use jax.lax.cond\n",
    "print(\"✓ Solution 1: Use jax.lax.cond\")\n",
    "@jax.jit\n",
    "def correct_function(x):\n",
    "    return jax.lax.cond(\n",
    "        x > 0,\n",
    "        lambda x: x * 2,\n",
    "        lambda x: x * 3,\n",
    "        x\n",
    "    )\n",
    "\n",
    "result = correct_function(5.0)\n",
    "print(f\"correct_function(5.0) = {result}\")\n",
    "\n",
    "result = correct_function(-3.0)\n",
    "print(f\"correct_function(-3.0) = {result}\")\n",
    "\n",
    "# ✓ SOLUTION 2: Use jnp.where for element-wise conditionals\n",
    "print(\"\\n✓ Solution 2: Use jnp.where\")\n",
    "@jax.jit\n",
    "def vectorized_function(x):\n",
    "    return jnp.where(x > 0, x * 2, x * 3)\n",
    "\n",
    "x = jnp.array([-2, -1, 0, 1, 2])\n",
    "result = vectorized_function(x)\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"Output: {result}\")\n",
    "\n",
    "# ✓ SOLUTION 3: Use static_argnums for control flow arguments\n",
    "print(\"\\n✓ Solution 3: static_argnums\")\n",
    "@jax.jit(static_argnums=(1,))\n",
    "def function_with_static(x, mode):\n",
    "    if mode == \"double\":\n",
    "        return x * 2\n",
    "    else:\n",
    "        return x * 3\n",
    "\n",
    "print(f\"function_with_static(5.0, 'double') = {function_with_static(5.0, 'double')}\")\n",
    "print(f\"function_with_static(5.0, 'triple') = {function_with_static(5.0, 'triple')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 NaN and Inf Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Error 3: NaN and Inf Values\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Common sources of NaN/Inf\n",
    "print(\"\\nCommon sources:\")\n",
    "print(\"1. Division by zero\")\n",
    "print(\"2. Logarithm of zero or negative numbers\")\n",
    "print(\"3. Numerical overflow\")\n",
    "print(\"4. Invalid mathematical operations\")\n",
    "\n",
    "# Detection utilities\n",
    "def check_nan_inf(x: jnp.ndarray, name: str = \"tensor\") -> bool:\n",
    "    \"\"\"\n",
    "    Check for NaN or Inf values.\n",
    "    \n",
    "    Args:\n",
    "        x: Array to check\n",
    "        name: Name for error message\n",
    "        \n",
    "    Returns:\n",
    "        True if any NaN or Inf found\n",
    "    \"\"\"\n",
    "    has_nan = jnp.any(jnp.isnan(x))\n",
    "    has_inf = jnp.any(jnp.isinf(x))\n",
    "    \n",
    "    if has_nan:\n",
    "        print(f\"⚠️  WARNING: {name} contains NaN values!\")\n",
    "        return True\n",
    "    if has_inf:\n",
    "        print(f\"⚠️  WARNING: {name} contains Inf values!\")\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Example: Division by zero\n",
    "print(\"\\nExample 1: Division by zero\")\n",
    "x = jnp.array([1.0, 2.0, 3.0])\n",
    "y = jnp.array([2.0, 0.0, 1.0])\n",
    "result = x / y\n",
    "print(f\"x / y = {result}\")\n",
    "check_nan_inf(result, \"division result\")\n",
    "\n",
    "# Solution: Add epsilon\n",
    "print(\"\\n✓ Solution: Add small epsilon\")\n",
    "epsilon = 1e-8\n",
    "result_safe = x / (y + epsilon)\n",
    "print(f\"x / (y + epsilon) = {result_safe}\")\n",
    "check_nan_inf(result_safe, \"safe division\")\n",
    "\n",
    "# Example: Log of zero\n",
    "print(\"\\nExample 2: Logarithm of zero\")\n",
    "x = jnp.array([1.0, 0.0, 2.0])\n",
    "result = jnp.log(x)\n",
    "print(f\"log(x) = {result}\")\n",
    "check_nan_inf(result, \"log result\")\n",
    "\n",
    "# Solution: Clipping\n",
    "print(\"\\n✓ Solution: Clip values\")\n",
    "result_safe = jnp.log(jnp.maximum(x, epsilon))\n",
    "print(f\"log(max(x, epsilon)) = {result_safe}\")\n",
    "check_nan_inf(result_safe, \"safe log\")\n",
    "\n",
    "# Enable NaN debugging\n",
    "print(\"\\n✓ Enable JAX NaN checking (for development):\")\n",
    "print(\"\"\"\n",
    "# At the start of your script:\n",
    "jax.config.update('jax_debug_nans', True)\n",
    "\n",
    "# This will raise an error immediately when NaN is created\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Type Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Error 4: Type Mismatches\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ❌ Common mistake: Mixing dtypes\n",
    "print(\"\\n❌ Common mistake:\")\n",
    "x_int = jnp.array([1, 2, 3], dtype=jnp.int32)\n",
    "y_float = jnp.array([1.5, 2.5, 3.5], dtype=jnp.float32)\n",
    "print(f\"x dtype: {x_int.dtype}\")\n",
    "print(f\"y dtype: {y_float.dtype}\")\n",
    "\n",
    "result = x_int + y_float  # Implicit cast\n",
    "print(f\"result dtype: {result.dtype}\")\n",
    "print(f\"result: {result}\")\n",
    "\n",
    "# ✓ Best practice: Explicit casting\n",
    "print(\"\\n✓ Solution: Explicit casting\")\n",
    "x_float = x_int.astype(jnp.float32)\n",
    "result = x_float + y_float\n",
    "print(f\"result dtype: {result.dtype}\")\n",
    "print(f\"result: {result}\")\n",
    "\n",
    "# Check dtypes\n",
    "def check_dtype(x: jnp.ndarray, expected_dtype: jnp.dtype, name: str = \"tensor\"):\n",
    "    \"\"\"Check if array has expected dtype.\"\"\"\n",
    "    if x.dtype != expected_dtype:\n",
    "        print(f\"⚠️  WARNING: {name} has dtype {x.dtype}, expected {expected_dtype}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "print(\"\\nType checking:\")\n",
    "check_dtype(x_int, jnp.float32, \"x_int\")\n",
    "check_dtype(y_float, jnp.float32, \"y_float\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. JAX Debugging Tools\n",
    "\n",
    "### 2.1 Debug Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"JAX Debug Printing\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Problem: Regular print doesn't work inside JIT\n",
    "print(\"\\n❌ Regular print in JIT:\")\n",
    "@jax.jit\n",
    "def function_with_print(x):\n",
    "    print(f\"x = {x}\")  # This prints during compilation, not execution!\n",
    "    return x * 2\n",
    "\n",
    "print(\"First call (compiles):\")\n",
    "result = function_with_print(5.0)\n",
    "print(\"\\nSecond call (cached):\")\n",
    "result = function_with_print(10.0)\n",
    "print(\"Notice: print only happened during compilation!\\n\")\n",
    "\n",
    "# ✓ Solution: jax.debug.print\n",
    "print(\"✓ Solution: jax.debug.print\")\n",
    "@jax.jit\n",
    "def function_with_debug_print(x):\n",
    "    jax.debug.print(\"x = {}\", x)\n",
    "    return x * 2\n",
    "\n",
    "print(\"First call:\")\n",
    "result = function_with_debug_print(5.0)\n",
    "print(\"\\nSecond call:\")\n",
    "result = function_with_debug_print(10.0)\n",
    "print(\"\\njax.debug.print works during execution!\")\n",
    "\n",
    "# Advanced: Conditional debug printing\n",
    "print(\"\\n✓ Conditional debug printing:\")\n",
    "@jax.jit\n",
    "def debug_conditional(x):\n",
    "    jax.debug.print(\"Input: x = {}\", x)\n",
    "    result = jnp.sqrt(x)\n",
    "    jax.debug.print(\"Output: sqrt(x) = {}\", result)\n",
    "    \n",
    "    # Print only if result is large\n",
    "    jax.lax.cond(\n",
    "        result > 5.0,\n",
    "        lambda: jax.debug.print(\"⚠️  Large result: {}\", result),\n",
    "        lambda: None,\n",
    "    )\n",
    "    return result\n",
    "\n",
    "print(\"\\nTesting with small input:\")\n",
    "_ = debug_conditional(4.0)\n",
    "print(\"\\nTesting with large input:\")\n",
    "_ = debug_conditional(36.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Disabling JIT for Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Disabling JIT for Debugging\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "@jax.jit\n",
    "def complex_function(x):\n",
    "    # Complex computation\n",
    "    y = jnp.sin(x)\n",
    "    z = jnp.exp(y)\n",
    "    return jnp.sum(z)\n",
    "\n",
    "# Option 1: Temporarily disable JIT globally\n",
    "print(\"\\nOption 1: Disable JIT globally\")\n",
    "print(\"with jax.disable_jit():\")\n",
    "print(\"    result = complex_function(x)\")\n",
    "print(\"    # Now you can use print(), debugger, etc.\")\n",
    "\n",
    "x = jnp.array([1.0, 2.0, 3.0])\n",
    "\n",
    "with jax.disable_jit():\n",
    "    result = complex_function(x)\n",
    "    print(f\"Result (no JIT): {result}\")\n",
    "\n",
    "# Option 2: Comment out @jax.jit decorator\n",
    "print(\"\\nOption 2: Remove decorator temporarily\")\n",
    "print(\"# @jax.jit  # Commented out\")\n",
    "print(\"def complex_function(x):\")\n",
    "print(\"    ...\")\n",
    "\n",
    "# Option 3: Conditional JIT\n",
    "print(\"\\nOption 3: Conditional JIT\")\n",
    "DEBUG = True\n",
    "\n",
    "def maybe_jit(func):\n",
    "    if DEBUG:\n",
    "        return func\n",
    "    else:\n",
    "        return jax.jit(func)\n",
    "\n",
    "@maybe_jit\n",
    "def debuggable_function(x):\n",
    "    print(f\"Debug: x = {x}\")  # Works when DEBUG=True\n",
    "    return x * 2\n",
    "\n",
    "result = debuggable_function(5.0)\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Inspecting Computation Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Inspecting JAX Computations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def simple_function(x, y):\n",
    "    return jnp.sin(x) + jnp.cos(y)\n",
    "\n",
    "# Get the jaxpr (JAX expression)\n",
    "print(\"\\nJAXPR (JAX intermediate representation):\")\n",
    "x = jnp.array(1.0)\n",
    "y = jnp.array(2.0)\n",
    "\n",
    "jaxpr = jax.make_jaxpr(simple_function)(x, y)\n",
    "print(jaxpr)\n",
    "\n",
    "print(\"\\n✓ Use make_jaxpr to understand what JAX is doing internally\")\n",
    "print(\"  Useful for:\")\n",
    "print(\"  - Understanding transformations\")\n",
    "print(\"  - Debugging shape inference\")\n",
    "print(\"  - Verifying optimizations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Debugging Model Code\n",
    "\n",
    "### 3.1 Shape Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Shape Debugging\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class DebuggableModel(bst.graph.Node):\n",
    "    \"\"\"Model with shape debugging.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, debug=True):\n",
    "        super().__init__()\n",
    "        self.fc1 = bst.nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = bst.nn.Linear(hidden_dim, output_dim)\n",
    "        self.debug = debug\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        if self.debug:\n",
    "            print(f\"Input shape: {x.shape}\")\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        if self.debug:\n",
    "            print(f\"After fc1: {x.shape}\")\n",
    "        \n",
    "        x = jax.nn.relu(x)\n",
    "        if self.debug:\n",
    "            print(f\"After relu: {x.shape}\")\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        if self.debug:\n",
    "            print(f\"Output shape: {x.shape}\")\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test with debug mode\n",
    "print(\"\\nWith debug=True:\")\n",
    "model = DebuggableModel(10, 20, 5, debug=True)\n",
    "x = bst.random.randn(3, 10)\n",
    "output = model(x)\n",
    "\n",
    "# Turn off debug for production\n",
    "print(\"\\nWith debug=False:\")\n",
    "model.debug = False\n",
    "output = model(x)\n",
    "print(f\"Final output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Gradient Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gradient Debugging\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class SimpleModel(bst.graph.Node):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = bst.nn.Linear(10, 5)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel()\n",
    "x = bst.random.randn(2, 10)\n",
    "_ = model(x)\n",
    "\n",
    "# Define loss\n",
    "def loss_fn(x):\n",
    "    output = model(x)\n",
    "    return jnp.sum(output ** 2)\n",
    "\n",
    "# Compute gradients\n",
    "params = model.states(bst.ParamState)\n",
    "loss, grads = bst.transform.grad(loss_fn, grad_states=params, return_value=True)(x)\n",
    "\n",
    "print(\"\\nGradient Statistics:\")\n",
    "for name, grad in grads.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Shape: {grad.shape}\")\n",
    "    print(f\"  Mean: {jnp.mean(grad):.6f}\")\n",
    "    print(f\"  Std: {jnp.std(grad):.6f}\")\n",
    "    print(f\"  Max: {jnp.max(jnp.abs(grad)):.6f}\")\n",
    "    \n",
    "    # Check for problems\n",
    "    if jnp.any(jnp.isnan(grad)):\n",
    "        print(\"  ⚠️  Contains NaN!\")\n",
    "    if jnp.any(jnp.isinf(grad)):\n",
    "        print(\"  ⚠️  Contains Inf!\")\n",
    "    if jnp.max(jnp.abs(grad)) > 10.0:\n",
    "        print(\"  ⚠️  Potentially exploding gradient!\")\n",
    "    if jnp.max(jnp.abs(grad)) < 1e-7:\n",
    "        print(\"  ⚠️  Potentially vanishing gradient!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 State Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"State Debugging\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class StatefulModel(bst.graph.Node):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.fc = bst.nn.Linear(size, size)\n",
    "        self.hidden = bst.ShortTermState(jnp.zeros(size))\n",
    "        self.counter = bst.LongTermState(jnp.array(0))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Update counter\n",
    "        self.counter.value = self.counter.value + 1\n",
    "        \n",
    "        # Update hidden state\n",
    "        new_hidden = jax.nn.tanh(self.fc(x) + self.hidden.value)\n",
    "        self.hidden.value = new_hidden\n",
    "        \n",
    "        return new_hidden\n",
    "    \n",
    "    def debug_states(self):\n",
    "        \"\"\"Print state information.\"\"\"\n",
    "        print(\"\\nState Debug Info:\")\n",
    "        print(f\"Counter: {self.counter.value}\")\n",
    "        print(f\"Hidden state:\")\n",
    "        print(f\"  Shape: {self.hidden.value.shape}\")\n",
    "        print(f\"  Mean: {jnp.mean(self.hidden.value):.6f}\")\n",
    "        print(f\"  Std: {jnp.std(self.hidden.value):.6f}\")\n",
    "        print(f\"  Range: [{jnp.min(self.hidden.value):.6f}, {jnp.max(self.hidden.value):.6f}]\")\n",
    "\n",
    "# Test stateful model\n",
    "model = StatefulModel(size=10)\n",
    "x = bst.random.randn(1, 10)\n",
    "\n",
    "print(\"Initial state:\")\n",
    "model.debug_states()\n",
    "\n",
    "for i in range(3):\n",
    "    _ = model(x)\n",
    "    print(f\"\\nAfter step {i+1}:\")\n",
    "    model.debug_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization for Debugging\n",
    "\n",
    "### 4.1 Weight Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Visualizing Weight Distributions\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def visualize_weights(model: bst.graph.Node):\n",
    "    \"\"\"Visualize weight distributions.\"\"\"\n",
    "    params = model.states(bst.ParamState)\n",
    "    \n",
    "    num_params = len(params)\n",
    "    fig, axes = plt.subplots(1, min(num_params, 4), figsize=(15, 3))\n",
    "    if num_params == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (name, param) in enumerate(list(params.items())[:4]):\n",
    "        ax = axes[idx]\n",
    "        weights = param.value.flatten()\n",
    "        \n",
    "        ax.hist(np.array(weights), bins=50, alpha=0.7, edgecolor='black')\n",
    "        ax.set_title(f\"{name}\\nMean: {jnp.mean(weights):.4f}\")\n",
    "        ax.set_xlabel('Value')\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add statistics\n",
    "        ax.axvline(jnp.mean(weights), color='red', linestyle='--', \n",
    "                   linewidth=2, label='Mean')\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create and visualize model\n",
    "model = SimpleModel()\n",
    "x = bst.random.randn(1, 10)\n",
    "_ = model(x)\n",
    "\n",
    "print(\"\\nWeight distributions:\")\n",
    "visualize_weights(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Activation Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Visualizing Activations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class ActivationTracker(bst.graph.Node):\n",
    "    \"\"\"Model that tracks activations.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = bst.nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = bst.nn.Linear(hidden_dim, output_dim)\n",
    "        self.activations = []\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.activations = []\n",
    "        \n",
    "        self.activations.append(('input', x))\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        self.activations.append(('fc1_pre', x))\n",
    "        \n",
    "        x = jax.nn.relu(x)\n",
    "        self.activations.append(('fc1_post', x))\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        self.activations.append(('output', x))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def plot_activations(self):\n",
    "        \"\"\"Plot activation statistics.\"\"\"\n",
    "        names = [name for name, _ in self.activations]\n",
    "        means = [float(jnp.mean(act)) for _, act in self.activations]\n",
    "        stds = [float(jnp.std(act)) for _, act in self.activations]\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        x_pos = range(len(names))\n",
    "        \n",
    "        ax1.bar(x_pos, means, alpha=0.7, edgecolor='black')\n",
    "        ax1.set_xticks(x_pos)\n",
    "        ax1.set_xticklabels(names, rotation=45, ha='right')\n",
    "        ax1.set_ylabel('Mean Activation')\n",
    "        ax1.set_title('Mean Activations')\n",
    "        ax1.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        ax2.bar(x_pos, stds, alpha=0.7, color='orange', edgecolor='black')\n",
    "        ax2.set_xticks(x_pos)\n",
    "        ax2.set_xticklabels(names, rotation=45, ha='right')\n",
    "        ax2.set_ylabel('Std Activation')\n",
    "        ax2.set_title('Activation Standard Deviation')\n",
    "        ax2.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Test activation tracking\n",
    "model = ActivationTracker(10, 20, 5)\n",
    "x = bst.random.randn(32, 10)\n",
    "output = model(x)\n",
    "\n",
    "print(\"\\nActivation statistics:\")\n",
    "for name, act in model.activations:\n",
    "    print(f\"{name:12s}: shape={str(act.shape):12s} \"\n",
    "          f\"mean={jnp.mean(act):7.4f} std={jnp.std(act):7.4f}\")\n",
    "\n",
    "model.plot_activations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Debugging Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Debugging Checklist\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "checklist = [\n",
    "    (\"Shape Errors\", [\n",
    "        \"Print shapes at each step\",\n",
    "        \"Check input/output dimensions\",\n",
    "        \"Verify broadcasting behavior\",\n",
    "        \"Use .shape attribute liberally\",\n",
    "    ]),\n",
    "    (\"Type Errors\", [\n",
    "        \"Check dtypes (float32, int32, etc.)\",\n",
    "        \"Explicit casting when needed\",\n",
    "        \"Consistent types throughout\",\n",
    "    ]),\n",
    "    (\"NaN/Inf Issues\", [\n",
    "        \"Enable jax_debug_nans during development\",\n",
    "        \"Check for division by zero\",\n",
    "        \"Validate log inputs (> 0)\",\n",
    "        \"Monitor gradient magnitudes\",\n",
    "        \"Use gradient clipping\",\n",
    "    ]),\n",
    "    (\"JIT/Tracer Errors\", [\n",
    "        \"Use jax.lax.cond instead of if\",\n",
    "        \"Use jax.lax.fori_loop instead of for\",\n",
    "        \"Mark control flow args as static\",\n",
    "        \"Use jax.debug.print for JIT\",\n",
    "        \"Disable JIT temporarily with disable_jit()\",\n",
    "    ]),\n",
    "    (\"State Management\", [\n",
    "        \"Verify state updates happen\",\n",
    "        \"Check state types (Param vs ShortTerm)\",\n",
    "        \"Reset states when needed\",\n",
    "        \"Track state statistics\",\n",
    "    ]),\n",
    "    (\"Performance Issues\", [\n",
    "        \"Profile code to find bottlenecks\",\n",
    "        \"Check for unnecessary recompilation\",\n",
    "        \"Verify JIT is being used\",\n",
    "        \"Monitor memory usage\",\n",
    "    ]),\n",
    "]\n",
    "\n",
    "for category, items in checklist:\n",
    "    print(f\"\\n{category}:\")\n",
    "    for item in items:\n",
    "        print(f\"  ✓ {item}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Quick Debug Commands:\")\n",
    "print(\"=\" * 80)\n",
    "commands = [\n",
    "    (\"Check shape\", \"print(f'Shape: {x.shape}')\"),\n",
    "    (\"Check dtype\", \"print(f'Dtype: {x.dtype}')\"),\n",
    "    (\"Check for NaN\", \"assert not jnp.any(jnp.isnan(x))\"),\n",
    "    (\"Check for Inf\", \"assert not jnp.any(jnp.isinf(x))\"),\n",
    "    (\"Check range\", \"print(f'Range: [{jnp.min(x)}, {jnp.max(x)}]')\"),\n",
    "    (\"Debug in JIT\", \"jax.debug.print('Value: {}', x)\"),\n",
    "    (\"Disable JIT\", \"with jax.disable_jit(): ...\"),\n",
    "    (\"Enable NaN check\", \"jax.config.update('jax_debug_nans', True)\"),\n",
    "]\n",
    "\n",
    "for desc, cmd in commands:\n",
    "    print(f\"{desc:20s}: {cmd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we covered:\n",
    "\n",
    "1. **Common Errors**:\n",
    "   - Shape mismatches and broadcasting\n",
    "   - Tracer errors in JIT\n",
    "   - NaN and Inf detection\n",
    "   - Type mismatches\n",
    "\n",
    "2. **JAX Debugging Tools**:\n",
    "   - jax.debug.print for JIT\n",
    "   - Disabling JIT temporarily\n",
    "   - Inspecting JAXPRs\n",
    "\n",
    "3. **Model Debugging**:\n",
    "   - Shape debugging\n",
    "   - Gradient checking\n",
    "   - State monitoring\n",
    "\n",
    "4. **Visualization**:\n",
    "   - Weight distributions\n",
    "   - Activation patterns\n",
    "   - Statistical analysis\n",
    "\n",
    "5. **Debugging Checklist**:\n",
    "   - Systematic approach\n",
    "   - Quick commands\n",
    "   - Best practices\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- **Use jax.debug.print** inside JIT functions\n",
    "- **Check shapes early and often**\n",
    "- **Enable NaN debugging** during development\n",
    "- **Disable JIT temporarily** for detailed debugging\n",
    "- **Visualize** weights and activations\n",
    "- **Monitor gradients** for training issues\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Practice debugging on your own code\n",
    "- Build debugging utilities for your workflow\n",
    "- Learn JAX internals for advanced debugging\n",
    "\n",
    "For more information:\n",
    "- [JAX Debugging Guide](https://jax.readthedocs.io/en/latest/debugging/index.html)\n",
    "- [BrainState Documentation](https://brainstate.readthedocs.io/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

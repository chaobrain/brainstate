{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Collective Operations in `brainstate.nn`\n\nThe `brainstate.nn._collective_ops` module provides helpers for managing *all* modules inside a model. These functions make it easy to initialise, reset, batch, and restore stateful objects without manually traversing the module graph. This notebook introduces the core APIs with practical examples.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Prerequisites\n\n- Familiarity with `brainstate.nn` modules and states\n- `brainunit` installed (required by the BrainState package)\n- Basic understanding of JAX and `vmap`\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "import brainstate\nimport jax.numpy as jnp\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Overview of the API\n\n`brainstate.nn._collective_ops` exposes several utilities:\n\n- `call_order` \u2014 decorator that fixes the execution order of methods\n- `call_all_fns` / `vmap_call_all_fns` \u2014 call the same method on each node in a model\n- `init_all_states` / `vmap_init_all_states` \u2014 initialise state variables everywhere\n- `reset_all_states` / `vmap_reset_all_states` \u2014 reset existing states\n- `assign_state_values` \u2014 restore state values from dictionaries keyed by absolute paths\n\nWe'll examine each group below.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Ordering Calls with `call_order`\n\nBy default `call_all_fns` respects the order that nodes appear in the graph, but complex modules may need explicit ordering. The `call_order` decorator attaches a `call_order` attribute to any method; lower levels run first.\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "class EncoderDecoder(brainstate.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = brainstate.nn.Linear((16,), (32,))\n        self.decoder = brainstate.nn.Linear((32,), (16,))\n\n    @brainstate.nn.call_order(0)\n    def init_state(self):\n        self.encoder.init_state()\n        self.decoder.init_state()\n\n    @brainstate.nn.call_order(1)\n    def reset_state(self):\n        self.encoder.reset_state()\n        self.decoder.reset_state()\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Even though `EncoderDecoder` simply forwards the calls, the decorator ensures that collective utilities honour the order when visiting child modules.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Initialising Every Module\n\nThe simplest helper is `init_all_states`. It walks the module graph and calls `init_state` on each node. You can pass keyword arguments and exclude specific nodes when necessary.\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "model = brainstate.nn.Sequential(\n    brainstate.nn.Linear((10,), (32,)),\n    brainstate.nn.GELU(),\n    brainstate.nn.Dropout(prob=0.1)\n)\n\n# Initialise the entire stack at once.\nbrainstate.nn.init_all_states(model, batch_size=4)\n\n# Exclude stateless nodes via a filter (here: Dropout layer).\nbrainstate.nn.init_all_states(model, node_to_exclude=brainstate.nn.Dropout)\n\n# Because the function returns the target, you can chain it during construction.\nmodel = brainstate.nn.init_all_states(model)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Resetting State Between Sequences\n\nFor recurrent models you often initialise once and then reset after processing a sequence. `reset_all_states` automates the reset pass across the entire module.\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "rnn = brainstate.nn.ValinaRNNCell(num_in=8, num_out=16)\nbrainstate.nn.init_all_states(rnn, batch_size=2)\n\n# ... run some inference / training ...\n\n# Reset hidden states before the next sequence.\nbrainstate.nn.reset_all_states(rnn)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "You can exclude nodes or pass additional arguments just like `init_all_states`. The decorator-driven order still applies, so you can reset buffers before hidden states if needed.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Batched Initialisation with `vmap_*`\n\nTo create multiple independent instances of a model (ensembles or Monte-Carlo batches), use the vectorised variants. They insert a leading axis and manage separate random keys for each copy.\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "policy = brainstate.nn.Sequential(\n    brainstate.nn.Linear((4,), (64,)),\n    brainstate.nn.GELU(),\n    brainstate.nn.Linear((64,), (2,))\n)\n\n# Create 8 independent versions of the policy.\nbrainstate.nn.vmap_init_all_states(policy, axis_size=8)\n\n# Parameters gain an extra axis on the leading dimension.\nweights = policy.layers['Linear_0'].weight.value\nprint('Weight shape with batching:', weights['weight'].shape)\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# When finished with a rollout, reset all batched states at once.\nbrainstate.nn.vmap_reset_all_states(policy, axis_size=8)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "If certain states should stay shared (for example statistics buffers), pass a `state_to_exclude` filter to `vmap_init_all_states`. Excluded states retain their original shape across the batch.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Calling Arbitrary Methods Collectively\n\n`call_all_fns` is the primitive behind the init/reset helpers. You can dispatch *any* method, provided that each child module implements it.\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "class LoggingLayer(brainstate.nn.Module):\n    def __init__(self, size):\n        super().__init__()\n        self.linear = brainstate.nn.Linear((size,), (size,))\n        self.logged = []\n\n    def init_state(self):\n        self.linear.init_state()\n\n    def log_stats(self):\n        weight = self.linear.weight.value['weight']\n        self.logged.append(jnp.mean(weight))\n\nnet = brainstate.nn.Sequential(\n    LoggingLayer(size=8),\n    LoggingLayer(size=8)\n)\n\nbrainstate.nn.init_all_states(net)\nbrainstate.nn.call_all_fns(net, 'log_stats')\n\nstats = [layer.logged for layer in net.layers.values()]\nprint('Logged means per layer:', stats)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Use `vmap_call_all_fns` to repeat the same method across `axis_size` independent instances. It shares the interface and filter options.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Restoring States with `assign_state_values`\n\nSerialisation often involves mapping absolute state names back to objects. The `assign_state_values` helper performs the updates and returns any mismatched keys.\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "autoencoder = brainstate.nn.Sequential(\n    brainstate.nn.Linear((16,), (8,)),\n    brainstate.nn.ReLU(),\n    brainstate.nn.Linear((8,), (16,))\n)\nbrainstate.nn.init_all_states(autoencoder)\n\n# Save values in a dict keyed by absolute state paths.\nstate_snapshot = {\n    path: state.value\n    for path, state in autoencoder.states().items()\n}\n\n# ... modify weights or states ...\n\nunexpected, missing = brainstate.nn.assign_state_values(autoencoder, state_snapshot)\nprint('Unexpected keys:', unexpected)\nprint('Missing keys:', missing)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Putting It All Together\n\nThe snippet below demonstrates a typical lifecycle for a batched recurrent network: initialise, perform computation, reset, and restore weights.\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "rnn = brainstate.nn.ValinaRNNCell(num_in=4, num_out=8)\nbrainstate.nn.vmap_init_all_states(rnn, axis_size=4, batch_size=1)\n\n# Save a snapshot of initial states.\nsnapshot = {path: state.value for path, state in rnn.states().items()}\n\n# Simulate a rollout.\ninputs = brainstate.random.randn(12, 4)\nfor t in range(inputs.shape[0]):\n    _ = rnn(inputs[t])\n\n# Reset before the next episode.\nbrainstate.nn.vmap_reset_all_states(rnn, axis_size=4)\n\n# Restore parameters and hidden states.\nbrainstate.nn.assign_state_values(rnn, snapshot)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Best Practices\n\n- Always call `init_all_states` once after constructing a module.\n- Decorate stateful methods with `call_order` when their interaction matters.\n- Use filters (`node_to_exclude`, `state_to_exclude`) to fine-tune traversal.\n- Inspect the return values from `assign_state_values` to catch mismatched checkpoints.\n- Employ the vmapped helpers for ensembles but remember the added leading axis.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Further Reading\n\n- [Module Basics](01_module_basics.ipynb)\n- [Recurrent Networks](04_recurrent_networks.ipynb)\n- API reference: `brainstate.nn._collective_ops`\n"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
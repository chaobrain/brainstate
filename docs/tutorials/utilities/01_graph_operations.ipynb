{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 14: Computation Graph and Node System\n",
    "\n",
    "In this tutorial, we'll explore BrainState's computation graph system based on the `graph.Node` class, which provides the foundation for building complex neural network architectures.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "- Understand the graph.Node base class and its role\n",
    "- Build computation graphs with nested modules\n",
    "- Manage state in graph structures\n",
    "- Traverse and inspect computation graphs\n",
    "- Optimize graph operations\n",
    "- Create custom graph nodes and operators\n",
    "- Implement advanced architectures using the graph system\n",
    "\n",
    "## What is the Graph System?\n",
    "\n",
    "BrainState's graph system provides:\n",
    "- **Hierarchical structure**: Organize modules in tree-like graphs\n",
    "- **State management**: Automatic tracking of parameters and state\n",
    "- **Composability**: Combine simple nodes into complex networks\n",
    "- **Introspection**: Query and analyze model structure"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T15:42:53.823485Z",
     "start_time": "2025-10-10T15:42:52.192668Z"
    }
   },
   "source": [
    "import brainstate\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Set random seed\n",
    "brainstate.random.seed(42)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The graph.Node Base Class\n",
    "\n",
    "All BrainState modules inherit from `graph.Node`, which provides core functionality for the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T15:42:54.095931Z",
     "start_time": "2025-10-10T15:42:53.823485Z"
    }
   },
   "source": [
    "# Basic Node example\n",
    "class SimpleNode(brainstate.graph.Node):\n",
    "    \"\"\"A simple node with parameters and state.\"\"\"\n",
    "    \n",
    "    def __init__(self, size: int):\n",
    "        super().__init__()\n",
    "        # Parameters (trainable)\n",
    "        self.weight = brainstate.ParamState(brainstate.random.randn(size))\n",
    "        self.bias = brainstate.ParamState(jnp.zeros(size))\n",
    "        \n",
    "        # Hidden state (non-trainable, changes during computation)\n",
    "        self.activation = brainstate.ShortTermState(jnp.zeros(size))\n",
    "        \n",
    "        # Regular attributes\n",
    "        self.size = size\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Compute output\n",
    "        output = x * self.weight.value + self.bias.value\n",
    "        # Update hidden state\n",
    "        self.activation.value = output\n",
    "        return output\n",
    "\n",
    "# Create and use\n",
    "node = SimpleNode(size=5)\n",
    "x = jnp.ones(5)\n",
    "output = node(x)\n",
    "\n",
    "print(f\"Output: {output}\")\n",
    "print(f\"\\nNode attributes:\")\n",
    "print(f\"  weight: {node.weight.value.shape}\")\n",
    "print(f\"  bias: {node.bias.value.shape}\")\n",
    "print(f\"  activation: {node.activation.value.shape}\")\n",
    "print(f\"  size: {node.size}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: [ 0.60576403  0.7990441  -0.908927   -0.63525754 -1.2226585 ]\n",
      "\n",
      "Node attributes:\n",
      "  weight: (5,)\n",
      "  bias: (5,)\n",
      "  activation: (5,)\n",
      "  size: 5\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node Features"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T15:42:54.721124Z",
     "start_time": "2025-10-10T15:42:54.113990Z"
    }
   },
   "source": [
    "# Explore Node capabilities\n",
    "print(\"=\" * 60)\n",
    "print(\"Node State Management\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get all states\n",
    "all_states = node.states()\n",
    "print(f\"\\nAll states: {list(all_states.keys())}\")\n",
    "\n",
    "# Get only parameters\n",
    "params = node.states(brainstate.ParamState)\n",
    "print(f\"Parameters: {list(params.keys())}\")\n",
    "\n",
    "# Get only hidden states\n",
    "hidden = node.states(brainstate.ShortTermState)\n",
    "print(f\"Hidden states: {list(hidden.keys())}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(s.value.size for s in params.values())\n",
    "print(f\"\\nTotal parameters: {total_params}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Node State Management\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SimpleNode' object has no attribute 'states'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      4\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m=\u001B[39m\u001B[33m\"\u001B[39m * \u001B[32m60\u001B[39m)\n\u001B[32m      6\u001B[39m \u001B[38;5;66;03m# Get all states\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m all_states = node.states()\n\u001B[32m      8\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mAll states: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlist\u001B[39m(all_states.keys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     10\u001B[39m \u001B[38;5;66;03m# Get only parameters\u001B[39;00m\n",
      "\u001B[31mAttributeError\u001B[39m: 'SimpleNode' object has no attribute 'states'"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Hierarchical Graphs\n",
    "\n",
    "Nodes can contain other nodes, creating a hierarchical computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical model\n",
    "class Layer(brainstate.graph.Node):\n",
    "    \"\"\"A single layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.linear = brainstate.nn.Linear(in_features, out_features)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return jnp.tanh(self.linear(x))\n",
    "\n",
    "class MLP(brainstate.graph.Node):\n",
    "    \"\"\"Multi-layer perceptron with hierarchical structure.\"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes):\n",
    "        super().__init__()\n",
    "        self.layers = []\n",
    "        \n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            layer = Layer(\n",
    "                layer_sizes[i], \n",
    "                layer_sizes[i + 1],\n",
    "                name=f'layer_{i}'\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "            # Register as attribute for proper graph construction\n",
    "            setattr(self, f'layer_{i}', layer)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# Create hierarchical model\n",
    "model = MLP([10, 20, 15, 5])\n",
    "x = brainstate.random.randn(3, 10)\n",
    "output = model(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nModel structure:\")\n",
    "print(f\"  Number of layers: {len(model.layers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Traversal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traverse the computation graph\n",
    "def print_graph_structure(node, prefix=\"\", is_last=True):\n",
    "    \"\"\"Recursively print graph structure.\"\"\"\n",
    "    marker = \"└── \" if is_last else \"├── \"\n",
    "    node_name = node.__class__.__name__\n",
    "    \n",
    "    # Get node identifier\n",
    "    if hasattr(node, 'name') and node.name:\n",
    "        node_id = f\"{node_name}(name='{node.name}')\"\n",
    "    else:\n",
    "        node_id = node_name\n",
    "    \n",
    "    print(f\"{prefix}{marker}{node_id}\")\n",
    "    \n",
    "    # Get child nodes\n",
    "    children = []\n",
    "    for attr_name in dir(node):\n",
    "        if attr_name.startswith('_'):\n",
    "            continue\n",
    "        try:\n",
    "            attr = getattr(node, attr_name)\n",
    "            if isinstance(attr, brainstate.graph.Node):\n",
    "                children.append((attr_name, attr))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Print children\n",
    "    extension = \"    \" if is_last else \"│   \"\n",
    "    for i, (name, child) in enumerate(children):\n",
    "        is_last_child = (i == len(children) - 1)\n",
    "        print_graph_structure(child, prefix + extension, is_last_child)\n",
    "\n",
    "print(\"Graph Structure:\")\n",
    "print(\"=\" * 60)\n",
    "print_graph_structure(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. State Collection and Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect states from hierarchical model\n",
    "print(\"State Collection:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# All parameters in the model\n",
    "all_params = model.states(brainstate.ParamState)\n",
    "print(f\"\\nTotal parameter tensors: {len(all_params)}\")\n",
    "print(\"\\nParameter details:\")\n",
    "for name, param in list(all_params.items())[:5]:  # Show first 5\n",
    "    print(f\"  {name}: shape={param.value.shape}, size={param.value.size}\")\n",
    "\n",
    "# Calculate total parameters\n",
    "total_params = sum(p.value.size for p in all_params.values())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Memory (float32): {total_params * 4 / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and restore states\n",
    "class StatefulModel(brainstate.graph.Node):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = brainstate.nn.Linear(5, 3)\n",
    "        self.counter = brainstate.ShortTermState(jnp.array(0))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.counter.value += 1\n",
    "        return self.linear(x)\n",
    "\n",
    "model = StatefulModel()\n",
    "\n",
    "# Use the model\n",
    "x = brainstate.random.randn(2, 5)\n",
    "y1 = model(x)\n",
    "y2 = model(x)\n",
    "print(f\"Counter after 2 calls: {model.counter.value}\")\n",
    "\n",
    "# Save state\n",
    "saved_states = {}\n",
    "for name, state in model.states().items():\n",
    "    saved_states[name] = state.value.copy()\n",
    "\n",
    "print(f\"\\nSaved {len(saved_states)} states\")\n",
    "\n",
    "# Continue using\n",
    "for _ in range(5):\n",
    "    model(x)\n",
    "print(f\"Counter after 7 total calls: {model.counter.value}\")\n",
    "\n",
    "# Restore state\n",
    "for name, value in saved_states.items():\n",
    "    model.states()[name].value = value\n",
    "\n",
    "print(f\"Counter after restoration: {model.counter.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Graph Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip Connections (ResNet-style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(brainstate.graph.Node):\n",
    "    \"\"\"Residual block with skip connection.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = brainstate.nn.Linear(dim, dim)\n",
    "        self.linear2 = brainstate.nn.Linear(dim, dim)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Main path\n",
    "        residual = x\n",
    "        x = jax.nn.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        # Skip connection\n",
    "        return jax.nn.relu(x + residual)\n",
    "\n",
    "class ResNet(brainstate.graph.Node):\n",
    "    \"\"\"Simple ResNet with multiple residual blocks.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim, n_blocks):\n",
    "        super().__init__()\n",
    "        self.blocks = [ResidualBlock(dim) for _ in range(n_blocks)]\n",
    "        # Register blocks\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            setattr(self, f'block_{i}', block)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "\n",
    "# Test ResNet\n",
    "resnet = ResNet(dim=10, n_blocks=3)\n",
    "x = brainstate.random.randn(5, 10)\n",
    "output = resnet(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nResNet structure:\")\n",
    "print_graph_structure(resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Path Networks (Inception-style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionBlock(brainstate.graph.Node):\n",
    "    \"\"\"Inception-style block with multiple parallel paths.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        # Path 1: 1x1 conv\n",
    "        self.path1 = brainstate.nn.Linear(in_dim, out_dim // 4)\n",
    "        \n",
    "        # Path 2: 1x1 -> 3x3 (simulated with linear)\n",
    "        self.path2_a = brainstate.nn.Linear(in_dim, out_dim // 4)\n",
    "        self.path2_b = brainstate.nn.Linear(out_dim // 4, out_dim // 4)\n",
    "        \n",
    "        # Path 3: 1x1 -> 5x5 (simulated)\n",
    "        self.path3_a = brainstate.nn.Linear(in_dim, out_dim // 4)\n",
    "        self.path3_b = brainstate.nn.Linear(out_dim // 4, out_dim // 4)\n",
    "        \n",
    "        # Path 4: pool -> 1x1\n",
    "        self.path4 = brainstate.nn.Linear(in_dim, out_dim // 4)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Execute all paths\n",
    "        out1 = jax.nn.relu(self.path1(x))\n",
    "        \n",
    "        out2 = jax.nn.relu(self.path2_a(x))\n",
    "        out2 = jax.nn.relu(self.path2_b(out2))\n",
    "        \n",
    "        out3 = jax.nn.relu(self.path3_a(x))\n",
    "        out3 = jax.nn.relu(self.path3_b(out3))\n",
    "        \n",
    "        out4 = jax.nn.relu(self.path4(x))\n",
    "        \n",
    "        # Concatenate outputs\n",
    "        return jnp.concatenate([out1, out2, out3, out4], axis=-1)\n",
    "\n",
    "# Test Inception block\n",
    "inception = InceptionBlock(in_dim=16, out_dim=32)\n",
    "x = brainstate.random.randn(4, 16)\n",
    "output = inception(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nInception block has {len(inception.states(brainstate.ParamState))} parameter tensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dynamic Graphs and Conditional Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveDepthNetwork(brainstate.graph.Node):\n",
    "    \"\"\"Network that adapts its depth based on input.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim, max_depth=5):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "        # Create all possible layers\n",
    "        self.layers = []\n",
    "        for i in range(max_depth):\n",
    "            layer = brainstate.nn.Linear(dim, dim)\n",
    "            self.layers.append(layer)\n",
    "            setattr(self, f'layer_{i}', layer)\n",
    "        \n",
    "        # Confidence predictor\n",
    "        self.confidence = brainstate.nn.Linear(dim, 1)\n",
    "    \n",
    "    def __call__(self, x, confidence_threshold=0.9):\n",
    "        \"\"\"Process input, stopping early if confident.\"\"\"\n",
    "        depth_used = 0\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = jax.nn.relu(layer(x))\n",
    "            depth_used = i + 1\n",
    "            \n",
    "            # Check confidence (simplified)\n",
    "            conf = jax.nn.sigmoid(self.confidence(x))\n",
    "            \n",
    "            # In practice, you'd use jax.lax.cond for JIT compatibility\n",
    "            if jnp.mean(conf) > confidence_threshold:\n",
    "                break\n",
    "        \n",
    "        return x, depth_used\n",
    "\n",
    "# Test adaptive network\n",
    "adaptive_net = AdaptiveDepthNetwork(dim=8, max_depth=5)\n",
    "\n",
    "# Easy input (should stop early)\n",
    "x_easy = jnp.ones((3, 8)) * 0.1\n",
    "out_easy, depth_easy = adaptive_net(x_easy, confidence_threshold=0.5)\n",
    "\n",
    "# Hard input (may use more layers)\n",
    "x_hard = brainstate.random.randn(3, 8) * 2\n",
    "out_hard, depth_hard = adaptive_net(x_hard, confidence_threshold=0.9)\n",
    "\n",
    "print(f\"Easy input used {depth_easy} layers\")\n",
    "print(f\"Hard input used {depth_hard} layers\")\n",
    "print(f\"\\nAdaptive execution saves computation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Graph Optimization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze graph structure\n",
    "def analyze_graph(node, prefix=\"root\"):\n",
    "    \"\"\"Analyze computation graph statistics.\"\"\"\n",
    "    stats = {\n",
    "        'total_nodes': 0,\n",
    "        'total_params': 0,\n",
    "        'param_tensors': 0,\n",
    "        'node_types': {},\n",
    "        'depth': 0\n",
    "    }\n",
    "    \n",
    "    def traverse(n, depth=0):\n",
    "        stats['total_nodes'] += 1\n",
    "        stats['depth'] = max(stats['depth'], depth)\n",
    "        \n",
    "        # Count node type\n",
    "        node_type = n.__class__.__name__\n",
    "        stats['node_types'][node_type] = stats['node_types'].get(node_type, 0) + 1\n",
    "        \n",
    "        # Count parameters\n",
    "        params = n.states(brainstate.ParamState)\n",
    "        stats['param_tensors'] += len(params)\n",
    "        stats['total_params'] += sum(p.value.size for p in params.values())\n",
    "        \n",
    "        # Traverse children\n",
    "        for attr_name in dir(n):\n",
    "            if attr_name.startswith('_'):\n",
    "                continue\n",
    "            try:\n",
    "                attr = getattr(n, attr_name)\n",
    "                if isinstance(attr, brainstate.graph.Node):\n",
    "                    traverse(attr, depth + 1)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    traverse(node)\n",
    "    return stats\n",
    "\n",
    "# Analyze different models\n",
    "models = {\n",
    "    'MLP': MLP([10, 20, 15, 5]),\n",
    "    'ResNet': ResNet(dim=10, n_blocks=3),\n",
    "    'Inception': InceptionBlock(in_dim=16, out_dim=32)\n",
    "}\n",
    "\n",
    "print(\"Graph Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Model':<15} {'Nodes':<8} {'Depth':<8} {'Param Tensors':<15} {'Total Params':<15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, model in models.items():\n",
    "    stats = analyze_graph(model)\n",
    "    print(f\"{name:<15} {stats['total_nodes']:<8} {stats['depth']:<8} \"\n",
    "          f\"{stats['param_tensors']:<15} {stats['total_params']:<15,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Detailed analysis of one model\n",
    "print(\"\\nDetailed Analysis of ResNet:\")\n",
    "resnet_stats = analyze_graph(models['ResNet'])\n",
    "print(f\"  Node types: {resnet_stats['node_types']}\")\n",
    "print(f\"  Memory footprint: {resnet_stats['total_params'] * 4 / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Custom Graph Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter initialization across graph\n",
    "def initialize_graph(node, init_fn):\n",
    "    \"\"\"Initialize all parameters in graph with given function.\"\"\"\n",
    "    params = node.states(brainstate.ParamState)\n",
    "    for name, param in params.items():\n",
    "        param.value = init_fn(param.value.shape)\n",
    "    return node\n",
    "\n",
    "# Test initialization\n",
    "model = MLP([5, 10, 5])\n",
    "\n",
    "# Custom initialization: Xavier/Glorot\n",
    "def xavier_init(shape):\n",
    "    if len(shape) == 1:\n",
    "        return jnp.zeros(shape)\n",
    "    fan_in, fan_out = shape[0], shape[-1]\n",
    "    limit = jnp.sqrt(6.0 / (fan_in + fan_out))\n",
    "    return brainstate.random.uniform(-limit, limit, shape)\n",
    "\n",
    "# Before initialization\n",
    "params_before = model.states(brainstate.ParamState)\n",
    "sample_param_before = list(params_before.values())[0].value\n",
    "print(f\"Before initialization (random):\")\n",
    "print(f\"  Sample mean: {jnp.mean(sample_param_before):.4f}\")\n",
    "print(f\"  Sample std: {jnp.std(sample_param_before):.4f}\")\n",
    "\n",
    "# Initialize\n",
    "initialize_graph(model, xavier_init)\n",
    "\n",
    "# After initialization\n",
    "params_after = model.states(brainstate.ParamState)\n",
    "sample_param_after = list(params_after.values())[0].value\n",
    "print(f\"\\nAfter Xavier initialization:\")\n",
    "print(f\"  Sample mean: {jnp.mean(sample_param_after):.4f}\")\n",
    "print(f\"  Sample std: {jnp.std(sample_param_after):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze/Unfreeze Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter freezing for transfer learning\n",
    "class FreezableModel(brainstate.graph.Node):\n",
    "    \"\"\"Model with freezable layers.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = brainstate.nn.Linear(10, 20, name='features')\n",
    "        self.classifier = brainstate.nn.Linear(20, 5, name='classifier')\n",
    "        self.frozen_params = set()\n",
    "    \n",
    "    def freeze_features(self):\n",
    "        \"\"\"Freeze feature extractor parameters.\"\"\"\n",
    "        feature_params = self.feature_extractor.states(brainstate.ParamState)\n",
    "        for name in feature_params.keys():\n",
    "            self.frozen_params.add(id(feature_params[name]))\n",
    "    \n",
    "    def get_trainable_params(self):\n",
    "        \"\"\"Get only non-frozen parameters.\"\"\"\n",
    "        all_params = self.states(brainstate.ParamState)\n",
    "        return {k: v for k, v in all_params.items() \n",
    "                if id(v) not in self.frozen_params}\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = jax.nn.relu(self.feature_extractor(x))\n",
    "        return self.classifier(x)\n",
    "\n",
    "# Test freezing\n",
    "model = FreezableModel()\n",
    "\n",
    "print(\"Before freezing:\")\n",
    "print(f\"  Total params: {len(model.states(brainstate.ParamState))}\")\n",
    "print(f\"  Trainable params: {len(model.get_trainable_params())}\")\n",
    "\n",
    "# Freeze features\n",
    "model.freeze_features()\n",
    "\n",
    "print(\"\\nAfter freezing features:\")\n",
    "print(f\"  Total params: {len(model.states(brainstate.ParamState))}\")\n",
    "print(f\"  Trainable params: {len(model.get_trainable_params())}\")\n",
    "print(f\"  Trainable param names: {list(model.get_trainable_params().keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Practical Example: Neural Architecture Search (NAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple NAS with different layer types\n",
    "class SearchableBlock(brainstate.graph.Node):\n",
    "    \"\"\"Block that can choose between different operations.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        # Multiple operation choices\n",
    "        self.ops = {\n",
    "            'linear': brainstate.nn.Linear(dim, dim),\n",
    "            'skip': lambda x: x,\n",
    "            'zero': lambda x: jnp.zeros_like(x),\n",
    "        }\n",
    "        \n",
    "        # Architecture parameters (which op to use)\n",
    "        self.arch_params = brainstate.ParamState(jnp.ones(len(self.ops)) / len(self.ops))\n",
    "        \n",
    "        # Register linear op for state tracking\n",
    "        self.linear = self.ops['linear']\n",
    "    \n",
    "    def __call__(self, x, mode='soft'):\n",
    "        \"\"\"Execute block.\n",
    "        \n",
    "        mode='soft': weighted combination of all ops\n",
    "        mode='hard': use best op only\n",
    "        \"\"\"\n",
    "        if mode == 'soft':\n",
    "            # Soft selection: weighted combination\n",
    "            weights = jax.nn.softmax(self.arch_params.value)\n",
    "            output = jnp.zeros_like(x)\n",
    "            \n",
    "            for i, (name, op) in enumerate(self.ops.items()):\n",
    "                output += weights[i] * op(x)\n",
    "            \n",
    "            return output\n",
    "        else:\n",
    "            # Hard selection: use best op\n",
    "            best_idx = jnp.argmax(self.arch_params.value)\n",
    "            ops_list = list(self.ops.values())\n",
    "            return ops_list[best_idx](x)\n",
    "\n",
    "class SearchableNetwork(brainstate.graph.Node):\n",
    "    \"\"\"Network with searchable architecture.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim, n_blocks):\n",
    "        super().__init__()\n",
    "        self.blocks = [SearchableBlock(dim) for _ in range(n_blocks)]\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            setattr(self, f'block_{i}', block)\n",
    "    \n",
    "    def __call__(self, x, mode='soft'):\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mode=mode)\n",
    "        return x\n",
    "    \n",
    "    def get_architecture(self):\n",
    "        \"\"\"Get discovered architecture.\"\"\"\n",
    "        arch = []\n",
    "        op_names = list(self.blocks[0].ops.keys())\n",
    "        \n",
    "        for i, block in enumerate(self.blocks):\n",
    "            weights = jax.nn.softmax(block.arch_params.value)\n",
    "            best_op = op_names[jnp.argmax(weights)]\n",
    "            arch.append((i, best_op, float(jnp.max(weights))))\n",
    "        \n",
    "        return arch\n",
    "\n",
    "# Create searchable network\n",
    "nas_net = SearchableNetwork(dim=8, n_blocks=3)\n",
    "x = brainstate.random.randn(4, 8)\n",
    "\n",
    "# Simulate architecture search (random for demo)\n",
    "for block in nas_net.blocks:\n",
    "    # Randomly prefer different operations\n",
    "    block.arch_params.value = brainstate.random.randn(len(block.ops))\n",
    "\n",
    "# Get discovered architecture\n",
    "architecture = nas_net.get_architecture()\n",
    "\n",
    "print(\"Discovered Architecture:\")\n",
    "print(\"=\" * 60)\n",
    "for block_id, op_name, confidence in architecture:\n",
    "    print(f\"  Block {block_id}: {op_name:<10} (confidence: {confidence:.2%})\")\n",
    "\n",
    "# Test both modes\n",
    "out_soft = nas_net(x, mode='soft')\n",
    "out_hard = nas_net(x, mode='hard')\n",
    "\n",
    "print(f\"\\nOutput shape (soft): {out_soft.shape}\")\n",
    "print(f\"Output shape (hard): {out_hard.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we covered:\n",
    "\n",
    "1. **graph.Node Base Class**: Foundation of BrainState's module system\n",
    "2. **Hierarchical Graphs**: Building complex nested structures\n",
    "3. **State Management**: Collecting and manipulating states across graphs\n",
    "4. **Advanced Patterns**: ResNet, Inception, adaptive depth networks\n",
    "5. **Graph Analysis**: Introspection and statistics\n",
    "6. **Custom Operations**: Initialization, freezing, graph manipulation\n",
    "7. **Practical Applications**: Neural architecture search\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **graph.Node** provides hierarchical structure and state management\n",
    "- States are automatically tracked across the graph\n",
    "- Graphs can be **traversed, analyzed, and manipulated**\n",
    "- **Skip connections** and **multi-path networks** are easy to implement\n",
    "- Graph operations enable **transfer learning** and **NAS**\n",
    "- The graph system is **composable** and **flexible**\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. Always call `super().__init__()` in custom nodes\n",
    "2. Register child nodes as attributes for proper graph construction\n",
    "3. Use descriptive names for better debugging\n",
    "4. Leverage state types (ParamState, ShortTermState) appropriately\n",
    "5. Design for composability - small, reusable components\n",
    "6. Document your graph structure for clarity\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next tutorial, we'll explore:\n",
    "- **Mixin System**: Mode, JointMode, Batching, Training\n",
    "- Computation modes for different behaviors\n",
    "- Custom mixins for specialized functionality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

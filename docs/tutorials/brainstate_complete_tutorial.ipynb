{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Brainstate Comprehensive Tutorial\n\nThis notebook curates every script under `examples/` into a single, end-to-end guide. Each section summarises the original program, explains the key Brainstate features it showcases, and provides runnable notebook snippets or references back to the full script for deeper study."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## How to Use This Notebook\n\n- Install the optional dependencies that individual sections require (see each notes block).\n- Toggle the runtime flags in the next cell to control whether expensive simulations or training loops execute.\n- When a section only references the original script, open the file path locally for the full code listing."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from pathlib import Path\n\nRUN_TRAINING = False       # Enable to execute longer optimisation loops (001, 002, 003, 300)\nRUN_NEURO_SIM = False      # Enable biophysical simulations (100-series, braincell, brainpy examples)\nRUN_BRAINSCALE = False     # Enable large brainscale training demos (203, 301)\nDATA_DIR = Path('notebook_data')\nDATA_DIR.mkdir(exist_ok=True)\nprint('Flags set. Toggle to run heavier workloads as needed.')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 0xx — Deep Neural Network Foundations\n\nThe 0xx scripts demonstrate core neural-network workflows on top of Brainstate's stateful module system. They read like Flax or Haiku examples but lean on `brainstate.graph` and `brainstate.transform` utilities."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Example 001 — Functional API Regression (`examples/001_functional_api.py`)\n\n**Highlights**\n- Splitting a stateful graph into parameter and auxiliary collections with `treefy_split`.\n- Re-merging the graph inside JAX-transformed training/evaluation steps.\n- Managing non-trainable counters via custom `brainstate.State` subclasses."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport brainstate\n\nrng = np.random.default_rng(0)\nX = np.linspace(0.0, 1.0, 100)[:, None]\nY = 0.8 * X ** 2 + 0.1 + rng.normal(0.0, 0.1, size=X.shape)\n\ndef dataset(batch_size: int):\n    while True:\n        idx = rng.choice(len(X), size=batch_size, replace=True)\n        yield X[idx], Y[idx]\n\nclass Linear(brainstate.nn.Module):\n    def __init__(self, din: int, dout: int):\n        super().__init__()\n        self.w = brainstate.ParamState(brainstate.random.rand(din, dout))\n        self.b = brainstate.ParamState(jnp.zeros((dout,)))\n\n    def __call__(self, x):\n        return x @ self.w.value + self.b.value\n\nclass CallCount(brainstate.State):\n    pass\n\nclass RegressionMLP(brainstate.graph.Node):\n    def __init__(self, din: int, dhidden: int, dout: int):\n        self.count = CallCount(jnp.array(0))\n        self.linear1 = Linear(din, dhidden)\n        self.linear2 = Linear(dhidden, dout)\n\n    def __call__(self, x):\n        self.count.value += 1\n        x = self.linear1(x)\n        x = jax.nn.relu(x)\n        return self.linear2(x)\n\ngraphdef, params, counts = brainstate.graph.treefy_split(\n    RegressionMLP(1, 32, 1), brainstate.ParamState, CallCount\n)\n\n@jax.jit\ndef train_step(param_states, count_states, batch):\n    x, y = batch\n\n    def loss_fn(pstates):\n        model = brainstate.graph.treefy_merge(graphdef, pstates, count_states)\n        y_pred = model(x)\n        new_counts = brainstate.graph.treefy_states(model, CallCount)\n        loss = jnp.mean((y - y_pred) ** 2)\n        return loss, new_counts\n\n    grads, count_states = jax.grad(loss_fn, has_aux=True)(param_states)\n    param_states = jax.tree.map(lambda w, g: w - 0.1 * g, param_states, grads)\n    return param_states, count_states\n\n@jax.jit\ndef eval_step(param_states, count_states, batch):\n    x, y = batch\n    model = brainstate.graph.treefy_merge(graphdef, param_states, count_states)\n    y_pred = model(x)\n    return {'loss': jnp.mean((y - y_pred) ** 2)}\n\nif RUN_TRAINING:\n    for step, batch in zip(range(600), dataset(32)):\n        params, counts = train_step(params, counts, batch)\n        if step % 200 == 0:\n            logs = eval_step(params, counts, (X, Y))\n            print(f\"step {step}: loss={logs['loss']:.4f}\")\nelse:\n    print('Training skipped (set RUN_TRAINING=True to run SGD).')\n\nmodel = brainstate.graph.treefy_merge(graphdef, params, counts)\nprint(f\"call count state: {model.count.value}\")\ny_pred = model(X)\nplt.scatter(X, Y, color='steelblue', label='data')\nplt.plot(X, y_pred, color='black', label='fit')\nplt.legend()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Example 002 — Lifted Transforms (`examples/002_lifted_transforms.py`)\n\n**Highlights**\n- Decorating training/evaluation with `@brainstate.transform.jit`.\n- Using `brainstate.transform.grad` to obtain gradients that respect state collections.\n- Managing optimiser state with `braintools.optim.SGD`."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import braintools\n\nclass LiftedMLP(brainstate.nn.Module):\n    def __init__(self, din: int, dhidden: int, dout: int):\n        super().__init__()\n        self.count = CallCount(jnp.array(0))\n        self.linear1 = Linear(din, dhidden)\n        self.linear2 = Linear(dhidden, dout)\n\n    def __call__(self, x):\n        self.count.value += 1\n        x = self.linear1(x)\n        x = jax.nn.relu(x)\n        return self.linear2(x)\n\nmodel2 = LiftedMLP(1, 32, 1)\noptimizer = braintools.optim.SGD(1e-3)\noptimizer.register_trainable_weights(model2.states(brainstate.ParamState))\n\n@brainstate.transform.jit\ndef train_step_lifted(batch):\n    x, y = batch\n\n    def loss_fn():\n        preds = model2(x)\n        return jnp.mean((y - preds) ** 2)\n\n    grads = brainstate.transform.grad(\n        loss_fn, optimizer.param_states.to_pytree()\n    )()\n    optimizer.update(grads)\n\n@brainstate.transform.jit\ndef eval_step_lifted(batch):\n    x, y = batch\n    preds = model2(x)\n    return {'loss': jnp.mean((y - preds) ** 2)}\n\nif RUN_TRAINING:\n    for step, batch in zip(range(600), dataset(32)):\n        train_step_lifted(batch)\n        if step % 200 == 0:\n            logs = eval_step_lifted((X, Y))\n            print(f\"[lifted] step {step}: loss={logs['loss']:.4f}\")\nelse:\n    print('Lifted training skipped (set RUN_TRAINING=True).')\n\nprint(f\"lifted forward count: {model2.count.value}\")\nplt.scatter(X, Y, color='steelblue', label='data')\nplt.plot(X, model2(X), color='darkorange', label='lifted fit')\nplt.legend()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Example 003 — Variational Auto-Encoder (`examples/003_vae.py`)\n\n**Highlights**\n- Combining multiple `brainstate.nn.Module` components to build an encoder/decoder pair.\n- Storing auxiliary losses (KL divergence) in custom `brainstate.State` nodes.\n- Mixing Brainstate transforms with Optax losses.\n\n> **Note:** Requires `datasets`, `optax`, `matplotlib`, and several minutes of training. Leave `RUN_TRAINING=False` to skip execution."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import typing as tp\ntry:\n    import optax\n    from datasets import load_dataset\nexcept ModuleNotFoundError:\n    optax = None\n    load_dataset = None\n\nlatent_size = 32\nimage_shape: tp.Sequence[int] = (28, 28)\n\nclass Loss(brainstate.State):\n    pass\n\nclass Encoder(brainstate.nn.Module):\n    def __init__(self, din: int, dmid: int, dout: int):\n        super().__init__()\n        self.linear1 = brainstate.nn.Linear(din, dmid)\n        self.linear_mean = brainstate.nn.Linear(dmid, dout)\n        self.linear_std = brainstate.nn.Linear(dmid, dout)\n\n    def __call__(self, x: jax.Array) -> jax.Array:\n        x = x.reshape((x.shape[0], -1))\n        x = self.linear1(x)\n        x = jax.nn.relu(x)\n        mean = self.linear_mean(x)\n        std = jnp.exp(self.linear_std(x))\n        loss = jnp.mean(0.5 * jnp.mean(-jnp.log(std ** 2) - 1.0 + std ** 2 + mean ** 2, axis=-1))\n        self.kl_loss = Loss(loss)\n        z = mean + std * brainstate.random.normal(size=mean.shape)\n        return z\n\nclass Decoder(brainstate.nn.Module):\n    def __init__(self, din: int, dmid: int, dout: int):\n        super().__init__()\n        self.linear1 = brainstate.nn.Linear(din, dmid)\n        self.linear2 = brainstate.nn.Linear(dmid, dout)\n\n    def __call__(self, z: jax.Array) -> jax.Array:\n        z = self.linear1(z)\n        z = jax.nn.relu(z)\n        return self.linear2(z)\n\nclass VAE(brainstate.nn.Module):\n    def __init__(self, din: int, hidden_size: int, latent_size: int, output_shape: tp.Sequence[int]):\n        super().__init__()\n        self.output_shape = output_shape\n        self.encoder = Encoder(din, hidden_size, latent_size)\n        self.decoder = Decoder(latent_size, hidden_size, int(np.prod(output_shape)))\n\n    def __call__(self, x: jax.Array) -> jax.Array:\n        logits = self.decoder(self.encoder(x))\n        return jnp.reshape(logits, (-1, *self.output_shape))\n\n    def generate(self, z):\n        logits = self.decoder(z)\n        return jax.nn.sigmoid(logits.reshape((-1, *self.output_shape)))\n\nif RUN_TRAINING and optax is not None and load_dataset is not None:\n    dataset = load_dataset('mnist')\n    X_train = (np.stack(dataset['train']['image']) > 0).astype(jnp.float32)\n    optimizer = braintools.optim.Adam(1e-3)\n    model = VAE(np.prod(image_shape), 256, latent_size, image_shape)\n    optimizer.register_trainable_weights(model.states(brainstate.ParamState))\n\n    @brainstate.transform.jit\n    def train_step(x: jax.Array):\n        def loss_fn():\n            logits = model(x)\n            losses = brainstate.graph.treefy_states(model, Loss)\n            kl_loss = sum(jax.tree_util.tree_leaves(losses), 0.0)\n            recon_loss = jnp.mean(optax.sigmoid_binary_cross_entropy(logits, x))\n            return recon_loss + 0.1 * kl_loss\n        grads, loss = brainstate.transform.grad(\n            loss_fn, optimizer.param_states.to_pytree(), return_value=True)()\n        optimizer.update(grads)\n        return loss\n\n    for epoch in range(3):\n        batch_idx = np.random.randint(0, len(X_train), size=(64,))\n        loss = train_step(X_train[batch_idx])\n        print(f\"epoch {epoch}: loss={float(loss):.4f}\")\nelse:\n    print('VAE training skipped (install optax/datasets and set RUN_TRAINING=True).')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Example 004 — Scan Over Layers (`examples/004_scan_over_layers.py`)\n\n**Highlights**\n- Building repeated submodules and iterating through them manually to mimic `jax.lax.scan`.\n- Combining dense, batch-norm, and dropout primitives within a single block.\n\nThis example is fast to run; no additional dependencies are required."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "class Block(brainstate.nn.Module):\n    def __init__(self, dim: int):\n        super().__init__()\n        self.linear = brainstate.nn.Linear(dim, dim)\n        self.bn = brainstate.nn.BatchNorm0d([dim])\n        self.dropout = brainstate.nn.Dropout(0.5)\n\n    def __call__(self, x: jax.Array):\n        return jax.nn.gelu(self.dropout(self.bn(self.linear(x))))\n\nclass ScanMLP(brainstate.nn.Module):\n    def __init__(self, dim: int, *, n_layers: int):\n        super().__init__()\n        self.layers = [Block(dim) for _ in range(n_layers)]\n\n    def __call__(self, x: jax.Array) -> jax.Array:\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\nwith brainstate.environ.context(fit=True):\n    model = ScanMLP(10, n_layers=5)\n    y = model(jnp.ones((3, 10)))\n\nprint(jax.tree.map(jnp.shape, brainstate.graph.treefy_states(model)))\nprint('Output shape:', y.shape)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Example 005 — Saving and Loading Checkpoints (`examples/005_save_load_checkpoints.py`)\n\n**Highlights**\n- Capturing the complete state tree of a model.\n- Restoring into an abstractly initialised model with Orbax."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import os\nfrom tempfile import TemporaryDirectory\nimport orbax.checkpoint as orbax\n\nclass CheckpointMLP(brainstate.nn.Module):\n    def __init__(self, din: int, dmid: int, dout: int):\n        super().__init__()\n        self.dense1 = brainstate.nn.Linear(din, dmid)\n        self.dense2 = brainstate.nn.Linear(dmid, dout)\n\n    def __call__(self, x: jax.Array) -> jax.Array:\n        return self.dense2(jax.nn.relu(self.dense1(x)))\n\ndef create_model(seed: int):\n    brainstate.random.seed(seed)\n    return CheckpointMLP(10, 20, 30)\n\ndef create_and_save(seed: int, path: str):\n    model = create_model(seed)\n    state_tree = brainstate.graph.treefy_states(model)\n    orbax.PyTreeCheckpointer().save(os.path.join(path, 'state'), state_tree)\n\ndef load_model(path: str) -> CheckpointMLP:\n    model = brainstate.transform.abstract_init(lambda: create_model(0))\n    state_tree = brainstate.graph.treefy_states(model)\n    state_tree = orbax.PyTreeCheckpointer().restore(os.path.join(path, 'state'), item=state_tree)\n    brainstate.graph.update_states(model, state_tree)\n    return model\n\nwith TemporaryDirectory() as tmpdir:\n    create_and_save(42, tmpdir)\n    restored = load_model(tmpdir)\n    y = restored(jnp.ones((1, 10)))\n    print(restored)\n    print('Sample output shape:', y.shape)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Example 007 — Parameter Surgery (`examples/007_parameter_surgery.py`)\n\n**Highlights**\n- Swapping submodules with pretrained replacements.\n- Filtering parameter trees to separate frozen and trainable weights."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def load_pretrained():\n    return brainstate.nn.Linear(784, 128)\n\nclass Classifier(brainstate.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backbone = brainstate.nn.Linear(784, 128)\n        self.head = brainstate.nn.Linear(128, 10)\n\n    def __call__(self, x):\n        x = jax.nn.relu(self.backbone(x))\n        return self.head(x)\n\nmodel = Classifier()\nmodel.backbone = load_pretrained()\n\ndef is_trainable(path, node):\n    return 'backbone' not in path and issubclass(node.type, brainstate.ParamState)\n\ngraphdef, trainable_params, frozen_params = brainstate.graph.treefy_split(model, is_trainable, ...)\nprint('Trainable shapes:', jax.tree.map(jnp.shape, trainable_params))\nprint('Frozen shapes:', jax.tree.map(jnp.shape, frozen_params))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1xx — Biophysical Dynamics and Continuous Attractors\n\nThese examples move from abstract modules to conductance-based neurons and population dynamics."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Example 100 — Hodgkin–Huxley Neuron (`examples/100_hh_neuron_model.py`)\n\n**Highlights**\n- Implementing the classic Hodgkin–Huxley system inside a `brainstate.nn.Dynamics` subclass.\n- Using `brainstate.nn.exp_euler_step` with units from `brainunit`.\n- Iterating the model with `brainstate.transform.for_loop`.\n\n> Enable `RUN_NEURO_SIM=True` to execute the full 100 ms simulation."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import brainunit as u\n\nclass HH(brainstate.nn.Dynamics):\n    def __init__(self, in_size):\n        super().__init__(in_size)\n        self.ENa = 50. * u.mV\n        self.EK = -77. * u.mV\n        self.EL = -54.387 * u.mV\n        self.gNa = 120. * u.mS / u.cm ** 2\n        self.gK = 36. * u.mS / u.cm ** 2\n        self.gL = 0.03 * u.mS / u.cm ** 2\n        self.C = 1.0 * u.uF / u.cm ** 2\n        self.V_th = 20. * u.mV\n\n    m_alpha = lambda self, V: 1. / u.math.exprel(-(V / u.mV + 40) / 10)\n    m_beta = lambda self, V: 4.0 * jnp.exp(-(V / u.mV + 65) / 18)\n    h_alpha = lambda self, V: 0.07 * jnp.exp(-(V / u.mV + 65) / 20.)\n    h_beta = lambda self, V: 1 / (1 + jnp.exp(-(V / u.mV + 35) / 10))\n    n_alpha = lambda self, V: 0.1 / u.math.exprel(-(V / u.mV + 55) / 10)\n    n_beta = lambda self, V: 0.125 * jnp.exp(-(V / u.mV + 65) / 80)\n\n    def init_state(self, batch_size=None):\n        V0 = jnp.ones(self.varshape, brainstate.environ.dftype()) * -65. * u.mV\n        self.V = brainstate.HiddenState(V0)\n        self.m = brainstate.HiddenState(self.m_alpha(V0) / (self.m_alpha(V0) + self.m_beta(V0)))\n        self.h = brainstate.HiddenState(self.h_alpha(V0) / (self.h_alpha(V0) + self.h_beta(V0)))\n        self.n = brainstate.HiddenState(self.n_alpha(V0) / (self.n_alpha(V0) + self.n_beta(V0)))\n\n    def dV(self, V, t, m, h, n, I):\n        I = self.sum_current_inputs(I, V)\n        I_Na = (self.gNa * m ** 3 * h) * (V - self.ENa)\n        I_K = (self.gK * n ** 4) * (V - self.EK)\n        I_leak = self.gL * (V - self.EL)\n        return (- I_Na - I_K - I_leak + I) / self.C\n\n    def update(self, x=0. * u.mA / u.cm ** 2):\n        t = brainstate.environ.get('t')\n        V = brainstate.nn.exp_euler_step(self.dV, self.V.value, t, self.m.value, self.h.value, self.n.value, x)\n        m = brainstate.nn.exp_euler_step(lambda m, t, V: (self.m_alpha(V) * (1 - m) - self.m_beta(V) * m) / u.ms, self.m.value, t, self.V.value)\n        h = brainstate.nn.exp_euler_step(lambda h, t, V: (self.h_alpha(V) * (1 - h) - self.h_beta(V) * h) / u.ms, self.h.value, t, self.V.value)\n        n = brainstate.nn.exp_euler_step(lambda n, t, V: (self.n_alpha(V) * (1 - n) - self.n_beta(V) * n) / u.ms, self.n.value, t, self.V.value)\n        V = self.sum_delta_inputs(init=V)\n        spike = jnp.logical_and(self.V.value < self.V_th, V >= self.V_th)\n        self.V.value, self.m.value, self.h.value, self.n.value = V, m, h, n\n        return spike\n\nif RUN_NEURO_SIM:\n    hh = HH(10)\n    brainstate.nn.init_all_states(hh)\n    dt = 0.01 * u.ms\n\n    def run(t, inp):\n        with brainstate.environ.context(t=t, dt=dt):\n            hh(inp)\n        return hh.V.value\n\n    times = u.math.arange(0. * u.ms, 10. * u.ms, dt)\n    vs = brainstate.transform.for_loop(run, times, brainstate.random.uniform(1., 5., times.shape) * u.uA / u.cm ** 2)\n    plt.plot(times.to_decimal(u.ms), vs.to_decimal(u.mV))\n    plt.xlabel('Time (ms)')\n    plt.ylabel('Membrane potential (mV)')\n    plt.show()\nelse:\n    print('HH simulation skipped (set RUN_NEURO_SIM=True).')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Example 101 — Continuous Attractor Neural Network (`examples/101_cann_1d_oscillatory_tracking.py`)\n\n**Highlights**\n- Building a 1D CANN with spike-frequency adaptation.\n- Using `brainstate.transform.for_loop` for iterative simulation and animation.\n- Integrating external inputs with Brainstate's environment context.\n\n> This script depends on `matplotlib.animation` and is computationally heavy. Execute only when `RUN_NEURO_SIM=True`."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "if not RUN_NEURO_SIM:\n    print('CANN simulation skipped (set RUN_NEURO_SIM=True).')\nelse:\n    print('Open `examples/101_cann_1d_oscillatory_tracking.py` for the full animated simulation.')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Example 105 — COBA HH Network with BrainCell (`examples/105_COBA_HH_2007_braincell.py`)\n\n**Highlights**\n- Combining Brainstate modules with the BrainCell biophysical library.\n- Driving an excitatory/inhibitory network via event-based projections.\n\n> Requires `braincell`, `brainpy`, and `brainunit`. Set `RUN_NEURO_SIM=True` to run."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "if RUN_NEURO_SIM:\n    print('Run `python examples/105_COBA_HH_2007_braincell.py` for the full raster plot (heavy dependencies).')\nelse:\n    print('BrainCell COBA network skipped (enable RUN_NEURO_SIM to execute).')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2xx — Brain-Inspired Computing and Brainscale Training\n\nThe 203 example introduces Brainscale tools for training surrogate-gradient SNNs. These scripts are long-running and rely on `brainscale`, `torch`, `numba`, and other packages."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Example 203 — Brainscale for SNNs (`examples/203_brainscale_for_snns.py`)\n\n**Highlights**\n- Command-line flag parsing for different eligibility-trace methods (diag, expsm_diag, bptt).\n- Custom GIF neuron with eligibility traces (`brainscale.ETraceState`).\n- Integration with PyTorch dataloaders and NumPy-based preprocessing.\n\n> The notebook does not replicate the full training loop. Use the original script with `python examples/203_brainscale_for_snns.py --help` to explore all options."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3xx — Rate-Based RNNs and Brainscale RNN Training\n\nThe 3xx series covers classic RNN training and a Brainscale-enhanced workflow."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Example 300 — Integrator RNN (`examples/300_integrator_rnn.py`)\n\n**Highlights**\n- Training a simple integrator to reproduce cumulative sums of noisy inputs.\n- Demonstrating stateful RNN cells with trainable initial states.\n- Combining Brainstate transforms with `braintools.optim.Adam`.\n\n> Set `RUN_TRAINING=True` to execute a short training loop."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "dt = 0.04\nnum_step = int(1.0 / dt)\nnum_batch = 128\n\n@brainstate.transform.jit(static_argnums=2)\ndef build_inputs_and_targets(mean=0.025, scale=0.01, batch_size=10):\n    sample = brainstate.random.normal(size=(1, batch_size, 1))\n    bias = mean * 2.0 * (sample - 0.5)\n    noise_t = scale / dt ** 0.5 * brainstate.random.normal(size=(num_step, batch_size, 1))\n    inputs = bias + noise_t\n    targets = jnp.cumsum(inputs, axis=0)\n    return inputs, targets\n\ndef train_data():\n    for _ in range(100):\n        yield build_inputs_and_targets(0.025, 0.01, num_batch)\n\nclass RNNCell(brainstate.nn.Module):\n    def __init__(self, num_in: int, num_out: int):\n        super().__init__()\n        self.num_out = num_out\n        self.W = brainstate.ParamState(braintools.init.XavierNormal()((num_in + num_out, num_out)))\n        self.b = brainstate.ParamState(braintools.init.ZeroInit()((num_out,)))\n        self.state_param = brainstate.ParamState(braintools.init.ZeroInit()((num_out,)))\n\n    def init_state(self, batch_size=None, **kwargs):\n        base = self.state_param.value\n        if batch_size is None:\n            self.state = brainstate.HiddenState(base)\n        else:\n            self.state = brainstate.HiddenState(jnp.repeat(base[None, :], batch_size, axis=0))\n\n    def update(self, x):\n        x = jnp.concatenate([x, self.state.value], axis=-1)\n        h = jax.nn.tanh(x @ self.W.value + self.b.value)\n        self.state.value = h\n        return h\n\nclass IntegratorRNN(brainstate.nn.Module):\n    def __init__(self, num_in, num_hidden):\n        super().__init__()\n        self.cell = RNNCell(num_in, num_hidden)\n        self.out = brainstate.nn.Linear(num_hidden, 1)\n\n    def update(self, x):\n        return x >> self.cell >> self.out\n\nmodel = IntegratorRNN(1, 64)\nweights = model.states(brainstate.ParamState)\n\n@brainstate.transform.jit\ndef f_predict(inputs):\n    brainstate.nn.init_all_states(model, batch_size=inputs.shape[1])\n    return brainstate.transform.for_loop(model.update, inputs)\n\ndef f_loss(inputs, targets, l2_reg=2e-4):\n    preds = f_predict(inputs)\n    mse = braintools.metric.squared_error(preds, targets).mean()\n    l2 = sum(jnp.sum(leaf ** 2) for leaf in jax.tree.leaves(weights))\n    return mse + l2_reg * l2\n\nopt = braintools.optim.Adam(lr=braintools.optim.ExponentialDecayLR(0.01, 1, 0.999))\nopt.register_trainable_weights(weights)\n\n@brainstate.transform.jit\ndef f_train(inputs, targets):\n    grads, loss = brainstate.transform.grad(f_loss, weights, return_value=True)(inputs, targets)\n    opt.update(grads)\n    return loss\n\nif RUN_TRAINING:\n    for step, (inp, tar) in zip(range(10), train_data()):\n        loss = f_train(inp, tar)\n        print(f\"batch {step}: loss={float(loss):.5f}\")\nelse:\n    print('Integrator training skipped (set RUN_TRAINING=True).')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Example 301 — Brainscale for RNNs (`examples/301_brainscale_for_rnns.py`)\n\n**Highlights**\n- Brainscale eligibility-trace training for recurrent models.\n- Dataset streaming with PyTorch `IterableDataset`.\n- Checkpointing via Orbax `CheckpointManager`.\n\n> Requires `brainscale`, `torch`, and `orbax`. Execute the original script directly for the full experiment."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## BrainCell Interoperation (`examples/braincell-interoperation/`)\n\nSeven scripts illustrate how Brainstate talks to the BrainCell ecosystem for detailed cellular modelling:\n\n- `SC01_fitting_a_hh_neuron.py`: Parameter fitting of a Hodgkin–Huxley neuron using experimental traces.\n- `SC03_COBA_HH_2007_braincell.py`: COBA network with HH neurons (BrainCell primitives).\n- `SC04_hh_neuron.py`: Standalone BrainCell HH neuron driven by Brainstate.\n- `SC05_thalamus_single_compartment_neurons.py`: Building thalamic cell types (RTC, TRN, HTC, IN).\n- `SC06_unified_thalamus_model.py`: Multi-population thalamus network (Li et al., 2017).\n- `SC07_Straital_beta_oscillation_2011.py`: Striatal beta oscillation model.\n\n> These programmes depend on `braincell`, `brainunit`, `braintools`, `numba`, and dataset files (for SC01). Run them directly from the `examples/braincell-interoperation/` folder."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## BrainMass Interoperation (`examples/brainmass-interoperation/`)\n\nThe BrainMass demos model meso-scale neural mass dynamics with Brainstate integration:\n\n- `00-hopf-osillator.py`: Hopf oscillator bifurcation and noise-driven dynamics.\n- `01-wilsonwowan-osillator.py`: Wilson–Cowan oscillations and nullclines.\n- `02-fhn-osillator.py`: FitzHugh–Nagumo excitability.\n- `03-jansenrit_single_node_simulation.py`: Jansen–Rit cortical column.\n- `Modeling_resting_state_MEG_data.py`: Resting-state MEG fitting with multi-region coupling.\n\n> All scripts rely on `brainmass`, `brainunit`, `matplotlib`, and sometimes `pandas`. They are organised as rich notebooks in script form (`#%%` markers) for Jupyter conversion."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## BrainPy Interoperation (`examples/brainpy-interoperation/`)\n\nBrainPy 3.x is rebuilt on top of Brainstate. This directory contains numerous examples spanning network simulations, gamma oscillation regimes, and surrogate-gradient training. Refer to `examples/brainpy-interoperation/README.md` for a curated list:\n\n- 100-series: balanced E/I networks, synfire chains, gamma oscillations.\n- 110-series: Susin & Destexhe gamma models (AI, CHING, ING, PING).\n- 200-series: Surrogate gradient training on MNIST/Fashion-MNIST.\n\n> Install `brainpy` with the appropriate extras (`brainpy[cpu]` or `brainpy[cuda12]`). Each script can be run individually via `python <filename>`."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Benchmarks (`benchmark/`)\n\nAlthough not part of the `examples/` folder, the `benchmark/` directory contains the CUBA and COBA reference benchmarks used across multiple demos. Consult these scripts when you need baseline parameters for large-scale simulations."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Next Steps\n\n1. Clone specific sections of this notebook into new experiments and customise the models.\n2. Explore the `docs/tutorials/` notebooks for guided lessons on modules, transforms, and advanced APIs.\n3. Combine Brainstate with the interoperation libraries to build hybrid simulations that mix detailed neurons with learning systems."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
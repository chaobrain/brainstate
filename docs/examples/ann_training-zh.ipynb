{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e91a13480dd82aa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 训练人工神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b8e8f3",
   "metadata": {},
   "source": [
    "近年来，人工神经网络蓬勃发展，在神经科学研究中也扮演重要角色。brainstate作为面向脑动力学建模的高性能计算框架，也支持人工神经网络训练，便于神经动力学模型和人工神经网络的对接。\n",
    "\n",
    "在这里，我们用一个简单的2层多层感知机（MLP）进行手写数字识别（MNIST）任务的例子，介绍如何用brainstate训练人工神经网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d512371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "import brainstate\n",
    "from braintools.metric import softmax_cross_entropy_with_integer_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd228991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "brainstate.__version__"
  },
  {
   "cell_type": "markdown",
   "id": "9889df14",
   "metadata": {},
   "source": [
    "## 准备数据集\n",
    "\n",
    "需要获取数据集内容，并将数据集包装成可迭代对象，按照batch大小进行自动采样、洗牌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01e96ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('mnist')\n",
    "X_train = np.array(np.stack(dataset['train']['image']), dtype=np.uint8)\n",
    "X_test = np.array(np.stack(dataset['test']['image']), dtype=np.uint8)\n",
    "X_train = (X_train > 0).astype(jnp.float32)\n",
    "X_test = (X_test > 0).astype(jnp.float32)\n",
    "Y_train = np.array(dataset['train']['label'], dtype=np.int32)\n",
    "Y_test = np.array(dataset['test']['label'], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b886cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, X, Y, batch_size, shuffle=True):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(X))\n",
    "        self.current_index = 0\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.current_index = 0\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        # Check if all samples have been processed\n",
    "        if self.current_index >= len(self.X):\n",
    "            raise StopIteration\n",
    "\n",
    "        # Define the start and end of the current batch\n",
    "        start = self.current_index\n",
    "        end = start + self.batch_size\n",
    "        if end > len(self.X):\n",
    "            end = len(self.X)\n",
    "        \n",
    "        # Update current index\n",
    "        self.current_index = end\n",
    "\n",
    "        # Select batch samples\n",
    "        batch_indices = self.indices[start:end]\n",
    "        batch_X = self.X[batch_indices]\n",
    "        batch_Y = self.Y[batch_indices]\n",
    "\n",
    "        # Ensure batch has consistent shape\n",
    "        if batch_X.ndim == 1:\n",
    "            batch_X = np.expand_dims(batch_X, axis=0)\n",
    "\n",
    "        return batch_X, batch_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c383f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training and testing datasets\n",
    "batch_size = 32\n",
    "train_dataset = Dataset(X_train, Y_train, batch_size, shuffle=True)\n",
    "test_dataset = Dataset(X_test, Y_test, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba1b50a",
   "metadata": {},
   "source": [
    "## 人工神经网络的定义\n",
    "\n",
    "brainstate在定义人工神经网络时，需要继承基类 ``brainstate.nn.Module``。定义时需要在类方法``__init__()``中定义网络中的层（注意要首先初始化基类``super().__init__()``）；需要在类方法``__call__()``中定义网络前向传播的方法。\n",
    "\n",
    "brainstate也支持具体定义网络某一层执行的操作，也需要继承基类 ``brainstate.nn.Module``, 用法和定义网络时类似。\n",
    "\n",
    "模型中所有需要改变的量都需封装在 ``State`` 对象中。其中，需要在训练中更新的模型参数需封装在``ParamState``（是``State`` 的子类）对象中。其他需要在训练中更新的量封装在``State``的另一个子类``ShortTermState``对象中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea8b5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define linear layer\n",
    "class Linear(brainstate.nn.Module):\n",
    "  def __init__(self, din: int, dout: int):\n",
    "    super().__init__()\n",
    "    self.w = brainstate.ParamState(brainstate.random.rand(din, dout))  # Initialize weight parameters\n",
    "    self.b = brainstate.ParamState(jnp.zeros((dout,)))  # Initialize bias parameters\n",
    "\n",
    "  def __call__(self, x):\n",
    "    return x @ self.w.value + self.b.value    # Perform linear transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e541917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a short-term state for counting times called\n",
    "class Count(brainstate.ShortTermState):\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9445bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MLP model\n",
    "class MLP(brainstate.nn.Module):\n",
    "  def __init__(self, din, dhidden, dout):\n",
    "    super().__init__()\n",
    "    self.count = Count(jnp.array(0))    # Count how many times model is called\n",
    "    self.linear1 = Linear(din, dhidden)          # brainstate有常规层的实现，可以直接写 self.linear1 = bst.nn.Linear(din, dhidden)\n",
    "    self.linear2 = Linear(dhidden, dout)\n",
    "    self.flatten = brainstate.nn.Flatten(start_axis=1)   # Flatten images to 1D\n",
    "    self.relu = brainstate.nn.ReLU()   # ReLU activation function\n",
    "\n",
    "  def __call__(self, x):\n",
    "    self.count.value += 1   # Increment call count\n",
    "\n",
    "    x = self.flatten(x)\n",
    "    x = self.linear1(x)\n",
    "    x = self.relu(x)      # 也兼容jax函数，可以直接写 x = jax.nn.relu(x)\n",
    "    x = self.linear2(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aefeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model with input, hidden, and output layer sizes\n",
    "model = MLP(din=28*28, dhidden=512, dout=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7edf945",
   "metadata": {},
   "source": [
    "## 优化器设置\n",
    "``brainstate.optim``提供了各种各样的优化器可供选择。\n",
    "\n",
    "优化器实例化后，需要在``optimizer.register_trainable_weights()``中规定传入模型中计划使用此优化器更新的参数。\n",
    "\n",
    "此处用``brainstate.nn.Module.states()``收集了该模型中所有网络节点和子节点的``State``，并且限制了所收集``State``的类型为``brainstate.ParamState``（在此例的模型中，``State``实例还有``Count``类，而这并非是需要此优化器更新的，因此需要加以类型限制）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dd079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer and register model parameters\n",
    "optimizer = brainstate.optim.SGD(lr = 1e-3)   # Initialize SGD optimizer with learning rate\n",
    "optimizer.register_trainable_weights(model.states(brainstate.ParamState))   # Register parameters for optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbbfad2",
   "metadata": {},
   "source": [
    "## 模型训练\n",
    "模型训练时，使用``brainstate.augment.grad``函数来计算梯度，需要传入损失函数和需计算梯度的参数``State``。\n",
    "\n",
    "然后将梯度通过``update()``传给先前定义好的优化器，进行更新。\n",
    "\n",
    "使用``brainstate.compile.jit``函数装饰单步训练函数，使之可以即时编译，提高计算效率和性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99272b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training step function\n",
    "@brainstate.compile.jit\n",
    "def train_step(batch):\n",
    "  x, y = batch\n",
    "  # Define loss function\n",
    "  def loss_fn():\n",
    "    return softmax_cross_entropy_with_integer_labels(model(x), y).mean()\n",
    "  \n",
    "  # Compute gradients of the loss with respect to model parameters\n",
    "  grads = brainstate.augment.grad(loss_fn, model.states(brainstate.ParamState))()\n",
    "  optimizer.update(grads)   # Update parameters using optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c2dfe0",
   "metadata": {},
   "source": [
    "## 模型测试\n",
    "使用``brainstate.compile.jit``函数装饰单步测试函数，使之可以即时编译，提高计算效率和性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7456afa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing step function\n",
    "@brainstate.compile.jit\n",
    "def test_step(batch):\n",
    "  x, y = batch\n",
    "  y_pred = model(x)   # Perform forward pass\n",
    "  loss = softmax_cross_entropy_with_integer_labels(y_pred, y).mean()   # Compute loss\n",
    "  correct = (y_pred.argmax(1) == y).sum()   # Count correct predictions\n",
    "\n",
    "  return {'loss': loss, 'correct': correct}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a550e569",
   "metadata": {},
   "source": [
    "## 训练进行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae5f682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, test loss: 410.2366638183594, test accuracy: 0.24890001118183136\n",
      "epoch: 1, test loss: 278.79864501953125, test accuracy: 0.6233000159263611\n",
      "epoch: 2, test loss: 75.72823333740234, test accuracy: 0.7638000249862671\n",
      "epoch: 3, test loss: 59.49712371826172, test accuracy: 0.7830000519752502\n",
      "epoch: 4, test loss: 38.07597351074219, test accuracy: 0.8623000383377075\n",
      "epoch: 5, test loss: 54.225074768066406, test accuracy: 0.8329000473022461\n",
      "epoch: 6, test loss: 74.46405792236328, test accuracy: 0.7676000595092773\n",
      "epoch: 7, test loss: 35.6864128112793, test accuracy: 0.867900013923645\n",
      "epoch: 8, test loss: 140.0616912841797, test accuracy: 0.7529000639915466\n",
      "epoch: 9, test loss: 42.05353927612305, test accuracy: 0.8574000597000122\n",
      "times called: 21880\n"
     ]
    }
   ],
   "source": [
    "# Execute training and testing\n",
    "total_steps = 20\n",
    "for epoch in range(10):\n",
    "  for step, batch in enumerate(train_dataset):\n",
    "    train_step(batch)   # Perform training step for each batch\n",
    "\n",
    "  # Calculate test loss and accuracy\n",
    "  test_loss, correct = 0, 0\n",
    "  for step_, test_ in enumerate(test_dataset):\n",
    "    logs = test_step(test_)\n",
    "    test_loss += logs['loss']\n",
    "    correct += logs['correct']\n",
    "    test_loss += logs['loss']\n",
    "  test_loss = test_loss / (step_ + 1)\n",
    "  test_accuracy = correct / len(X_test)\n",
    "  print(f\"epoch: {epoch}, test loss: {test_loss}, test accuracy: {test_accuracy}\")\n",
    "\n",
    "print('times model called:', model.count.value)   # Output number of model calls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

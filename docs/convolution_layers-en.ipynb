{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Layers in BrainState\n",
    "\n",
    "This tutorial provides a comprehensive guide to using convolution layers in BrainState, covering both standard convolutions and transposed convolutions for various deep learning applications.\n",
    "\n",
    "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/chaobrain/brainstate/blob/main/docs/tutorials/convolution_layers-en.ipynb)\n",
    "[![Open in Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/chaobrain/brainstate/blob/main/docs/tutorials/convolution_layers-en.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Standard Convolutions](#standard-convolutions)\n",
    "   - [Conv1d: 1D Convolutions](#conv1d)\n",
    "   - [Conv2d: 2D Convolutions](#conv2d)\n",
    "   - [Conv3d: 3D Convolutions](#conv3d)\n",
    "3. [Weight-Standardized Convolutions](#weight-standardized-convolutions)\n",
    "   - [ScaledWSConv1d, ScaledWSConv2d, ScaledWSConv3d](#scaled-ws-convolutions)\n",
    "4. [Transposed Convolutions](#transposed-convolutions)\n",
    "   - [ConvTranspose1d, ConvTranspose2d, ConvTranspose3d](#conv-transpose)\n",
    "5. [Channel Formats: channels-last vs channels-first](#channel-formats)\n",
    "6. [Advanced Features](#advanced-features)\n",
    "   - [Grouped Convolutions](#grouped-convolutions)\n",
    "   - [Dilated Convolutions](#dilated-convolutions)\n",
    "   - [Padding Modes](#padding-modes)\n",
    "7. [Practical Examples](#practical-examples)\n",
    "   - [Building a CNN for Image Classification](#cnn-example)\n",
    "   - [Building an Autoencoder](#autoencoder-example)\n",
    "   - [U-Net for Segmentation](#unet-example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation (if needed)\n",
    "# !pip install brainstate --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brainstate\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "jax.config.update('jax_platform_name', 'cpu')\n",
    "print(f\"BrainState version: { brainstate.__version__}\")\n",
    "print(f\"JAX version: {jax.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"introduction\"></a>\n",
    "## 1. Introduction\n",
    "\n",
    "Convolution layers are fundamental building blocks in deep learning, especially for processing grid-like data such as images, audio, and video. BrainState provides a comprehensive set of convolution layers that are:\n",
    "\n",
    "- **Flexible**: Support both JAX-style (channels-last) and PyTorch-style (channels-first) data formats\n",
    "- **Efficient**: Built on top of JAX's high-performance `conv_general_dilated` operation\n",
    "- **Feature-rich**: Include standard convolutions, weight-standardized convolutions, and transposed convolutions\n",
    "- **Well-documented**: Comprehensive docstrings with examples and comparisons to PyTorch\n",
    "\n",
    "### Convolution Types in BrainState\n",
    "\n",
    "| Layer Type | 1D | 2D | 3D | Use Cases |\n",
    "|-----------|----|----|----|-----------|\n",
    "| **Standard Conv** | `Conv1d` | `Conv2d` | `Conv3d` | Feature extraction, downsampling |\n",
    "| **Weight-Standardized** | `ScaledWSConv1d` | `ScaledWSConv2d` | `ScaledWSConv3d` | Improved training stability |\n",
    "| **Transposed Conv** | `ConvTranspose1d` | `ConvTranspose2d` | `ConvTranspose3d` | Upsampling, generation |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"standard-convolutions\"></a>\n",
    "## 2. Standard Convolutions\n",
    "\n",
    "Standard convolutions are used for feature extraction and spatial downsampling in neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conv1d\"></a>\n",
    "### Conv1d: 1D Convolutions\n",
    "\n",
    "1D convolutions are commonly used for sequence data such as audio signals, text, and time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Basic Conv1d for time series\n",
    "# Input: (batch_size, time_steps, channels)\n",
    "batch_size = 4\n",
    "time_steps = 100\n",
    "in_channels = 3\n",
    "out_channels = 16\n",
    "\n",
    "# Create Conv1d layer\n",
    "conv1d =  brainstate.nn.Conv1d(\n",
    "    in_size=(time_steps, in_channels),\n",
    "    out_channels=out_channels,\n",
    "    kernel_size=5,\n",
    "    stride=1,\n",
    "    padding='SAME'\n",
    ")\n",
    "\n",
    "# Create sample input\n",
    "x = jnp.ones((batch_size, time_steps, in_channels))\n",
    "\n",
    "# Forward pass\n",
    "y = conv1d(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print(f\"Kernel shape: {conv1d.kernel_shape}\")\n",
    "print(f\"Number of parameters: {np.prod(conv1d.kernel_shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Conv1d with different stride and padding\n",
    "conv1d_downsample =  brainstate.nn.Conv1d(\n",
    "    in_size=(100, 16),\n",
    "    out_channels=32,\n",
    "    kernel_size=3,\n",
    "    stride=2,  # Downsample by factor of 2\n",
    "    padding='SAME'\n",
    ")\n",
    "\n",
    "x = jnp.ones((4, 100, 16))\n",
    "y = conv1d_downsample(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape (stride=2): {y.shape}\")\n",
    "print(f\"Downsampling factor: {x.shape[1] / y.shape[1]:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conv2d\"></a>\n",
    "### Conv2d: 2D Convolutions\n",
    "\n",
    "2D convolutions are the workhorses of computer vision, used in virtually all image processing networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Basic Conv2d for images\n",
    "# Input: (batch_size, height, width, channels) - channels-last format\n",
    "batch_size = 8\n",
    "height, width = 32, 32\n",
    "in_channels = 3  # RGB\n",
    "out_channels = 64\n",
    "\n",
    "# Create Conv2d layer\n",
    "conv2d =  brainstate.nn.Conv2d(\n",
    "    in_size=(height, width, in_channels),\n",
    "    out_channels=out_channels,\n",
    "    kernel_size=3,\n",
    "    stride=1,\n",
    "    padding='SAME',\n",
    "    b_init= brainstate.init.Constant(0.0)  # Add bias\n",
    ")\n",
    "\n",
    "# Create sample input (e.g., RGB images)\n",
    "x = jnp.ones((batch_size, height, width, in_channels))\n",
    "\n",
    "# Forward pass\n",
    "y = conv2d(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print(f\"Kernel shape: {conv2d.kernel_shape}\")\n",
    "print(f\"Has bias: {'bias' in conv2d.weight.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Conv2d with different kernel sizes\n",
    "# Visualize how different kernel sizes affect the receptive field\n",
    "\n",
    "kernel_sizes = [1, 3, 5, 7]\n",
    "for k in kernel_sizes:\n",
    "    conv =  brainstate.nn.Conv2d(\n",
    "        in_size=(32, 32, 3),\n",
    "        out_channels=16,\n",
    "        kernel_size=k,\n",
    "        padding='SAME'\n",
    "    )\n",
    "    x = jnp.ones((1, 32, 32, 3))\n",
    "    y = conv(x)\n",
    "    params = np.prod(conv.kernel_shape)\n",
    "    print(f\"Kernel size {k}x{k}: Output shape {y.shape}, Parameters: {params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conv3d\"></a>\n",
    "### Conv3d: 3D Convolutions\n",
    "\n",
    "3D convolutions are used for video processing, 3D medical imaging, and volumetric data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Conv3d for video data\n",
    "# Input: (batch_size, frames, height, width, channels)\n",
    "batch_size = 2\n",
    "frames = 16\n",
    "height, width = 64, 64\n",
    "in_channels = 3\n",
    "out_channels = 32\n",
    "\n",
    "# Create Conv3d layer\n",
    "conv3d =  brainstate.nn.Conv3d(\n",
    "    in_size=(frames, height, width, in_channels),\n",
    "    out_channels=out_channels,\n",
    "    kernel_size=3,  # 3x3x3 kernel\n",
    "    stride=1,\n",
    "    padding='SAME'\n",
    ")\n",
    "\n",
    "# Create sample input (e.g., video clip)\n",
    "x = jnp.ones((batch_size, frames, height, width, in_channels))\n",
    "\n",
    "# Forward pass\n",
    "y = conv3d(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print(f\"Kernel shape: {conv3d.kernel_shape}\")\n",
    "print(f\"Parameters: {np.prod(conv3d.kernel_shape):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"weight-standardized-convolutions\"></a>\n",
    "## 3. Weight-Standardized Convolutions\n",
    "\n",
    "Weight standardization is a technique that normalizes convolutional weights to have zero mean and unit variance, which can improve training stability and performance, especially when combined with Group Normalization.\n",
    "\n",
    "**Reference**: [Weight Standardization (Qiao et al., 2019)](https://arxiv.org/abs/1903.10520)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"scaled-ws-convolutions\"></a>\n",
    "### ScaledWSConv: Weight Standardization with Learnable Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: ScaledWSConv2d for improved training\n",
    "ws_conv =  brainstate.nn.ScaledWSConv2d(\n",
    "    in_size=(32, 32, 64),\n",
    "    out_channels=128,\n",
    "    kernel_size=3,\n",
    "    ws_gain=True,  # Include learnable per-channel gain\n",
    "    eps=1e-4  # Small constant for numerical stability\n",
    ")\n",
    "\n",
    "x = jnp.ones((4, 32, 32, 64))\n",
    "y = ws_conv(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print(f\"Has gain parameter: {'gain' in ws_conv.weight.value}\")\n",
    "\n",
    "# Weight standardization formula:\n",
    "# W_hat = gain * (W - mean(W)) / (std(W) + eps)\n",
    "print(\"\\nWeight standardization normalizes weights to:\")\n",
    "print(\"- Zero mean\")\n",
    "print(\"- Unit variance\")\n",
    "print(\"- With optional learnable gain for expressiveness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison: Standard Conv vs Weight-Standardized Conv\n",
    "print(\"Standard Conv2d:\")\n",
    "standard_conv =  brainstate.nn.Conv2d(\n",
    "    in_size=(32, 32, 64),\n",
    "    out_channels=128,\n",
    "    kernel_size=3\n",
    ")\n",
    "print(f\"  Parameters: {list(standard_conv.weight.value.keys())}\")\n",
    "\n",
    "print(\"\\nWeight-Standardized Conv2d:\")\n",
    "ws_conv =  brainstate.nn.ScaledWSConv2d(\n",
    "    in_size=(32, 32, 64),\n",
    "    out_channels=128,\n",
    "    kernel_size=3,\n",
    "    ws_gain=True\n",
    ")\n",
    "print(f\"  Parameters: {list(ws_conv.weight.value.keys())}\")\n",
    "print(f\"  Gain shape: {ws_conv.weight.value['gain'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"transposed-convolutions\"></a>\n",
    "## 4. Transposed Convolutions\n",
    "\n",
    "Transposed convolutions (also called deconvolutions) are used for upsampling feature maps. They're essential in:\n",
    "- Autoencoders (decoder path)\n",
    "- Generative models (GANs, VAEs)\n",
    "- Semantic segmentation (U-Net, FCN)\n",
    "- Super-resolution networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conv-transpose\"></a>\n",
    "### ConvTranspose1d, ConvTranspose2d, ConvTranspose3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: ConvTranspose1d for upsampling sequences\n",
    "conv_transpose_1d =  brainstate.nn.ConvTranspose1d(\n",
    "    in_size=(50, 32),  # (time_steps, channels)\n",
    "    out_channels=16,\n",
    "    kernel_size=4,\n",
    "    stride=2,  # Upsample by 2x\n",
    "    padding='SAME'\n",
    ")\n",
    "\n",
    "x = jnp.ones((4, 50, 32))\n",
    "y = conv_transpose_1d(x)\n",
    "\n",
    "print(\"ConvTranspose1d:\")\n",
    "print(f\"  Input shape: {x.shape}\")\n",
    "print(f\"  Output shape: {y.shape}\")\n",
    "print(f\"  Upsampling factor: {y.shape[1] / x.shape[1]:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: ConvTranspose2d for image upsampling\n",
    "conv_transpose_2d =  brainstate.nn.ConvTranspose2d(\n",
    "    in_size=(16, 16, 128),  # (height, width, channels)\n",
    "    out_channels=64,\n",
    "    kernel_size=4,\n",
    "    stride=2,  # Upsample by 2x in each spatial dimension\n",
    "    padding='SAME'\n",
    ")\n",
    "\n",
    "x = jnp.ones((4, 16, 16, 128))\n",
    "y = conv_transpose_2d(x)\n",
    "\n",
    "print(\"ConvTranspose2d:\")\n",
    "print(f\"  Input shape: {x.shape}\")\n",
    "print(f\"  Output shape: {y.shape}\")\n",
    "print(f\"  Spatial upsampling: {x.shape[1]}x{x.shape[2]} -> {y.shape[1]}x{y.shape[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Visualizing upsampling with different strides\n",
    "strides = [1, 2, 3, 4]\n",
    "in_size = 16\n",
    "\n",
    "print(\"Effect of stride on upsampling:\")\n",
    "print(\"=\"*50)\n",
    "for stride in strides:\n",
    "    conv_t =  brainstate.nn.ConvTranspose2d(\n",
    "        in_size=(in_size, in_size, 64),\n",
    "        out_channels=32,\n",
    "        kernel_size=4,\n",
    "        stride=stride,\n",
    "        padding='SAME'\n",
    "    )\n",
    "    x = jnp.ones((1, in_size, in_size, 64))\n",
    "    y = conv_t(x)\n",
    "    print(f\"Stride {stride}: {in_size}x{in_size} -> {y.shape[1]}x{y.shape[2]} ({y.shape[1]/in_size:.1f}x upsampling)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: ConvTranspose3d for video upsampling\n",
    "conv_transpose_3d =  brainstate.nn.ConvTranspose3d(\n",
    "    in_size=(8, 16, 16, 64),  # (frames, height, width, channels)\n",
    "    out_channels=32,\n",
    "    kernel_size=4,\n",
    "    stride=2,\n",
    "    padding='SAME'\n",
    ")\n",
    "\n",
    "x = jnp.ones((2, 8, 16, 16, 64))\n",
    "y = conv_transpose_3d(x)\n",
    "\n",
    "print(\"ConvTranspose3d:\")\n",
    "print(f\"  Input shape: {x.shape}\")\n",
    "print(f\"  Output shape: {y.shape}\")\n",
    "print(f\"  3D upsampling: {x.shape[1]}x{x.shape[2]}x{x.shape[3]} -> {y.shape[1]}x{y.shape[2]}x{y.shape[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"channel-formats\"></a>\n",
    "## 5. Channel Formats: channels-last vs channels-first\n",
    "\n",
    "BrainState supports both data format conventions:\n",
    "- **Channels-last (default)**: JAX/TensorFlow style - `[B, H, W, C]`\n",
    "- **Channels-first**: PyTorch style - `[B, C, H, W]`\n",
    "\n",
    "Use the `channel_first` parameter to switch between formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Comparing channels-last and channels-first\n",
    "\n",
    "# Channels-last (JAX/TensorFlow style) - DEFAULT\n",
    "conv_last =  brainstate.nn.Conv2d(\n",
    "    in_size=(32, 32, 3),  # (H, W, C)\n",
    "    out_channels=64,\n",
    "    kernel_size=3,\n",
    "    channel_first=False  # Default\n",
    ")\n",
    "\n",
    "x_last = jnp.ones((8, 32, 32, 3))  # (B, H, W, C)\n",
    "y_last = conv_last(x_last)\n",
    "\n",
    "print(\"Channels-last format (JAX/TensorFlow):\")\n",
    "print(f\"  Input shape: {x_last.shape} (B, H, W, C)\")\n",
    "print(f\"  Output shape: {y_last.shape} (B, H, W, C)\")\n",
    "print(f\"  Channel axis: -1\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Channels-first (PyTorch style)\n",
    "conv_first =  brainstate.nn.Conv2d(\n",
    "    in_size=(3, 32, 32),  # (C, H, W)\n",
    "    out_channels=64,\n",
    "    kernel_size=3,\n",
    "    channel_first=True  # PyTorch compatible\n",
    ")\n",
    "\n",
    "x_first = jnp.ones((8, 3, 32, 32))  # (B, C, H, W)\n",
    "y_first = conv_first(x_first)\n",
    "\n",
    "print(\"Channels-first format (PyTorch):\")\n",
    "print(f\"  Input shape: {x_first.shape} (B, C, H, W)\")\n",
    "print(f\"  Output shape: {y_first.shape} (B, C, H, W)\")\n",
    "print(f\"  Channel axis: 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting between formats\n",
    "def channels_last_to_first(x):\n",
    "    \"\"\"Convert (B, H, W, C) to (B, C, H, W)\"\"\"\n",
    "    return jnp.transpose(x, (0, 3, 1, 2))\n",
    "\n",
    "def channels_first_to_last(x):\n",
    "    \"\"\"Convert (B, C, H, W) to (B, H, W, C)\"\"\"\n",
    "    return jnp.transpose(x, (0, 2, 3, 1))\n",
    "\n",
    "# Example\n",
    "x_last = jnp.ones((4, 32, 32, 3))\n",
    "x_first = channels_last_to_first(x_last)\n",
    "x_back = channels_first_to_last(x_first)\n",
    "\n",
    "print(f\"Original (channels-last): {x_last.shape}\")\n",
    "print(f\"Converted to channels-first: {x_first.shape}\")\n",
    "print(f\"Converted back: {x_back.shape}\")\n",
    "print(f\"Arrays equal: {jnp.allclose(x_last, x_back)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"advanced-features\"></a>\n",
    "## 6. Advanced Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"grouped-convolutions\"></a>\n",
    "### Grouped Convolutions\n",
    "\n",
    "Grouped convolutions divide input and output channels into groups, with each group processed independently. This reduces parameters and computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Comparing standard vs grouped convolutions\n",
    "\n",
    "in_channels = 64\n",
    "out_channels = 128\n",
    "kernel_size = 3\n",
    "\n",
    "# Standard convolution\n",
    "conv_standard =  brainstate.nn.Conv2d(\n",
    "    in_size=(32, 32, in_channels),\n",
    "    out_channels=out_channels,\n",
    "    kernel_size=kernel_size,\n",
    "    groups=1  # No grouping\n",
    ")\n",
    "\n",
    "# Grouped convolution (4 groups)\n",
    "conv_grouped =  brainstate.nn.Conv2d(\n",
    "    in_size=(32, 32, in_channels),\n",
    "    out_channels=out_channels,\n",
    "    kernel_size=kernel_size,\n",
    "    groups=4  # Divide into 4 groups\n",
    ")\n",
    "\n",
    "# Depthwise convolution (groups = in_channels)\n",
    "conv_depthwise =  brainstate.nn.Conv2d(\n",
    "    in_size=(32, 32, in_channels),\n",
    "    out_channels=in_channels,  # Must equal in_channels for depthwise\n",
    "    kernel_size=kernel_size,\n",
    "    groups=in_channels  # Each channel processed separately\n",
    ")\n",
    "\n",
    "print(\"Parameter comparison:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Standard convolution:    {np.prod(conv_standard.kernel_shape):,} parameters\")\n",
    "print(f\"Grouped convolution (4): {np.prod(conv_grouped.kernel_shape):,} parameters\")\n",
    "print(f\"Depthwise convolution:   {np.prod(conv_depthwise.kernel_shape):,} parameters\")\n",
    "print(\"\\nParameter reduction with grouped convolution:\")\n",
    "reduction = (1 - np.prod(conv_grouped.kernel_shape) / np.prod(conv_standard.kernel_shape)) * 100\n",
    "print(f\"  {reduction:.1f}% fewer parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dilated-convolutions\"></a>\n",
    "### Dilated Convolutions\n",
    "\n",
    "Dilated (atrous) convolutions expand the receptive field without increasing the number of parameters by inserting zeros between kernel elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Dilated convolutions for larger receptive fields\n",
    "\n",
    "dilations = [1, 2, 4, 8]\n",
    "kernel_size = 3\n",
    "\n",
    "print(\"Effective receptive field with dilation:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for dilation in dilations:\n",
    "    conv_dilated =  brainstate.nn.Conv2d(\n",
    "        in_size=(32, 32, 64),\n",
    "        out_channels=64,\n",
    "        kernel_size=kernel_size,\n",
    "        rhs_dilation=dilation  # Kernel dilation\n",
    "    )\n",
    "    \n",
    "    # Effective kernel size = kernel_size + (kernel_size - 1) * (dilation - 1)\n",
    "    effective_kernel = kernel_size + (kernel_size - 1) * (dilation - 1)\n",
    "    \n",
    "    print(f\"Dilation {dilation}: {kernel_size}x{kernel_size} kernel -> {effective_kernel}x{effective_kernel} receptive field\")\n",
    "    print(f\"  Parameters: {np.prod(conv_dilated.kernel_shape):,} (same for all dilations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"padding-modes\"></a>\n",
    "### Padding Modes\n",
    "\n",
    "BrainState supports multiple padding modes:\n",
    "- **'SAME'**: Output size equals input size (when stride=1)\n",
    "- **'VALID'**: No padding, output size is reduced\n",
    "- **Explicit padding**: Custom padding values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Different padding modes\n",
    "\n",
    "in_size = (32, 32, 3)\n",
    "kernel_size = 5\n",
    "\n",
    "# SAME padding\n",
    "conv_same =  brainstate.nn.Conv2d(\n",
    "    in_size=in_size,\n",
    "    out_channels=64,\n",
    "    kernel_size=kernel_size,\n",
    "    padding='SAME'\n",
    ")\n",
    "\n",
    "# VALID padding (no padding)\n",
    "conv_valid =  brainstate.nn.Conv2d(\n",
    "    in_size=in_size,\n",
    "    out_channels=64,\n",
    "    kernel_size=kernel_size,\n",
    "    padding='VALID'\n",
    ")\n",
    "\n",
    "# Explicit padding\n",
    "conv_custom =  brainstate.nn.Conv2d(\n",
    "    in_size=in_size,\n",
    "    out_channels=64,\n",
    "    kernel_size=kernel_size,\n",
    "    padding=[(2, 2), (2, 2)]  # (top/bottom, left/right)\n",
    ")\n",
    "\n",
    "x = jnp.ones((1, 32, 32, 3))\n",
    "\n",
    "print(\"Effect of padding modes:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"SAME padding:   {conv_same(x).shape}\")\n",
    "print(f\"VALID padding:  {conv_valid(x).shape}\")\n",
    "print(f\"Custom padding: {conv_custom(x).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"practical-examples\"></a>\n",
    "## 7. Practical Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cnn-example\"></a>\n",
    "### Example 1: Building a CNN for Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN( brainstate.nn.Module):\n",
    "    \"\"\"Simple CNN for image classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Feature extraction layers\n",
    "        self.conv1 =  brainstate.nn.Conv2d(\n",
    "            in_size=(32, 32, 3),\n",
    "            out_channels=32,\n",
    "            kernel_size=3,\n",
    "            padding='SAME'\n",
    "        )\n",
    "        \n",
    "        self.conv2 =  brainstate.nn.Conv2d(\n",
    "            in_size=(16, 16, 32),  # After pooling\n",
    "            out_channels=64,\n",
    "            kernel_size=3,\n",
    "            padding='SAME'\n",
    "        )\n",
    "        \n",
    "        self.conv3 =  brainstate.nn.Conv2d(\n",
    "            in_size=(8, 8, 64),  # After pooling\n",
    "            out_channels=128,\n",
    "            kernel_size=3,\n",
    "            padding='SAME'\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.fc =  brainstate.nn.Linear(4 * 4 * 128, num_classes)\n",
    "    \n",
    "    def update(self, x):\n",
    "        # Block 1\n",
    "        x = self.conv1(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        x = jax.lax.reduce_window(\n",
    "            x, -jnp.inf, jax.lax.max, \n",
    "            window_dimensions=(1, 2, 2, 1),\n",
    "            window_strides=(1, 2, 2, 1),\n",
    "            padding='VALID'\n",
    "        )  # Max pooling 2x2\n",
    "        \n",
    "        # Block 2\n",
    "        x = self.conv2(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        x = jax.lax.reduce_window(\n",
    "            x, -jnp.inf, jax.lax.max,\n",
    "            window_dimensions=(1, 2, 2, 1),\n",
    "            window_strides=(1, 2, 2, 1),\n",
    "            padding='VALID'\n",
    "        )  # Max pooling 2x2\n",
    "        \n",
    "        # Block 3\n",
    "        x = self.conv3(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        x = jax.lax.reduce_window(\n",
    "            x, -jnp.inf, jax.lax.max,\n",
    "            window_dimensions=(1, 2, 2, 1),\n",
    "            window_strides=(1, 2, 2, 1),\n",
    "            padding='VALID'\n",
    "        )  # Max pooling 2x2\n",
    "        \n",
    "        # Classification\n",
    "        x = x.reshape(x.shape[0], -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Create and test the model\n",
    "model = SimpleCNN(num_classes=10)\n",
    "x = jnp.ones((4, 32, 32, 3))\n",
    "logits = model(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {logits.shape}\")\n",
    "print(f\"Number of classes: {logits.shape[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"autoencoder-example\"></a>\n",
    "### Example 2: Building an Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoencoder( brainstate.nn.Module):\n",
    "    \"\"\"Convolutional Autoencoder for image reconstruction.\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc_conv1 =  brainstate.nn.Conv2d(\n",
    "            in_size=(64, 64, 3),\n",
    "            out_channels=32,\n",
    "            kernel_size=4,\n",
    "            stride=2,\n",
    "            padding='SAME'\n",
    "        )  # 64x64 -> 32x32\n",
    "        \n",
    "        self.enc_conv2 =  brainstate.nn.Conv2d(\n",
    "            in_size=(32, 32, 32),\n",
    "            out_channels=64,\n",
    "            kernel_size=4,\n",
    "            stride=2,\n",
    "            padding='SAME'\n",
    "        )  # 32x32 -> 16x16\n",
    "        \n",
    "        self.enc_conv3 =  brainstate.nn.Conv2d(\n",
    "            in_size=(16, 16, 64),\n",
    "            out_channels=latent_dim,\n",
    "            kernel_size=4,\n",
    "            stride=2,\n",
    "            padding='SAME'\n",
    "        )  # 16x16 -> 8x8\n",
    "        \n",
    "        # Decoder (using transposed convolutions)\n",
    "        self.dec_conv1 =  brainstate.nn.ConvTranspose2d(\n",
    "            in_size=(8, 8, latent_dim),\n",
    "            out_channels=64,\n",
    "            kernel_size=4,\n",
    "            stride=2,\n",
    "            padding='SAME'\n",
    "        )  # 8x8 -> 16x16\n",
    "        \n",
    "        self.dec_conv2 =  brainstate.nn.ConvTranspose2d(\n",
    "            in_size=(16, 16, 64),\n",
    "            out_channels=32,\n",
    "            kernel_size=4,\n",
    "            stride=2,\n",
    "            padding='SAME'\n",
    "        )  # 16x16 -> 32x32\n",
    "        \n",
    "        self.dec_conv3 =  brainstate.nn.ConvTranspose2d(\n",
    "            in_size=(32, 32, 32),\n",
    "            out_channels=3,\n",
    "            kernel_size=4,\n",
    "            stride=2,\n",
    "            padding='SAME'\n",
    "        )  # 32x32 -> 64x64\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode image to latent representation.\"\"\"\n",
    "        x = jax.nn.relu(self.enc_conv1(x))\n",
    "        x = jax.nn.relu(self.enc_conv2(x))\n",
    "        x = jax.nn.relu(self.enc_conv3(x))\n",
    "        return x\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"Decode latent representation to image.\"\"\"\n",
    "        x = jax.nn.relu(self.dec_conv1(z))\n",
    "        x = jax.nn.relu(self.dec_conv2(x))\n",
    "        x = jax.nn.sigmoid(self.dec_conv3(x))  # Output in [0, 1]\n",
    "        return x\n",
    "    \n",
    "    def update(self, x):\n",
    "        \"\"\"Full forward pass: encode then decode.\"\"\"\n",
    "        z = self.encode(x)\n",
    "        reconstruction = self.decode(z)\n",
    "        return reconstruction\n",
    "\n",
    "# Create and test the autoencoder\n",
    "autoencoder = ConvAutoencoder(latent_dim=128)\n",
    "x = jnp.ones((4, 64, 64, 3))\n",
    "\n",
    "# Encode\n",
    "z = autoencoder.encode(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Latent shape: {z.shape}\")\n",
    "\n",
    "# Decode\n",
    "reconstruction = autoencoder.decode(z)\n",
    "print(f\"Reconstruction shape: {reconstruction.shape}\")\n",
    "\n",
    "# Full pass\n",
    "output = autoencoder(x)\n",
    "print(f\"\\nFull pass output shape: {output.shape}\")\n",
    "print(f\"Reconstruction matches input shape: {output.shape == x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"unet-example\"></a>\n",
    "### Example 3: U-Net for Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleUNet( brainstate.nn.Module):\n",
    "    \"\"\"Simplified U-Net architecture for semantic segmentation.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder (downsampling path)\n",
    "        self.enc1 =  brainstate.nn.Conv2d(\n",
    "            in_size=(128, 128, 3),\n",
    "            out_channels=64,\n",
    "            kernel_size=3,\n",
    "            padding='SAME'\n",
    "        )\n",
    "        \n",
    "        self.enc2 =  brainstate.nn.Conv2d(\n",
    "            in_size=(64, 64, 64),\n",
    "            out_channels=128,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding='SAME'\n",
    "        )\n",
    "        \n",
    "        self.enc3 =  brainstate.nn.Conv2d(\n",
    "            in_size=(32, 32, 128),\n",
    "            out_channels=256,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding='SAME'\n",
    "        )\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck =  brainstate.nn.Conv2d(\n",
    "            in_size=(16, 16, 256),\n",
    "            out_channels=512,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding='SAME'\n",
    "        )\n",
    "        \n",
    "        # Decoder (upsampling path)\n",
    "        self.dec1 =  brainstate.nn.ConvTranspose2d(\n",
    "            in_size=(16, 16, 512),\n",
    "            out_channels=256,\n",
    "            kernel_size=4,\n",
    "            stride=2,\n",
    "            padding='SAME'\n",
    "        )\n",
    "        \n",
    "        self.dec2 =  brainstate.nn.ConvTranspose2d(\n",
    "            in_size=(32, 32, 256 + 256),  # +256 from skip connection\n",
    "            out_channels=128,\n",
    "            kernel_size=4,\n",
    "            stride=2,\n",
    "            padding='SAME'\n",
    "        )\n",
    "        \n",
    "        self.dec3 =  brainstate.nn.ConvTranspose2d(\n",
    "            in_size=(64, 64, 128 + 128),  # +128 from skip connection\n",
    "            out_channels=64,\n",
    "            kernel_size=4,\n",
    "            stride=2,\n",
    "            padding='SAME'\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.output =  brainstate.nn.Conv2d(\n",
    "            in_size=(128, 128, 64 + 64),  # +64 from skip connection\n",
    "            out_channels=num_classes,\n",
    "            kernel_size=1,\n",
    "            padding='SAME'\n",
    "        )\n",
    "    \n",
    "    def update(self, x):\n",
    "        # Encoder\n",
    "        e1 = jax.nn.relu(self.enc1(x))  # 128x128x64\n",
    "        e2 = jax.nn.relu(self.enc2(e1))  # 64x64x128\n",
    "        e3 = jax.nn.relu(self.enc3(e2))  # 32x32x256\n",
    "        \n",
    "        # Bottleneck\n",
    "        b = jax.nn.relu(self.bottleneck(e3))  # 16x16x512\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        d1 = jax.nn.relu(self.dec1(b))  # 32x32x256\n",
    "        d1 = jnp.concatenate([d1, e3], axis=-1)  # Skip connection\n",
    "        \n",
    "        d2 = jax.nn.relu(self.dec2(d1))  # 64x64x128\n",
    "        d2 = jnp.concatenate([d2, e2], axis=-1)  # Skip connection\n",
    "        \n",
    "        d3 = jax.nn.relu(self.dec3(d2))  # 128x128x64\n",
    "        d3 = jnp.concatenate([d3, e1], axis=-1)  # Skip connection\n",
    "        \n",
    "        # Output\n",
    "        out = self.output(d3)  # 128x128x num_classes\n",
    "        return out\n",
    "\n",
    "# Create and test the U-Net\n",
    "unet = SimpleUNet(num_classes=2)\n",
    "x = jnp.ones((2, 128, 128, 3))\n",
    "segmentation_map = unet(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Segmentation map shape: {segmentation_map.shape}\")\n",
    "print(f\"Number of classes: {segmentation_map.shape[-1]}\")\n",
    "print(f\"\\nU-Net preserves spatial dimensions: {x.shape[1:3] == segmentation_map.shape[1:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This tutorial covered the comprehensive convolution API in BrainState:\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Standard Convolutions** (`Conv1d`, `Conv2d`, `Conv3d`)\n",
    "   - Feature extraction and downsampling\n",
    "   - Support for various kernel sizes, strides, and padding modes\n",
    "   - Grouped and dilated convolutions for efficiency\n",
    "\n",
    "2. **Weight-Standardized Convolutions** (`ScaledWSConv*`)\n",
    "   - Improved training stability\n",
    "   - Works well with Group Normalization\n",
    "   - Optional learnable gain parameter\n",
    "\n",
    "3. **Transposed Convolutions** (`ConvTranspose*`)\n",
    "   - Upsampling for decoders, generators, and segmentation\n",
    "   - Controllable upsampling factor via stride\n",
    "   - Essential for encoder-decoder architectures\n",
    "\n",
    "4. **Flexible Data Formats**\n",
    "   - Channels-last (JAX/TensorFlow): default\n",
    "   - Channels-first (PyTorch): via `channel_first=True`\n",
    "   - Easy migration from PyTorch\n",
    "\n",
    "5. **Advanced Features**\n",
    "   - Grouped convolutions for parameter efficiency\n",
    "   - Dilated convolutions for larger receptive fields\n",
    "   - Multiple padding modes (SAME, VALID, explicit)\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "- Use **'SAME' padding** to preserve spatial dimensions when stride=1\n",
    "- Use **grouped convolutions** to reduce parameters in large models\n",
    "- Use **dilated convolutions** to increase receptive field without pooling\n",
    "- Use **weight standardization** for training stability, especially with small batches\n",
    "- Use **transposed convolutions** with kernel_size = stride * 2 for smooth upsampling\n",
    "- Choose **data format** based on your framework background (JAX: channels-last, PyTorch: channels-first)\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Combine convolutions with normalization layers (BatchNorm, GroupNorm)\n",
    "- Experiment with different activation functions\n",
    "- Build more complex architectures (ResNet, DenseNet, etc.)\n",
    "- Train models on real datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. **Weight Standardization**: Qiao, S., Wang, H., Liu, C., Shen, W., & Yuille, A. (2019). Weight Standardization. arXiv:1903.10520\n",
    "2. **U-Net**: Ronneberger, O., Fischer, P., & Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. MICCAI 2015\n",
    "3. **Grouped Convolutions**: Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. NIPS 2012\n",
    "4. **Dilated Convolutions**: Yu, F., & Koltun, V. (2015). Multi-Scale Context Aggregation by Dilated Convolutions. ICLR 2016\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [BrainState Documentation](https://brainstate.readthedocs.io/)\n",
    "- [JAX Documentation](https://jax.readthedocs.io/)\n",
    "- [Convolution Arithmetic Guide](https://github.com/vdumoulin/conv_arithmetic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
